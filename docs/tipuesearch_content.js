var tipuesearch = {"pages":[{"title":"GitHub Page","text":"All course content (except Lectures 2-4,12-17) are in: GitHub repo We suggest that you clone this repository.","tags":"pages","url":"pages/github.html"},{"title":"Modules Page","text":"Module description and details go here","tags":"pages","url":"pages/modules.html"},{"title":"Schedule","text":"﻿Week Date Lecture (Mon) Date Lecture (Wed) Lab Advanced Section (Wed) Assignment (release and due) 1 28-Jan Lecture 1: Intro + Review of 109A Preview of 109B 30-Jan Lecture 2: Smoothing and Additive 1/3 Lab 1: Setting up enviroment 2 4-Feb Lecture 3: Smoothing and Additive 2/3 6-Feb Lecture 4: Smoothing and GAM 3/3 Lab 2: Smoothing/GAM HW1 (2/3) 3 11-Feb Lecture 5: Feed Forward + Reg + Review from NN fall 13-Feb Lecture 6: Optimization of NN (Solvers) Lab 3: Optimization Advanced Section 1: Optimization/Dropout HW2 (2/10) 4 18-Feb Holiday 20-Feb Lecture 7: AWS scalable systems SQL Lab 4: Setting UP AWS Advanced Section 2: Optimal Transport 5 25-Feb Lecture 8: CNNs-1 27-Feb Lecture 9: CNNs-2 Lab 5: CNNs Advanced Section 3: CNNs and Object Detection HW3 (2/24) 6 4-Mar Lecture 10: RNN 1 6-Mar Lecture 11: RNN 2 Lab 6: RNNS Advanced Section 4: LSTN, GRU in NLP HW4 (3/3) 7 11-Mar Lecture 12: Unsupervised learning/clustering 1 13-Mar Lecture 13: Unsupervised learning/clustering 2 Lab 7: Clusterig Advanced Section 5: Unsup + AE HW5 (3/10) 8 25-Mar Lecture 14: Autoencoders 27-Mar Lecture 15: Bayesian 1/3 Lab 8: Bayes 1 9 1-Apr Lecture 16: Bayesian 2/3 3-Apr Lecture 17: Bayesian 3/3 Lab 9: Bayes 2 Advanced Section 7:LDA and Bayes HW6 (3/30) 10 8-Apr Lecture 18: Generative Models Varational Autoenders 1 8-Apr Lecture 19: Generative Models Varational Autoenders 2 Lab 10: VAE Advanced Section 8:VAE+GANS HW7 (4/7) 11 15-Apr Lecture 20: GANS 1 17-Apr Lecture 21: GANS 2 Lab 11: GANS","tags":"pages","url":"pages/schedule.html"},{"title":"Syllabus","text":"Course description Tentative Course Topics Course Objectives Course Components Lectures Labs In-class Quizzes Advanced Sections Exams Projects Homework Assignments Course Resources Online Materials Recommended Textbooks Getting Help Course Policies and Expectations Grading Collaboration Policy Late or Wrongly Submitted Assignments Re-grade Requests Auditing the Class Academic Integrity Accommodations for students with disabilities Diversity and Inclusion Statement Course description Data Science 2 is the second half of a one-year introduction to data science. Building upon the material in Data Science 1, the course introduces advanced methods for data wrangling, data visualization, and deep neural networks, statistical modeling, and prediction. Topics include big data and database management, multiple deep learning subjects such as CNNs, RNNs, autoencoders, and generative models as well as basic Bayesian methods, nonlinear statistical models and unsupervised learning. The programming language will be python. Tentative Course Topics Feed Forward Neural Networks Regularization of Neural Network Optimization of Neural Networks Autoencoders Convolutional Neural Networks Recurrent Neural Networks Variational AutoEncoders Generative Adversarial Networks Smoothing and Additive Models Unsupervised Learning Bayesian Inference Models, LDA SQL AWS Course Objectives Upon successful completion of this course, you should feel comfortable with the material mentioned above, plus you will have had experience in working with others on real-world problems. Both the content knowledge, the project, and teamwork, will prepare you for the professional world or further studies. Course Components There will be live video feed available only to distance education students for lectures, labs, and advanced sections. Recordings for all other students will be available within 24 hrs. Lectures The class meets twice a week for lectures. Attending lectures is a crucial component of learning the material presented in this course. Labs Lectures are supplemented by weekly programming labs. Labs are an important aspect of the course, as they supplement material from lectures with examples, discuss programming environments, and teach you important skills. In-class Quizzes At the end of each lecture, we will ask you to take a short graded quiz on the material presented in class. These quizzes will count towards your final grade. Advanced Sections The course will include advanced sections for 209 students and will cover a different topic per week. These are 75 min lectures and they will cover advanced topics like the mathematical underpinnings of the methods seen in lecture and lab and extensions of those methods. The material covered in the advanced sections is required for all ac209 students. Tentative topics are: Earth Mover's Distance Dropout ConvNets: LeNet, AlexNet, VGG-15, ResNet and Inception LSTN, GRU in NLP Neural style transfer learning Deep Reinforcement Learning Variational Inference Exams There are no exams in this course. Projects - For distant students The goal of the project is to have a complete end-to-end data science process encompassing both semesters of subject material while working as a 3-4 person team. We will supply a small set of project choices within the thematic categories. Teams may propose a different project with sufficient notice and will be subject to approval by the course staff. - For campus students During the final four (4) weeks of the course, students will be divided in break-out thematic sections where they will study topics such as medicine, law, astronomy, e-commerce, and government. Each section will include lectures by Harvard faculty, experts on the field, followed by project work also led by that faculty. You will get to present your projects in the SEAS Design Fair at the end of the semester. Homework Assignments There will be seven graded homework assignments. Some of them will be due one week after being assigned and some will be due two weeks after being assigned. For five assignments, you have the option to work and submit in pairs, the two remaining are to be completed individually. Course Resources Online Materials All course materials, including lecture notes, lab notes, and section notes will be published in the class GitHub: https://github.com/Harvard-IACS/2019-CS109B . Assignments will only be posted on Canvas. Working environment You will be working in Jupyter Notebooks which you can run in your own machine or in the SEAS JupyterHub cloud (details on this to come). Recommended Textbooks ISLR: An Introduction to Statistical Learning. by James, Witten, Hastie, Tibshirani (Springer: New York, 2013) DL: Deep Learning by Goodfellow, Bengio and Courville. Free electronic versions are available ( ISLR ), ( DL) or hard copy through Amazon ( ISLR) , ( DL ). Getting Help For questions about homework, course content, package installation, the process is: try to troubleshoot yourself by reading the lecture, lab, and section notes, and looking up online resources. go to office hours this is the best way to get help. post on the class Piazza; we want you and your peers to engage in helping each other. TFs also monitor Piazza and will respond within 24 hours. Note that Piazza questions are visible to everyone. If you are citing homework solution code you want to post privately so that only the staff sees your message. watch for official announcements via Canvas so make sure you have your Canvas notifications turned on. Piazza should always be your first resource for seeking answers to your content questions. send an email to the Helpline ( cs109b2019@gmail.com ) for administrative issues, regrade requests, and non-content specific questions we have this email account. for personal matters that you do not feel comfortable sharing with the TFs, you may send an email to either or both of the instructors. Course Policies and Expectations Grading for CS109b, STAT121b, and CS209b: <<<<<<< HEAD ======= 3aa6558f04b882eacb6af4d0b69db82445ca5329 Paired-option Homeworks 45% (5 homework for which you have the option to work in pairs) Individual Homeworks 25% (2 homework which you must complete individually) Quizzes 10% (you may drop 40% of the quizzes) Project 20% TOTAL 100% Note: Regular homework (for everyone) counts as 5 points. 209b extra homework counts as 1 point. Grading for DCE: Paired-option Homeworks 48% (5 homework for which you have the option to work in pairs) Individual Homeworks 28% (2 homework which you must complete individually) Project 24% TOTAL 100% Collaboration Policy We expect you to adhere to the Harvard Honor Code at all times. Failure to adhere to the honor code and our policies may result in serious penalties, up to and including automatic failure in the course and reference to the ad board. If you work with a partner on an assignment make sure both parties solve all the problems. Do not divide and conquer. You are expected to be intellectually honest and give credit where credit is due. In particular: if you work with a fellow student but decide to submit different papers, include the name of each other in the designated area of the submission paper. if you work with a fellow student and want to submit the same paper you need to form a group prior to the submission. Details in the assignment. Not all assignments will permit group submissions. you need to write your solutions entirely on your own or with your collaborator you are welcome to take ideas from code presented in labs, lecture, or sections but you need to change it, adapt it to your style, and ultimately write your own. We do not want to see code copied verbatim from the above sources. if you use code found on the internet, books, or other sources you need to cite those sources. you should not view any written materials or code created by other students for the same assignment; you may not provide or make available solutions to individuals who take or may take this course in the future. if the assignment allows it you may use third-party libraries and example code, so long as the material is available to all students in the class and you give proper attribution. Do not remove any original copyright notices and headers. Late or Wrongly Submitted Assignments There are no late days in homework submission. We will accept late submissions only for medical reasons and if accompanied by a doctor's note. To submit after Canvas has closed or to ask for an extension , send an email to the Helpline with subject line \"Submit HW1: Reason=the flu\" replacing 'HW1' with the name of the current assignment and \"the flu\" with your reason. You need to attach the note from your medical provider otherwise we will not accept the request. If you forgot to join a Group with your peer and are asking for the same grade we will accept this with no penalty up to HW3. For homeworks beyond that we feel that you should be familiar with the process of joining groups. After that there will be a penalty of -1 point for both members of the group provided the submission was on time. Re-grade Requests Our graders and instructors make every effort in grading accurately and in giving you a lot of feedback. If you discover that your answer to a homework problem was correct but it was marked as incorrect, send an email to the Helpline with a description of the error. Please do not submit regrade requests based on what you perceive is overly harsh grading . The points we take off are based on a grading rubric that is being applied uniformly to all assignments. If you decide to send a regrade request , send an email to the Helpline with subject line \"Regrade HW1: Grader=johnsmith\" replacing 'HW1' with the current assignment and 'johnsmith' with the name of the grader within 48 hours of the grade release . Auditing the Class If you would like to audit the class, please send an email to the Helpline indicating who you are and why you want to audit the class. You need a HUID to be included to Canvas. Academic Integrity Ethical behavior is an important trait of a Data Scientist, from ethically handling data to attribution of code and work of others. Thus, in CS109 we give a strong emphasis to Academic Honesty. As a student your best guidelines are to be reasonable and fair. We encourage teamwork for problem sets, but you should not split the homework and you should work on all the problems together. For more detailed expectations, please refer to the Collaborations section above. Accommodations for students with disabilities Students needing academic adjustments or accommodations because of a documented disability must present their Faculty Letter from the Accessible Education Office (AEO) and speak with the professor by the end of the second week of the term, (fill in specific date). Failure to do so may result in the Course Head's inability to respond in a timely manner. All discussions will remain confidential, although Faculty are invited to contact AEO to discuss appropriate implementation. Diversity and Inclusion Statement Data Science, like many fields of science, has historically only been represented by a small sliver of the population. This is despite some of the early computer scientist pioneers being women (see Ada Lovelace and Grace Hopper for two examples). Recent initiatives have attempted to overcome some barriers to entry: Made w/ Code . We would like to attempt to discuss diversity in data science from time to time where appropriate and possible. Please contact us (in person or electronically) or submit anonymous feedback if you have any suggestions to improve the diversity of the course materials. Furthermore, we would like to create a learning environment for our students that supports a diversity of thoughts, perspectives and experiences, and honors your identities (including race, gender, class, sexuality, religion, ability, etc.) To help accomplish this: If you have a name and/or set of pronouns that differ from those that appear in your official Harvard records, please let us know! If you feel like your performance in the class is being impacted by your experiences outside of class, please don't hesitate to come and talk with us. We want to be a resource for you. Remember that you can also submit anonymous feedback (which will lead to me making a general announcement to the class, if necessary to address your concerns). If you prefer to speak with someone outside of the course, you may find helpful resources at the Harvard Office of Diversity and Inclusion . We (like many people) are still in the process of learning about diverse perspectives and identities. If something was said in class (by anyone) that made you feel uncomfortable, please talk to us about it. (Again, anonymous feedback is always an option.) As a participant in course discussions, you should also strive to honor the diversity of your classmates.","tags":"pages","url":"pages/syllabus.html"},{"title":"Advanced Section 3: CNNs and Object Detection","text":"Slides PDF","tags":"A-sections","url":"a-sections/a-section3/"},{"title":"Lecture 8: CNN-1","text":"Slides Lecture 8 PDF Lecture 8 PPTX Associated Material","tags":"lectures","url":"lectures/lecture8/"},{"title":"Advanced Section 2: Optimal Transport","text":"Slides PDF Lecture Notes PDF Intro to optimization notes PDF References for further reading Computation optimal transport Learning with a Wasserstein Loss Optimal Transport for Domain Adaptation","tags":"A-sections","url":"a-sections/a-section2/"},{"title":"Lab 3: Optimization of Neural Networks","text":"Lab 3 Notebooks Lab3 Optimization in NNs Lab3 Optimization with solutions","tags":"lab","url":"lab/lab3/"},{"title":"Lab 3: Optimization of Neural Networks","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS-109B Introduction to Data Science Lab 3: Optimization in Artificial Neural Networks Harvard University Spring 2019 Lab instructor : Eleni Kaxiras Instructors: Pavlos Protopapas and Mark Glickman In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: blockquote { background: #AEDE94; } h1 { padding-top: 25px; padding-bottom: 25px; text-align: left; padding-left: 10px; background-color: #DDDDDD; color: black; } h2 { padding-top: 10px; padding-bottom: 10px; text-align: left; padding-left: 5px; background-color: #EEEEEE; color: black; } div.exercise { background-color: #ffcccc; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; } div.discussion { background-color: #ccffcc; border-color: #88E97A; border-left: 5px solid #0A8000; padding: 0.5em; } div.theme { background-color: #DDDDDD; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; font-size: 18pt; } div.gc { background-color: #AEDE94; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; font-size: 12pt; } p.q1 { padding-top: 5px; padding-bottom: 5px; text-align: left; padding-left: 5px; background-color: #EEEEEE; color: black; } header { padding-top: 35px; padding-bottom: 35px; text-align: left; padding-left: 10px; background-color: #DDDDDD; color: black; } Learning Goals In this lab, we'll explore ways to optimize the loss function of a Multilayer Learning Perceptor (MLP) by tuning the model hyperparameters. We'll also explore the use of cross-validation as a technique for checking potential values for these hyperparameters. By the end of this lab, you should: Be familiar with the use of sklearn 's optimize function. Be able to identify the hyperparameters that go into the training of a MLP. Be familiar with the implementation in keras of various optimization techniques. Know how to use callbacks Apply cross-validation to check for multiple values of hyperparameters. In [2]: import matplotlib.pyplot as plt import numpy as np from scipy.optimize import minimize % matplotlib inline Part 1: Beale's function First let's look at function optimization in scipy.optimize , using Beale's function as an example Optimizing a function $f: A\\rightarrow R$, from some set A to the real numbers is finding an element $x_0\\,\\epsilon\\, A$ such that $f(x_0)\\leq f(x)$ for all $x\\,\\epsilon\\, A$ (finding the minimum) or such that $f(x_0)\\geq f(x)$ for all $x\\,\\epsilon\\, A$ (finding the maximum). To illustrate our point we will use a function of two parameters. Our goal is to optimize over these 2 parameters. We can extend to higher dimensions by plotting pairs of parameters against each other. The Wikipedia article on Test functions for optimization has a few functions that are useful for evaluating optimization algorithms. Here is Beale's function: $f(x,y)$ = $(1.5−x+xy)&#94;2+(2.25−x+xy&#94;2)&#94;2+(2.625−x+xy&#94;3)&#94;2$ We already know that this function has a minimum at [3.0, 0.5]. Let's see if scipy will find it. source: https://en.wikipedia.org/wiki/Test_functions_for_optimization In [3]: # define Beale's function which we want to minimize def objective ( X ): x = X [ 0 ]; y = X [ 1 ] return ( 1.5 - x + x * y ) ** 2 + ( 2.25 - x + x * y ** 2 ) ** 2 + ( 2.625 - x + x * y ** 3 ) ** 2 In [4]: # function boundaries xmin , xmax , xstep = - 4.5 , 4.5 , . 9 ymin , ymax , ystep = - 4.5 , 4.5 , . 9 In [5]: # Let's create some points x1 , y1 = np . meshgrid ( np . arange ( xmin , xmax + xstep , xstep ), np . arange ( ymin , ymax + ystep , ystep )) Let's make an initial guess In [6]: # initial guess x0 = [ 4. , 4. ] f0 = objective ( x0 ) print ( f0 ) 68891.203125 In [7]: bnds = (( xmin , xmax ), ( ymin , ymax )) minimum = minimize ( objective , x0 , bounds = bnds ) In [8]: print ( minimum ) fun: 2.068025638865627e-12 hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64> jac: array([-1.55969780e-06, 9.89837957e-06]) message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL' nfev: 60 nit: 14 status: 0 success: True x: array([3.00000257, 0.50000085]) In [9]: real_min = [ 3.0 , 0.5 ] print ( f 'The answer, {minimum.x} , is very close to the optimum as we know it, which is {real_min} ' ) print ( f 'The value of the objective for {real_min} is {objective(real_min)}' ) The answer, [3.00000257 0.50000085], is very close to the optimum as we know it, which is [3.0, 0.5] The value of the objective for [3.0, 0.5] is 0.0 Part 2: Optimization in neural networks In general: Learning Representation --> Objective function --> Optimization algorithm A neural network can be defined as a framework that combines inputs and tries to guess the output. If we are lucky enough to have some results, called \"the ground truth\", to compare the outputs produced by the network, we can calculate the error . So the network guesses, calculates some error function, guesses again, trying to minimize this error, guesses again, until the error does not go down any more. This is optimization. In neural networks the most common used optimization algorithms, are flavors of GD (gradient descent) . The objective function used in gradient descent is the loss function which we want to minimize . A keras Refresher Keras is a Python library for deep learning that can run on top of both Theano or TensorFlow, two powerful Python libraries for fast numerical computing created and released by Facebook and Google, respectevely. Keras was developed to make developing deep learning models as fast and easy as possible for research and practical applications. It runs on Python 2.7 or 3.5 and can seamlessly execute on GPUs and CPUs. Keras is built on the idea of a model. At its core we have a sequence of layers called the Sequential model which is a linear stack of layers. Keras also provides the functional API , a way to define complex models, such as multi-output models, directed acyclic graphs, or models with shared layers. We can summarize the construction of deep learning models in Keras using the Sequential model as follows: Define your model : create a Sequential model and add layers. Compile your model : specify loss function and optimizers and call the .compile() function. Fit your model : train the model on data by calling the .fit() function. Make predictions : use the model to generate predictions on new data by calling functions such as .evaluate() or .predict() . Callbacks: taking a peek into our model while it's training You can look at what is happening in various stages of your model by using callbacks . A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument callbacks) to the .fit() method of the Sequential or Model classes. The relevant methods of the callbacks will then be called at each stage of the training. A callback function you are already familiar with is keras.callbacks.History() . This is automatically included in .fit() . Another very useful one is keras.callbacks.ModelCheckpoint which saves the model with its weights at a certain point in the training. This can prove useful if your model is running for a long time and a system failure happens. Not all is lost then. It's a good practice to save the model weights only when an improvement is observed as measured by the acc , for example. keras.callbacks.EarlyStopping stops the training when a monitored quantity has stopped improving. keras.callbacks.LearningRateScheduler will change the learning rate during training. We will apply some callbacks later. For full documentation on callbacks see https://keras.io/callbacks/ What are the steps to optimizing our network? In [10]: import tensorflow as tf import keras from keras import layers from keras import models from keras import utils from keras.layers import Dense from keras.models import Sequential from keras.layers import Flatten from keras.layers import Dropout from keras.layers import Activation from keras.regularizers import l2 from keras.optimizers import SGD from keras.optimizers import RMSprop from keras import datasets from keras.callbacks import LearningRateScheduler from keras.callbacks import History from keras import losses from sklearn.utils import shuffle print ( tf . VERSION ) print ( tf . keras . __version__ ) 1.12.0 2.1.6-tf Using TensorFlow backend. In [11]: # fix random seed for reproducibility np . random . seed ( 5 ) Step 1 - Deciding on the network topology (not really considered optimization but is obviously very important) We will use the MNIST dataset which consists of grayscale images of handwritten digits (0-9) whose dimension is 28x28 pixels. Each pixel is 8 bits so its value ranges from 0 to 255. In [12]: #mnist = tf.keras.datasets.mnist mnist = keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train . shape , y_train . shape Out[12]: ((60000, 28, 28), (60000,)) Each label is a number between 0 and 9 In [13]: print ( y_train ) [5 0 4 ... 5 6 8] Let's look at some 10 of the images In [14]: plt . figure ( figsize = ( 10 , 10 )) for i in range ( 10 ): plt . subplot ( 5 , 5 , i + 1 ) plt . xticks ([]) plt . yticks ([]) plt . grid ( False ) plt . imshow ( x_train [ i ], cmap = plt . cm . binary ) plt . xlabel ( y_train [ i ]) In [15]: x_train [ 45 ] . shape x_train [ 45 , 15 : 20 , 15 : 20 ] Out[15]: array([[ 11, 198, 231, 41, 0], [ 82, 252, 204, 0, 0], [253, 253, 141, 0, 0], [252, 220, 36, 0, 0], [252, 96, 0, 0, 0]], dtype=uint8) In [17]: print ( f 'We have {x_train.shape[0]} train samples' ) print ( f 'We have {x_test.shape[0]} test samples' ) We have 60000 train samples We have 10000 test samples Preprocessing the data To run our NN we need to pre-process the data First we need to make the 2D image arrays into 1D (flatten them). We can either perform this by using array reshaping with numpy.reshape() or the keras ' method for this: a layer called tf.keras.layers.Flatten which transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1D-array of 28 * 28 = 784 pixels. Then we need to normalize the pixel values (give them values between 0 and 1) using the following transformation: \\begin{align} x := \\dfrac{x - x_{min}}{x_{max} - x_{min}} \\textrm{} \\end{align} In our case $x_{min} = 0$ and $x_{max} = 255$ so the formula becomes simply $x := {x}/255$ In [18]: # normalize the data x_train , x_test = x_train / 255.0 , x_test / 255.0 In [19]: # reshape the data into 1D vectors x_train = x_train . reshape ( 60000 , 784 ) x_test = x_test . reshape ( 10000 , 784 ) num_classes = 10 In [20]: x_train . shape [ 1 ] Out[20]: 784 Now let's prepare our class vector (y) to a binary class matrix, e.g. for use with categorical_crossentropy. In [21]: # Convert class vectors to binary class matrices y_train = keras . utils . to_categorical ( y_train , num_classes ) y_test = keras . utils . to_categorical ( y_test , num_classes ) In [22]: y_train [ 0 ] Out[22]: array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32) Now we are ready to build the model! Step 2 - Adjusting the learning rate One of the most common optimization algorithm is Stochastic Gradient Descent (SGD). The hyperparameters that can be optimized in SGD are learning rate , momentum , decay and nesterov . Learning rate controls the weight at the end of each batch, and momentum controls how much to let the previous update influence the current weight update. Decay indicates the learning rate decay over each update, and nesterov takes the value True or False depending on if we want to apply Nesterov momentum. Typical values for those hyperparameters are lr=0.01, decay=1e-6, momentum=0.9, and nesterov=True. The learning rate hyperparameter goes into the optimizer function which we will see below. Keras has a default learning rate scheduler in the SGD optimizer that decreases the learning rate during the stochastic gradient descent optimization algorithm. The learning rate is decreased according to this formula: \\begin{align} lr = lr * 1./(1. + decay * epoch) \\textrm{} \\end{align} source: http://cs231n.github.io/neural-networks-3 Let's implement a learning rate adaptation schedule in Keras . We'll start with SGD and a learning rate value of 0.1. We will then train the model for 60 epochs and set the decay argument to 0.0016 (0.1/60). We also include a momentum value of 0.8 since that seems to work well when using an adaptive learning rate. In [23]: epochs = 60 learning_rate = 0.1 decay_rate = learning_rate / epochs momentum = 0.8 sgd = SGD ( lr = learning_rate , momentum = momentum , decay = decay_rate , nesterov = False ) In [24]: # build the model input_dim = x_train . shape [ 1 ] lr_model = Sequential () lr_model . add ( Dense ( 64 , activation = tf . nn . relu , kernel_initializer = 'uniform' , input_dim = input_dim )) lr_model . add ( Dropout ( 0.1 )) lr_model . add ( Dense ( 64 , kernel_initializer = 'uniform' , activation = tf . nn . relu )) lr_model . add ( Dense ( num_classes , kernel_initializer = 'uniform' , activation = tf . nn . softmax )) # compile the model lr_model . compile ( loss = 'categorical_crossentropy' , optimizer = sgd , metrics = [ 'acc' ]) In [25]: %%time # Fit the model batch_size = int ( input_dim / 100 ) lr_model_history = lr_model . fit ( x_train , y_train , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_data = ( x_test , y_test )) Train on 60000 samples, validate on 10000 samples Epoch 1/60 60000/60000 [==============================] - 9s 145us/step - loss: 0.3158 - acc: 0.9043 - val_loss: 0.1467 - val_acc: 0.9550 Epoch 2/60 60000/60000 [==============================] - 8s 136us/step - loss: 0.1478 - acc: 0.9555 - val_loss: 0.1194 - val_acc: 0.9617 Epoch 3/60 60000/60000 [==============================] - 8s 137us/step - loss: 0.1248 - acc: 0.9620 - val_loss: 0.1122 - val_acc: 0.9646 Epoch 4/60 60000/60000 [==============================] - 8s 136us/step - loss: 0.1167 - acc: 0.9638 - val_loss: 0.1075 - val_acc: 0.9681 Epoch 5/60 60000/60000 [==============================] - 8s 137us/step - loss: 0.1103 - acc: 0.9666 - val_loss: 0.1039 - val_acc: 0.9691 Epoch 6/60 60000/60000 [==============================] - 8s 137us/step - loss: 0.1051 - acc: 0.9677 - val_loss: 0.1015 - val_acc: 0.9694 Epoch 7/60 60000/60000 [==============================] - 8s 136us/step - loss: 0.1003 - acc: 0.9691 - val_loss: 0.1002 - val_acc: 0.9694 Epoch 8/60 60000/60000 [==============================] - 9s 144us/step - loss: 0.0961 - acc: 0.9707 - val_loss: 0.0998 - val_acc: 0.9694 Epoch 9/60 60000/60000 [==============================] - 9s 154us/step - loss: 0.0951 - acc: 0.9707 - val_loss: 0.0989 - val_acc: 0.9699 Epoch 10/60 60000/60000 [==============================] - 9s 150us/step - loss: 0.0919 - acc: 0.9721 - val_loss: 0.0978 - val_acc: 0.9696 Epoch 11/60 60000/60000 [==============================] - 8s 141us/step - loss: 0.0930 - acc: 0.9720 - val_loss: 0.0964 - val_acc: 0.9702 Epoch 12/60 60000/60000 [==============================] - 8s 141us/step - loss: 0.0899 - acc: 0.9728 - val_loss: 0.0965 - val_acc: 0.9703 Epoch 13/60 60000/60000 [==============================] - 8s 141us/step - loss: 0.0883 - acc: 0.9732 - val_loss: 0.0951 - val_acc: 0.9713 Epoch 14/60 60000/60000 [==============================] - 8s 141us/step - loss: 0.0871 - acc: 0.9733 - val_loss: 0.0958 - val_acc: 0.9705 Epoch 15/60 60000/60000 [==============================] - 8s 141us/step - loss: 0.0888 - acc: 0.9731 - val_loss: 0.0952 - val_acc: 0.9709 Epoch 16/60 60000/60000 [==============================] - 9s 145us/step - loss: 0.0857 - acc: 0.9743 - val_loss: 0.0950 - val_acc: 0.9713 Epoch 17/60 60000/60000 [==============================] - 9s 157us/step - loss: 0.0843 - acc: 0.9742 - val_loss: 0.0957 - val_acc: 0.9709 Epoch 18/60 60000/60000 [==============================] - 8s 142us/step - loss: 0.0842 - acc: 0.9749 - val_loss: 0.0942 - val_acc: 0.9719 Epoch 19/60 60000/60000 [==============================] - 9s 142us/step - loss: 0.0839 - acc: 0.9750 - val_loss: 0.0936 - val_acc: 0.9723 Epoch 20/60 60000/60000 [==============================] - 9s 142us/step - loss: 0.0824 - acc: 0.9748 - val_loss: 0.0942 - val_acc: 0.9723 Epoch 21/60 60000/60000 [==============================] - 9s 143us/step - loss: 0.0824 - acc: 0.9749 - val_loss: 0.0940 - val_acc: 0.9725 Epoch 22/60 60000/60000 [==============================] - 9s 142us/step - loss: 0.0829 - acc: 0.9752 - val_loss: 0.0938 - val_acc: 0.9718 Epoch 23/60 60000/60000 [==============================] - 9s 143us/step - loss: 0.0795 - acc: 0.9763 - val_loss: 0.0939 - val_acc: 0.9718 Epoch 24/60 60000/60000 [==============================] - 9s 149us/step - loss: 0.0796 - acc: 0.9763 - val_loss: 0.0936 - val_acc: 0.9722 Epoch 25/60 60000/60000 [==============================] - 8s 140us/step - loss: 0.0783 - acc: 0.9759 - val_loss: 0.0935 - val_acc: 0.9724 Epoch 26/60 60000/60000 [==============================] - 8s 140us/step - loss: 0.0805 - acc: 0.9755 - val_loss: 0.0937 - val_acc: 0.9721 Epoch 27/60 60000/60000 [==============================] - 8s 140us/step - loss: 0.0795 - acc: 0.9759 - val_loss: 0.0930 - val_acc: 0.9721 Epoch 28/60 60000/60000 [==============================] - 9s 148us/step - loss: 0.0786 - acc: 0.9765 - val_loss: 0.0931 - val_acc: 0.9721 Epoch 29/60 60000/60000 [==============================] - 9s 148us/step - loss: 0.0780 - acc: 0.9764 - val_loss: 0.0926 - val_acc: 0.9726 Epoch 30/60 60000/60000 [==============================] - 9s 144us/step - loss: 0.0759 - acc: 0.9768 - val_loss: 0.0925 - val_acc: 0.9726 Epoch 31/60 60000/60000 [==============================] - 9s 147us/step - loss: 0.0780 - acc: 0.9768 - val_loss: 0.0931 - val_acc: 0.9727 Epoch 32/60 60000/60000 [==============================] - 9s 155us/step - loss: 0.0768 - acc: 0.9765 - val_loss: 0.0925 - val_acc: 0.9726 Epoch 33/60 60000/60000 [==============================] - 9s 144us/step - loss: 0.0762 - acc: 0.9770 - val_loss: 0.0927 - val_acc: 0.9723 Epoch 34/60 60000/60000 [==============================] - 9s 149us/step - loss: 0.0765 - acc: 0.9770 - val_loss: 0.0928 - val_acc: 0.9723 Epoch 35/60 60000/60000 [==============================] - 8s 140us/step - loss: 0.0751 - acc: 0.9778 - val_loss: 0.0928 - val_acc: 0.9721 Epoch 36/60 60000/60000 [==============================] - 8s 138us/step - loss: 0.0756 - acc: 0.9772 - val_loss: 0.0919 - val_acc: 0.9721 Epoch 37/60 60000/60000 [==============================] - 8s 140us/step - loss: 0.0760 - acc: 0.9769 - val_loss: 0.0923 - val_acc: 0.9723 Epoch 38/60 60000/60000 [==============================] - 8s 138us/step - loss: 0.0751 - acc: 0.9772 - val_loss: 0.0921 - val_acc: 0.9726 Epoch 39/60 60000/60000 [==============================] - 9s 148us/step - loss: 0.0756 - acc: 0.9774 - val_loss: 0.0924 - val_acc: 0.9728 Epoch 40/60 60000/60000 [==============================] - 8s 141us/step - loss: 0.0750 - acc: 0.9774 - val_loss: 0.0924 - val_acc: 0.9728 Epoch 41/60 60000/60000 [==============================] - 9s 142us/step - loss: 0.0760 - acc: 0.9774 - val_loss: 0.0926 - val_acc: 0.9724 Epoch 42/60 60000/60000 [==============================] - 8s 142us/step - loss: 0.0719 - acc: 0.9783 - val_loss: 0.0920 - val_acc: 0.9730 Epoch 43/60 60000/60000 [==============================] - 9s 143us/step - loss: 0.0730 - acc: 0.9779 - val_loss: 0.0919 - val_acc: 0.9726 Epoch 44/60 60000/60000 [==============================] - 8s 140us/step - loss: 0.0722 - acc: 0.9785 - val_loss: 0.0920 - val_acc: 0.9728 Epoch 45/60 60000/60000 [==============================] - 9s 142us/step - loss: 0.0746 - acc: 0.9774 - val_loss: 0.0923 - val_acc: 0.9730 Epoch 46/60 60000/60000 [==============================] - 9s 148us/step - loss: 0.0736 - acc: 0.9778 - val_loss: 0.0920 - val_acc: 0.9729 Epoch 47/60 60000/60000 [==============================] - 9s 156us/step - loss: 0.0739 - acc: 0.9777 - val_loss: 0.0920 - val_acc: 0.9725 Epoch 48/60 60000/60000 [==============================] - 9s 151us/step - loss: 0.0720 - acc: 0.9783 - val_loss: 0.0917 - val_acc: 0.9731 Epoch 49/60 60000/60000 [==============================] - 9s 146us/step - loss: 0.0735 - acc: 0.9780 - val_loss: 0.0917 - val_acc: 0.9729 Epoch 50/60 60000/60000 [==============================] - 9s 152us/step - loss: 0.0729 - acc: 0.9780 - val_loss: 0.0923 - val_acc: 0.9723 Epoch 51/60 60000/60000 [==============================] - 9s 151us/step - loss: 0.0716 - acc: 0.9777 - val_loss: 0.0919 - val_acc: 0.9727 Epoch 52/60 60000/60000 [==============================] - 9s 145us/step - loss: 0.0716 - acc: 0.9784 - val_loss: 0.0915 - val_acc: 0.9726 Epoch 53/60 60000/60000 [==============================] - 9s 149us/step - loss: 0.0715 - acc: 0.9782 - val_loss: 0.0912 - val_acc: 0.9722 Epoch 54/60 60000/60000 [==============================] - 9s 143us/step - loss: 0.0704 - acc: 0.9786 - val_loss: 0.0911 - val_acc: 0.9720 Epoch 55/60 60000/60000 [==============================] - 9s 142us/step - loss: 0.0721 - acc: 0.9782 - val_loss: 0.0917 - val_acc: 0.9727 Epoch 56/60 60000/60000 [==============================] - 9s 143us/step - loss: 0.0717 - acc: 0.9784 - val_loss: 0.0918 - val_acc: 0.9725 Epoch 57/60 60000/60000 [==============================] - 9s 151us/step - loss: 0.0717 - acc: 0.9783 - val_loss: 0.0918 - val_acc: 0.9726 Epoch 58/60 60000/60000 [==============================] - 9s 144us/step - loss: 0.0708 - acc: 0.9783 - val_loss: 0.0916 - val_acc: 0.9725 Epoch 59/60 60000/60000 [==============================] - 8s 137us/step - loss: 0.0703 - acc: 0.9782 - val_loss: 0.0916 - val_acc: 0.9731 Epoch 60/60 60000/60000 [==============================] - 8s 137us/step - loss: 0.0703 - acc: 0.9785 - val_loss: 0.0918 - val_acc: 0.9727 CPU times: user 15min 16s, sys: 3min 41s, total: 18min 57s Wall time: 8min 37s In [26]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( np . sqrt ( lr_model_history . history [ 'loss' ]), 'r' , label = 'train' ) ax . plot ( np . sqrt ( lr_model_history . history [ 'val_loss' ]), 'b' , label = 'val' ) ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Loss' , fontsize = 20 ) ax . legend () ax . tick_params ( labelsize = 20 ) In [27]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( np . sqrt ( lr_model_history . history [ 'acc' ]), 'r' , label = 'train' ) ax . plot ( np . sqrt ( lr_model_history . history [ 'val_acc' ]), 'b' , label = 'val' ) ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Accuracy' , fontsize = 20 ) ax . legend () ax . tick_params ( labelsize = 20 ) Exercise 1: Apply a custon learning rate change using LearningRateScheduler Write a function that performs the exponential learning rate decay as indicated by the following formula: \\begin{align} lr = lr0 * e&#94;{(-kt)} \\textrm{} \\end{align} In [28]: # your code here In [29]: # solution epochs = 60 learning_rate = 0.1 # initial learning rate decay_rate = 0.1 momentum = 0.8 # define the optimizer function sgd = SGD ( lr = learning_rate , momentum = momentum , decay = decay_rate , nesterov = False ) In [30]: input_dim = x_train . shape [ 1 ] num_classes = 10 batch_size = 196 # build the model exponential_decay_model = Sequential () exponential_decay_model . add ( Dense ( 64 , activation = tf . nn . relu , kernel_initializer = 'uniform' , input_dim = input_dim )) exponential_decay_model . add ( Dropout ( 0.1 )) exponential_decay_model . add ( Dense ( 64 , kernel_initializer = 'uniform' , activation = tf . nn . relu )) exponential_decay_model . add ( Dense ( num_classes , kernel_initializer = 'uniform' , activation = tf . nn . softmax )) # compile the model exponential_decay_model . compile ( loss = 'categorical_crossentropy' , optimizer = sgd , metrics = [ 'acc' ]) In [31]: # define the learning rate change def exp_decay ( epoch ): lrate = learning_rate * np . exp ( - decay_rate * epoch ) return lrate In [32]: # learning schedule callback loss_history = History () lr_rate = LearningRateScheduler ( exp_decay ) callbacks_list = [ loss_history , lr_rate ] # you invoke the LearningRateScheduler during the .fit() phase exponential_decay_model_history = exponential_decay_model . fit ( x_train , y_train , batch_size = batch_size , epochs = epochs , callbacks = callbacks_list , verbose = 1 , validation_data = ( x_test , y_test )) Train on 60000 samples, validate on 10000 samples Epoch 1/60 60000/60000 [==============================] - 1s 16us/step - loss: 1.9924 - acc: 0.3865 - val_loss: 1.4953 - val_acc: 0.5841 Epoch 2/60 60000/60000 [==============================] - 1s 11us/step - loss: 1.2430 - acc: 0.6362 - val_loss: 1.0153 - val_acc: 0.7164 Epoch 3/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.9789 - acc: 0.7141 - val_loss: 0.8601 - val_acc: 0.7617 Epoch 4/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.8710 - acc: 0.7452 - val_loss: 0.7811 - val_acc: 0.7865 Epoch 5/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.8115 - acc: 0.7609 - val_loss: 0.7336 - val_acc: 0.7968 Epoch 6/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.7749 - acc: 0.7678 - val_loss: 0.7030 - val_acc: 0.8035 Epoch 7/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.7524 - acc: 0.7742 - val_loss: 0.6822 - val_acc: 0.8095 Epoch 8/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.7342 - acc: 0.7788 - val_loss: 0.6673 - val_acc: 0.8122 Epoch 9/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.7218 - acc: 0.7840 - val_loss: 0.6562 - val_acc: 0.8148 Epoch 10/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.7144 - acc: 0.7836 - val_loss: 0.6475 - val_acc: 0.8168 Epoch 11/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.7054 - acc: 0.7857 - val_loss: 0.6408 - val_acc: 0.8175 Epoch 12/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.7008 - acc: 0.7896 - val_loss: 0.6354 - val_acc: 0.8185 Epoch 13/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6950 - acc: 0.7885 - val_loss: 0.6311 - val_acc: 0.8197 Epoch 14/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6921 - acc: 0.7895 - val_loss: 0.6274 - val_acc: 0.8199 Epoch 15/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6888 - acc: 0.7913 - val_loss: 0.6244 - val_acc: 0.8204 Epoch 16/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6833 - acc: 0.7932 - val_loss: 0.6219 - val_acc: 0.8206 Epoch 17/60 60000/60000 [==============================] - 1s 12us/step - loss: 0.6831 - acc: 0.7942 - val_loss: 0.6199 - val_acc: 0.8208 Epoch 18/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6822 - acc: 0.7937 - val_loss: 0.6182 - val_acc: 0.8212 Epoch 19/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6790 - acc: 0.7955 - val_loss: 0.6167 - val_acc: 0.8215 Epoch 20/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6797 - acc: 0.7935 - val_loss: 0.6155 - val_acc: 0.8218 Epoch 21/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6773 - acc: 0.7953 - val_loss: 0.6144 - val_acc: 0.8222 Epoch 22/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6742 - acc: 0.7960 - val_loss: 0.6135 - val_acc: 0.8227 Epoch 23/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6746 - acc: 0.7958 - val_loss: 0.6127 - val_acc: 0.8236 Epoch 24/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6729 - acc: 0.7973 - val_loss: 0.6120 - val_acc: 0.8237 Epoch 25/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6749 - acc: 0.7963 - val_loss: 0.6114 - val_acc: 0.8238 Epoch 26/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6715 - acc: 0.7967 - val_loss: 0.6109 - val_acc: 0.8241 Epoch 27/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6728 - acc: 0.7975 - val_loss: 0.6105 - val_acc: 0.8241 Epoch 28/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6721 - acc: 0.7964 - val_loss: 0.6101 - val_acc: 0.8246 Epoch 29/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6707 - acc: 0.7972 - val_loss: 0.6098 - val_acc: 0.8247 Epoch 30/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6711 - acc: 0.7980 - val_loss: 0.6095 - val_acc: 0.8247 Epoch 31/60 60000/60000 [==============================] - 1s 12us/step - loss: 0.6721 - acc: 0.7960 - val_loss: 0.6092 - val_acc: 0.8248 Epoch 32/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6708 - acc: 0.7981 - val_loss: 0.6090 - val_acc: 0.8249 Epoch 33/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6720 - acc: 0.7968 - val_loss: 0.6088 - val_acc: 0.8249 Epoch 34/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6700 - acc: 0.7973 - val_loss: 0.6086 - val_acc: 0.8250 Epoch 35/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6714 - acc: 0.7979 - val_loss: 0.6084 - val_acc: 0.8250 Epoch 36/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6687 - acc: 0.7980 - val_loss: 0.6083 - val_acc: 0.8250 Epoch 37/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6700 - acc: 0.7963 - val_loss: 0.6082 - val_acc: 0.8250 Epoch 38/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6702 - acc: 0.7964 - val_loss: 0.6081 - val_acc: 0.8252 Epoch 39/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6711 - acc: 0.7963 - val_loss: 0.6080 - val_acc: 0.8250 Epoch 40/60 60000/60000 [==============================] - 1s 12us/step - loss: 0.6698 - acc: 0.7971 - val_loss: 0.6079 - val_acc: 0.8251 Epoch 41/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6696 - acc: 0.7967 - val_loss: 0.6079 - val_acc: 0.8252 Epoch 42/60 60000/60000 [==============================] - 1s 12us/step - loss: 0.6699 - acc: 0.7989 - val_loss: 0.6078 - val_acc: 0.8252 Epoch 43/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6701 - acc: 0.7952 - val_loss: 0.6077 - val_acc: 0.8252 Epoch 44/60 60000/60000 [==============================] - 1s 12us/step - loss: 0.6704 - acc: 0.7973 - val_loss: 0.6077 - val_acc: 0.8252 Epoch 45/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6697 - acc: 0.7971 - val_loss: 0.6076 - val_acc: 0.8252 Epoch 46/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6718 - acc: 0.7969 - val_loss: 0.6076 - val_acc: 0.8252 Epoch 47/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6708 - acc: 0.7965 - val_loss: 0.6076 - val_acc: 0.8252 Epoch 48/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6690 - acc: 0.7968 - val_loss: 0.6075 - val_acc: 0.8252 Epoch 49/60 60000/60000 [==============================] - 1s 12us/step - loss: 0.6700 - acc: 0.7959 - val_loss: 0.6075 - val_acc: 0.8252 Epoch 50/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6722 - acc: 0.7963 - val_loss: 0.6075 - val_acc: 0.8252 Epoch 51/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6698 - acc: 0.7950 - val_loss: 0.6075 - val_acc: 0.8252 Epoch 52/60 60000/60000 [==============================] - 1s 12us/step - loss: 0.6689 - acc: 0.7978 - val_loss: 0.6075 - val_acc: 0.8252 Epoch 53/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6696 - acc: 0.7973 - val_loss: 0.6074 - val_acc: 0.8252 Epoch 54/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6678 - acc: 0.7975 - val_loss: 0.6074 - val_acc: 0.8252 Epoch 55/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6694 - acc: 0.7969 - val_loss: 0.6074 - val_acc: 0.8252 Epoch 56/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6688 - acc: 0.7981 - val_loss: 0.6074 - val_acc: 0.8252 Epoch 57/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6674 - acc: 0.7988 - val_loss: 0.6074 - val_acc: 0.8252 Epoch 58/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6685 - acc: 0.7970 - val_loss: 0.6074 - val_acc: 0.8252 Epoch 59/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6702 - acc: 0.7953 - val_loss: 0.6074 - val_acc: 0.8252 Epoch 60/60 60000/60000 [==============================] - 1s 11us/step - loss: 0.6701 - acc: 0.7965 - val_loss: 0.6074 - val_acc: 0.8252 In [33]: # check on the variables that can show me the learning rate decay exponential_decay_model_history . history . keys () Out[33]: dict_keys(['val_loss', 'val_acc', 'loss', 'acc', 'lr']) In [34]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( exponential_decay_model_history . history [ 'lr' ] , 'r' ) #, label='learn rate') ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Learning Rate' , fontsize = 20 ) #ax.legend() ax . tick_params ( labelsize = 20 ) In [35]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( np . sqrt ( exponential_decay_model_history . history [ 'loss' ]), 'r' , label = 'train' ) ax . plot ( np . sqrt ( exponential_decay_model_history . history [ 'val_loss' ]), 'b' , label = 'val' ) ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Loss' , fontsize = 20 ) ax . legend () ax . tick_params ( labelsize = 20 ) Step 3 - Choosing an optimizer and a loss function When constructing a model and using it to make our predictions, for example to assign label scores to images (\"cat\", \"plane\", etc), we want to measure our success or failure by defining a \"loss\" function (or objective function). The goal of optimization is to efficiently calculate the parameters/weights that minimize this loss function. keras provides various types of loss functions . Sometimes the \"loss\" function measures the \"distance\". We can define this \"distance\" between two data points in various ways suitable to the problem or dataset. Distance Euclidean Manhattan others such as Hamming which measures distances between strings, for example. The Hamming distance of \"carolin\" and \"cathrin\" is 3. Loss functions MSE (for regression) categorical cross-entropy (for classification) binary cross entropy (for classification) In [40]: # build the model input_dim = x_train . shape [ 1 ] model = Sequential () model . add ( Dense ( 64 , activation = tf . nn . relu , kernel_initializer = 'uniform' , input_dim = input_dim )) # fully-connected layer with 64 hidden units model . add ( Dropout ( 0.1 )) model . add ( Dense ( 64 , kernel_initializer = 'uniform' , activation = tf . nn . relu )) model . add ( Dense ( num_classes , kernel_initializer = 'uniform' , activation = tf . nn . softmax )) In [41]: # defining the parameters for RMSprop (I used the keras defaults here) rms = RMSprop ( lr = 0.001 , rho = 0.9 , epsilon = None , decay = 0.0 ) model . compile ( loss = 'categorical_crossentropy' , optimizer = rms , metrics = [ 'acc' ]) Step 4 - Deciding on the batch size and number of epochs In [42]: %%time batch_size = input_dim epochs = 60 model_history = model . fit ( x_train , y_train , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_data = ( x_test , y_test )) Train on 60000 samples, validate on 10000 samples Epoch 1/60 60000/60000 [==============================] - 1s 14us/step - loss: 1.1320 - acc: 0.7067 - val_loss: 0.5628 - val_acc: 0.8237 Epoch 2/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.4831 - acc: 0.8570 - val_loss: 0.3674 - val_acc: 0.8934 Epoch 3/60 60000/60000 [==============================] - 1s 9us/step - loss: 0.3665 - acc: 0.8931 - val_loss: 0.3199 - val_acc: 0.9061 Epoch 4/60 60000/60000 [==============================] - 1s 9us/step - loss: 0.3100 - acc: 0.9092 - val_loss: 0.2664 - val_acc: 0.9233 Epoch 5/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.2699 - acc: 0.9206 - val_loss: 0.2295 - val_acc: 0.9326 Epoch 6/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.2391 - acc: 0.9305 - val_loss: 0.2104 - val_acc: 0.9362 Epoch 7/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.2115 - acc: 0.9383 - val_loss: 0.1864 - val_acc: 0.9459 Epoch 8/60 60000/60000 [==============================] - 1s 9us/step - loss: 0.1900 - acc: 0.9451 - val_loss: 0.1658 - val_acc: 0.9493 Epoch 9/60 60000/60000 [==============================] - 1s 9us/step - loss: 0.1714 - acc: 0.9492 - val_loss: 0.1497 - val_acc: 0.9538 Epoch 10/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.1565 - acc: 0.9539 - val_loss: 0.1404 - val_acc: 0.9591 Epoch 11/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.1443 - acc: 0.9569 - val_loss: 0.1305 - val_acc: 0.9616 Epoch 12/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.1334 - acc: 0.9596 - val_loss: 0.1224 - val_acc: 0.9628 Epoch 13/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.1257 - acc: 0.9627 - val_loss: 0.1133 - val_acc: 0.9660 Epoch 14/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.1169 - acc: 0.9652 - val_loss: 0.1116 - val_acc: 0.9674 Epoch 15/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.1091 - acc: 0.9675 - val_loss: 0.1104 - val_acc: 0.9670 Epoch 16/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.1051 - acc: 0.9689 - val_loss: 0.1030 - val_acc: 0.9692 Epoch 17/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0978 - acc: 0.9697 - val_loss: 0.1044 - val_acc: 0.9686 Epoch 18/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0929 - acc: 0.9718 - val_loss: 0.0996 - val_acc: 0.9689 Epoch 19/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0882 - acc: 0.9738 - val_loss: 0.1035 - val_acc: 0.9695 Epoch 20/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0850 - acc: 0.9737 - val_loss: 0.0941 - val_acc: 0.9717 Epoch 21/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0803 - acc: 0.9751 - val_loss: 0.0953 - val_acc: 0.9715 Epoch 22/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0793 - acc: 0.9762 - val_loss: 0.0898 - val_acc: 0.9729 Epoch 23/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0747 - acc: 0.9775 - val_loss: 0.0901 - val_acc: 0.9732 Epoch 24/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0718 - acc: 0.9778 - val_loss: 0.0948 - val_acc: 0.9720 Epoch 25/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0697 - acc: 0.9781 - val_loss: 0.0908 - val_acc: 0.9727 Epoch 26/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0668 - acc: 0.9794 - val_loss: 0.0917 - val_acc: 0.9726 Epoch 27/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0648 - acc: 0.9800 - val_loss: 0.0895 - val_acc: 0.9737 Epoch 28/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0637 - acc: 0.9798 - val_loss: 0.0868 - val_acc: 0.9728 Epoch 29/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0598 - acc: 0.9813 - val_loss: 0.0883 - val_acc: 0.9736 Epoch 30/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0570 - acc: 0.9820 - val_loss: 0.0869 - val_acc: 0.9741 Epoch 31/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0555 - acc: 0.9825 - val_loss: 0.0896 - val_acc: 0.9732 Epoch 32/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0554 - acc: 0.9827 - val_loss: 0.0843 - val_acc: 0.9743 Epoch 33/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0512 - acc: 0.9836 - val_loss: 0.0843 - val_acc: 0.9746 Epoch 34/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0509 - acc: 0.9835 - val_loss: 0.0868 - val_acc: 0.9753 Epoch 35/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0491 - acc: 0.9842 - val_loss: 0.0841 - val_acc: 0.9755 Epoch 36/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0471 - acc: 0.9848 - val_loss: 0.0887 - val_acc: 0.9728 Epoch 37/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0466 - acc: 0.9850 - val_loss: 0.0876 - val_acc: 0.9756 Epoch 38/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0456 - acc: 0.9856 - val_loss: 0.0833 - val_acc: 0.9769 Epoch 39/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0431 - acc: 0.9866 - val_loss: 0.0869 - val_acc: 0.9759 Epoch 40/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0413 - acc: 0.9869 - val_loss: 0.0926 - val_acc: 0.9743 Epoch 41/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0401 - acc: 0.9872 - val_loss: 0.0851 - val_acc: 0.9756 Epoch 42/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0401 - acc: 0.9876 - val_loss: 0.0856 - val_acc: 0.9764 Epoch 43/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0392 - acc: 0.9870 - val_loss: 0.0861 - val_acc: 0.9771 Epoch 44/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0396 - acc: 0.9870 - val_loss: 0.0918 - val_acc: 0.9756 Epoch 45/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0371 - acc: 0.9883 - val_loss: 0.0866 - val_acc: 0.9766 Epoch 46/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0374 - acc: 0.9883 - val_loss: 0.0888 - val_acc: 0.9748 Epoch 47/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0376 - acc: 0.9878 - val_loss: 0.0850 - val_acc: 0.9761 Epoch 48/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0351 - acc: 0.9890 - val_loss: 0.0848 - val_acc: 0.9777 Epoch 49/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0361 - acc: 0.9884 - val_loss: 0.0850 - val_acc: 0.9771 Epoch 50/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0341 - acc: 0.9887 - val_loss: 0.0889 - val_acc: 0.9769 Epoch 51/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0322 - acc: 0.9897 - val_loss: 0.0882 - val_acc: 0.9771 Epoch 52/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0322 - acc: 0.9895 - val_loss: 0.0892 - val_acc: 0.9762 Epoch 53/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0313 - acc: 0.9892 - val_loss: 0.0916 - val_acc: 0.9771 Epoch 54/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0300 - acc: 0.9897 - val_loss: 0.0913 - val_acc: 0.9772 Epoch 55/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0306 - acc: 0.9902 - val_loss: 0.0904 - val_acc: 0.9763 Epoch 56/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0307 - acc: 0.9900 - val_loss: 0.0910 - val_acc: 0.9777 Epoch 57/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0299 - acc: 0.9901 - val_loss: 0.0918 - val_acc: 0.9763 Epoch 58/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0299 - acc: 0.9906 - val_loss: 0.0914 - val_acc: 0.9778 Epoch 59/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0284 - acc: 0.9909 - val_loss: 0.0907 - val_acc: 0.9769 Epoch 60/60 60000/60000 [==============================] - 0s 8us/step - loss: 0.0283 - acc: 0.9908 - val_loss: 0.0962 - val_acc: 0.9761 CPU times: user 1min 17s, sys: 7.4 s, total: 1min 24s Wall time: 29.3 s In [43]: score = model . evaluate ( x_test , y_test , verbose = 0 ) print ( 'Test loss:' , score [ 0 ]) print ( 'Test accuracy:' , score [ 1 ]) Test loss: 0.09620895183624088 Test accuracy: 0.9761 In [44]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( np . sqrt ( model_history . history [ 'acc' ]), 'r' , label = 'train_acc' ) ax . plot ( np . sqrt ( model_history . history [ 'val_acc' ]), 'b' , label = 'val_acc' ) ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Accuracy' , fontsize = 20 ) ax . legend () ax . tick_params ( labelsize = 20 ) In [45]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( np . sqrt ( model_history . history [ 'loss' ]), 'r' , label = 'train' ) ax . plot ( np . sqrt ( model_history . history [ 'val_loss' ]), 'b' , label = 'val' ) ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Loss' , fontsize = 20 ) ax . legend () ax . tick_params ( labelsize = 20 ) Step 5 - Random restarts This method does not seem to have an implementation in keras . We will leave it as a home exercise! Hint: you can use keras.callbacks.LearningRateScheduler . See how we used it to set a custom learning rate. Tuning the Hyperparameters using Cross Validation Now instead of trying different values by hand, we will use GridSearchCV from Scikit-Learn to try out several values for our hyperparameters and compare the results. To do cross-validation with keras we will use the wrappers for the Scikit-Learn API. They provide a way to use Sequential Keras models (single-input only) as part of your Scikit-Learn workflow. There are two wrappers available: keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params) , which implements the Scikit-Learn classifier interface, keras.wrappers.scikit_learn.KerasRegressor(build_fn=None, **sk_params) , which implements the Scikit-Learn regressor interface. In [46]: import numpy from sklearn.model_selection import GridSearchCV from keras.wrappers.scikit_learn import KerasClassifier Trying different weight initializations In [47]: # let's create a function that creates the model (required for KerasClassifier) # while accepting the hyperparameters we want to tune # we also pass some default values such as optimizer='rmsprop' def create_model ( init_mode = 'uniform' ): # define model model = Sequential () model . add ( Dense ( 64 , kernel_initializer = init_mode , activation = tf . nn . relu , input_dim = 784 )) model . add ( Dropout ( 0.1 )) model . add ( Dense ( 64 , kernel_initializer = init_mode , activation = tf . nn . relu )) model . add ( Dense ( 10 , kernel_initializer = init_mode , activation = tf . nn . softmax )) # compile model model . compile ( loss = 'categorical_crossentropy' , optimizer = RMSprop (), metrics = [ 'accuracy' ]) return model In [49]: %%time seed = 7 numpy . random . seed ( seed ) batch_size = 128 epochs = 10 model_CV = KerasClassifier ( build_fn = create_model , epochs = epochs , batch_size = batch_size , verbose = 1 ) # define the grid search parameters init_mode = [ 'uniform' , 'lecun_uniform' , 'normal' , 'zero' , 'glorot_normal' , 'glorot_uniform' , 'he_normal' , 'he_uniform' ] param_grid = dict ( init_mode = init_mode ) grid = GridSearchCV ( estimator = model_CV , param_grid = param_grid , n_jobs =- 1 , cv = 3 ) grid_result = grid . fit ( x_train , y_train ) Epoch 1/10 60000/60000 [==============================] - 1s 21us/step - loss: 0.4118 - acc: 0.8824 Epoch 2/10 60000/60000 [==============================] - 1s 15us/step - loss: 0.1936 - acc: 0.9437 Epoch 3/10 60000/60000 [==============================] - 1s 14us/step - loss: 0.1482 - acc: 0.9553 Epoch 4/10 60000/60000 [==============================] - 1s 14us/step - loss: 0.1225 - acc: 0.9631 Epoch 5/10 60000/60000 [==============================] - 1s 14us/step - loss: 0.1064 - acc: 0.9676 Epoch 6/10 60000/60000 [==============================] - 1s 14us/step - loss: 0.0944 - acc: 0.9710 Epoch 7/10 60000/60000 [==============================] - 1s 14us/step - loss: 0.0876 - acc: 0.9732 Epoch 8/10 60000/60000 [==============================] - 1s 15us/step - loss: 0.0809 - acc: 0.9745 Epoch 9/10 60000/60000 [==============================] - 1s 14us/step - loss: 0.0741 - acc: 0.9775 Epoch 10/10 60000/60000 [==============================] - 1s 15us/step - loss: 0.0709 - acc: 0.9783 CPU times: user 21 s, sys: 3.56 s, total: 24.5 s Wall time: 1min 20s In [50]: # print results print ( f 'Best Accuracy for {grid_result.best_score_} using {grid_result.best_params_} ' ) means = grid_result . cv_results_ [ 'mean_test_score' ] stds = grid_result . cv_results_ [ 'std_test_score' ] params = grid_result . cv_results_ [ 'params' ] for mean , stdev , param in zip ( means , stds , params ): print ( f ' mean= {mean:.4} , std= {stdev:.4} using {param} ' ) Best Accuracy for 0.9689333333333333 using {'init_mode': 'lecun_uniform'} mean=0.9647, std=0.001438 using {'init_mode': 'uniform'} mean=0.9689, std=0.001044 using {'init_mode': 'lecun_uniform'} mean=0.9651, std=0.001515 using {'init_mode': 'normal'} mean=0.1124, std=0.002416 using {'init_mode': 'zero'} mean=0.9657, std=0.0005104 using {'init_mode': 'glorot_normal'} mean=0.9687, std=0.0008436 using {'init_mode': 'glorot_uniform'} mean=0.9681, std=0.002145 using {'init_mode': 'he_normal'} mean=0.9685, std=0.001952 using {'init_mode': 'he_uniform'} Save Your Neural Network Model to JSON The Hierarchical Data Format (HDF5) is a data storage format for storing large arrays of data including values for the weights in a neural network. You can install HDF5 Python module: pip install h5py Keras gives you the ability to describe and save any model using the JSON format. In [51]: from keras.models import model_from_json # serialize model to JSON model_json = model . to_json () with open ( \"model.json\" , \"w\" ) as json_file : json_file . write ( model_json ) # save weights to HDF5 model . save_weights ( \"model.h5\" ) print ( \"Model saved\" ) # when you want to retrieve the model: load json and create model json_file = open ( 'model.json' , 'r' ) saved_model = json_file . read () # close the file as good practice json_file . close () model_from_json = model_from_json ( saved_model ) # load weights into new model model_from_json . load_weights ( \"model.h5\" ) print ( \"Model loaded\" ) Model saved Model loaded Exercise 2: Cross-validation with more than one hyperparameters We can do cross-validation with more than one parameters simultaneously, effectively trying out combinations of them. Note: Cross-validation in neural networks is computationally expensive . Think before you experiment! Multiply the number of features you are validating on to see how many combinations there are. Each combination is evaluated using the cv-fold cross-validation (cv is a parameter we choose). For example, we can choose to search for different values of: batch size, number of epochs and initialization mode. The choises are specifed into a dictionary and passed to GridSearchCV. Perform a GridSearch for batch size , number of epochs and initializer combined. In [52]: #your code here In [53]: # solutions # repeat some of the initial values here so we make sure they were not changed input_dim = x_train . shape [ 1 ] num_classes = 10 # let's create a function that creates the model (required for KerasClassifier) # while accepting the hyperparameters we want to tune # we also pass some default values such as optimizer='rmsprop' def create_model_2 ( optimizer = 'rmsprop' , init = 'glorot_uniform' ): model = Sequential () model . add ( Dense ( 64 , input_dim = input_dim , kernel_initializer = init , activation = 'relu' )) model . add ( Dropout ( 0.1 )) model . add ( Dense ( 64 , kernel_initializer = init , activation = tf . nn . relu )) model . add ( Dense ( num_classes , kernel_initializer = init , activation = tf . nn . softmax )) # compile model model . compile ( loss = 'categorical_crossentropy' , optimizer = optimizer , metrics = [ 'accuracy' ]) return model In [54]: %%time # fix random seed for reproducibility (this might work or might not work # depending on each library's implenentation) seed = 7 numpy . random . seed ( seed ) # create the sklearn model for the network model_init_batch_epoch_CV = KerasClassifier ( build_fn = create_model_2 , verbose = 1 ) # we choose the initializers that came at the top in our previous cross-validation!! init_mode = [ 'glorot_uniform' , 'uniform' ] batches = [ 128 , 512 ] epochs = [ 10 , 20 ] # grid search for initializer, batch size and number of epochs param_grid = dict ( epochs = epochs , batch_size = batches , init = init_mode ) grid = GridSearchCV ( estimator = model_init_batch_epoch_CV , param_grid = param_grid , cv = 3 ) grid_result = grid . fit ( x_train , y_train ) Epoch 1/10 40000/40000 [==============================] - 1s 21us/step - loss: 0.4801 - acc: 0.8601 Epoch 2/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.2309 - acc: 0.9310 Epoch 3/10 40000/40000 [==============================] - 1s 14us/step - loss: 0.1744 - acc: 0.9479 Epoch 4/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1422 - acc: 0.9575 Epoch 5/10 40000/40000 [==============================] - 1s 14us/step - loss: 0.1214 - acc: 0.9625 Epoch 6/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1081 - acc: 0.9675 Epoch 7/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.0974 - acc: 0.9693 Epoch 8/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.0874 - acc: 0.9730 Epoch 9/10 40000/40000 [==============================] - 1s 14us/step - loss: 0.0800 - acc: 0.9750 Epoch 10/10 40000/40000 [==============================] - 1s 14us/step - loss: 0.0750 - acc: 0.9765 20000/20000 [==============================] - 0s 10us/step 40000/40000 [==============================] - 0s 6us/step Epoch 1/10 40000/40000 [==============================] - 1s 22us/step - loss: 0.4746 - acc: 0.8656 Epoch 2/10 40000/40000 [==============================] - 1s 14us/step - loss: 0.2264 - acc: 0.9336 Epoch 3/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1734 - acc: 0.9487 Epoch 4/10 40000/40000 [==============================] - 1s 14us/step - loss: 0.1436 - acc: 0.9568 Epoch 5/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1256 - acc: 0.9614 Epoch 6/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1104 - acc: 0.9660 Epoch 7/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.0973 - acc: 0.9707 Epoch 8/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.0870 - acc: 0.9733 Epoch 9/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.0818 - acc: 0.9748 Epoch 10/10 40000/40000 [==============================] - 1s 14us/step - loss: 0.0730 - acc: 0.9770 20000/20000 [==============================] - 0s 10us/step 40000/40000 [==============================] - 0s 6us/step Epoch 1/10 40000/40000 [==============================] - 1s 23us/step - loss: 0.4639 - acc: 0.8671 Epoch 2/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.2208 - acc: 0.9344 Epoch 3/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1693 - acc: 0.9491 Epoch 4/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1388 - acc: 0.9580 Epoch 5/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1206 - acc: 0.9634 Epoch 6/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1062 - acc: 0.9678 Epoch 7/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.0956 - acc: 0.9711 Epoch 8/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.0870 - acc: 0.9728 Epoch 9/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.0794 - acc: 0.9750 Epoch 10/10 40000/40000 [==============================] - 1s 14us/step - loss: 0.0748 - acc: 0.9774 20000/20000 [==============================] - 0s 11us/step 40000/40000 [==============================] - 0s 7us/step Epoch 1/10 40000/40000 [==============================] - 1s 23us/step - loss: 0.7144 - acc: 0.7894 Epoch 2/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.3246 - acc: 0.9045 Epoch 3/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.2482 - acc: 0.9268 Epoch 4/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.2005 - acc: 0.9407 Epoch 5/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1673 - acc: 0.9485 Epoch 6/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1462 - acc: 0.9559 Epoch 7/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1305 - acc: 0.9604 Epoch 8/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1166 - acc: 0.9643 Epoch 9/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1079 - acc: 0.9675 Epoch 10/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.0983 - acc: 0.9695 20000/20000 [==============================] - 0s 12us/step 40000/40000 [==============================] - 0s 6us/step Epoch 1/10 40000/40000 [==============================] - 1s 24us/step - loss: 0.6894 - acc: 0.7944 Epoch 2/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.3171 - acc: 0.9061 Epoch 3/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.2358 - acc: 0.9312 Epoch 4/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1911 - acc: 0.9422 Epoch 5/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1595 - acc: 0.9526 Epoch 6/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1401 - acc: 0.9579 Epoch 7/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1230 - acc: 0.9636 Epoch 8/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1096 - acc: 0.9672 Epoch 9/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1027 - acc: 0.9692 Epoch 10/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.0936 - acc: 0.9718 20000/20000 [==============================] - 0s 12us/step 40000/40000 [==============================] - 0s 7us/step Epoch 1/10 40000/40000 [==============================] - 1s 24us/step - loss: 0.7028 - acc: 0.7976 Epoch 2/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.3189 - acc: 0.9055 Epoch 3/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.2390 - acc: 0.9307 Epoch 4/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1910 - acc: 0.9435 Epoch 5/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1595 - acc: 0.9528 Epoch 6/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1376 - acc: 0.9583 Epoch 7/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1213 - acc: 0.9629 Epoch 8/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.1100 - acc: 0.9664 Epoch 9/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.0987 - acc: 0.9697 Epoch 10/10 40000/40000 [==============================] - 1s 15us/step - loss: 0.0932 - acc: 0.9714 20000/20000 [==============================] - 0s 13us/step 40000/40000 [==============================] - 0s 7us/step Epoch 1/20 40000/40000 [==============================] - 1s 25us/step - loss: 0.4938 - acc: 0.8589 Epoch 2/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.2286 - acc: 0.9324 Epoch 3/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1751 - acc: 0.9470 Epoch 4/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1458 - acc: 0.9553 Epoch 5/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1237 - acc: 0.9620 Epoch 6/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1106 - acc: 0.9669 Epoch 7/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0998 - acc: 0.9696 Epoch 8/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0898 - acc: 0.9729 Epoch 9/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0799 - acc: 0.9749 Epoch 10/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0746 - acc: 0.9769 Epoch 11/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0688 - acc: 0.9783 Epoch 12/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0655 - acc: 0.9792 Epoch 13/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0619 - acc: 0.9801 Epoch 14/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0574 - acc: 0.9814 Epoch 15/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0549 - acc: 0.9836 Epoch 16/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0512 - acc: 0.9837 Epoch 17/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0464 - acc: 0.9855 Epoch 18/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0466 - acc: 0.9850 Epoch 19/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0431 - acc: 0.9863 Epoch 20/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0430 - acc: 0.9861 20000/20000 [==============================] - 0s 13us/step 40000/40000 [==============================] - 0s 7us/step Epoch 1/20 40000/40000 [==============================] - 1s 25us/step - loss: 0.4956 - acc: 0.8584 Epoch 2/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.2286 - acc: 0.9321 Epoch 3/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1746 - acc: 0.9474 Epoch 4/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1438 - acc: 0.9574 Epoch 5/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1232 - acc: 0.9634 Epoch 6/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1104 - acc: 0.9665 Epoch 7/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0986 - acc: 0.9696 Epoch 8/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0886 - acc: 0.9723 Epoch 9/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0828 - acc: 0.9741 Epoch 10/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0763 - acc: 0.9768 Epoch 11/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0679 - acc: 0.9781 Epoch 12/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0656 - acc: 0.9788 Epoch 13/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0601 - acc: 0.9810 Epoch 14/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0571 - acc: 0.9817 Epoch 15/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0517 - acc: 0.9832 Epoch 16/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0509 - acc: 0.9834 Epoch 17/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0478 - acc: 0.9850 Epoch 18/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0461 - acc: 0.9852 Epoch 19/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0442 - acc: 0.9854 Epoch 20/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0427 - acc: 0.9866 20000/20000 [==============================] - 0s 14us/step 40000/40000 [==============================] - 0s 7us/step Epoch 1/20 40000/40000 [==============================] - 1s 27us/step - loss: 0.4694 - acc: 0.8670 Epoch 2/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.2196 - acc: 0.9336 Epoch 3/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1681 - acc: 0.9495 Epoch 4/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1391 - acc: 0.9589 Epoch 5/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1200 - acc: 0.9630 Epoch 6/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1067 - acc: 0.9671 Epoch 7/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0968 - acc: 0.9713 Epoch 8/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0880 - acc: 0.9730 Epoch 9/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0815 - acc: 0.9754 Epoch 10/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0742 - acc: 0.9771 Epoch 11/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0711 - acc: 0.9778 Epoch 12/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0643 - acc: 0.9806 Epoch 13/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0613 - acc: 0.9815 Epoch 14/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0561 - acc: 0.9818 Epoch 15/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0534 - acc: 0.9832 Epoch 16/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0528 - acc: 0.9832 Epoch 17/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0479 - acc: 0.9848 Epoch 18/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0486 - acc: 0.9844 Epoch 19/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0438 - acc: 0.9861 Epoch 20/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0427 - acc: 0.9861 20000/20000 [==============================] - 0s 14us/step 40000/40000 [==============================] - 0s 7us/step Epoch 1/20 40000/40000 [==============================] - 1s 27us/step - loss: 0.6830 - acc: 0.8003 Epoch 2/20 40000/40000 [==============================] - 1s 16us/step - loss: 0.3234 - acc: 0.9044 Epoch 3/20 40000/40000 [==============================] - 1s 16us/step - loss: 0.2456 - acc: 0.9269 Epoch 4/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1982 - acc: 0.9407 Epoch 5/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1627 - acc: 0.9506 Epoch 6/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1422 - acc: 0.9561 Epoch 7/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1251 - acc: 0.9618 Epoch 8/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1134 - acc: 0.9650 Epoch 9/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1037 - acc: 0.9677 Epoch 10/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0957 - acc: 0.9705 Epoch 11/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0891 - acc: 0.9730 Epoch 12/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0832 - acc: 0.9745 Epoch 13/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0760 - acc: 0.9768 Epoch 14/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0727 - acc: 0.9778 Epoch 15/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0681 - acc: 0.9782 Epoch 16/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0628 - acc: 0.9803 Epoch 17/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0613 - acc: 0.9801 Epoch 18/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0572 - acc: 0.9824 Epoch 19/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0549 - acc: 0.9821 Epoch 20/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0521 - acc: 0.9834 20000/20000 [==============================] - 0s 15us/step 40000/40000 [==============================] - 0s 7us/step Epoch 1/20 40000/40000 [==============================] - 1s 28us/step - loss: 0.6742 - acc: 0.8034 Epoch 2/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.3142 - acc: 0.9081 Epoch 3/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.2365 - acc: 0.9306 Epoch 4/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1873 - acc: 0.9453 Epoch 5/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1563 - acc: 0.9531 Epoch 6/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1373 - acc: 0.9580 Epoch 7/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1212 - acc: 0.9640 Epoch 8/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1084 - acc: 0.9665 Epoch 9/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1014 - acc: 0.9695 Epoch 10/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0917 - acc: 0.9732 Epoch 11/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0838 - acc: 0.9739 Epoch 12/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0788 - acc: 0.9749 Epoch 13/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0720 - acc: 0.9779 Epoch 14/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0685 - acc: 0.9793 Epoch 15/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0666 - acc: 0.9797 Epoch 16/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0618 - acc: 0.9807 Epoch 17/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0578 - acc: 0.9819 Epoch 18/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0557 - acc: 0.9825 Epoch 19/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0535 - acc: 0.9831 Epoch 20/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0482 - acc: 0.9848 20000/20000 [==============================] - 0s 15us/step 40000/40000 [==============================] - 0s 7us/step Epoch 1/20 40000/40000 [==============================] - 1s 29us/step - loss: 0.6865 - acc: 0.8001 Epoch 2/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.3089 - acc: 0.9106 Epoch 3/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.2292 - acc: 0.9304 Epoch 4/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1845 - acc: 0.9435 Epoch 5/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1562 - acc: 0.9527 Epoch 6/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1371 - acc: 0.9584 Epoch 7/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1206 - acc: 0.9629 Epoch 8/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.1098 - acc: 0.9667 Epoch 9/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0990 - acc: 0.9699 Epoch 10/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0912 - acc: 0.9718 Epoch 11/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0863 - acc: 0.9740 Epoch 12/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0792 - acc: 0.9757 Epoch 13/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0721 - acc: 0.9782 Epoch 14/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0696 - acc: 0.9782 Epoch 15/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0661 - acc: 0.9796 Epoch 16/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0621 - acc: 0.9810 Epoch 17/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0588 - acc: 0.9813 Epoch 18/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0541 - acc: 0.9831 Epoch 19/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0513 - acc: 0.9834 Epoch 20/20 40000/40000 [==============================] - 1s 15us/step - loss: 0.0514 - acc: 0.9835 20000/20000 [==============================] - 0s 16us/step 40000/40000 [==============================] - 0s 7us/step Epoch 1/10 40000/40000 [==============================] - 1s 22us/step - loss: 0.7656 - acc: 0.7926 Epoch 2/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.3345 - acc: 0.9021 Epoch 3/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2633 - acc: 0.9225 Epoch 4/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2207 - acc: 0.9357 Epoch 5/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1877 - acc: 0.9450 Epoch 6/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1684 - acc: 0.9500 Epoch 7/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1517 - acc: 0.9552 Epoch 8/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1358 - acc: 0.9587 Epoch 9/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1236 - acc: 0.9627 Epoch 10/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1153 - acc: 0.9661 20000/20000 [==============================] - 0s 14us/step 40000/40000 [==============================] - 0s 4us/step Epoch 1/10 40000/40000 [==============================] - 1s 22us/step - loss: 0.7640 - acc: 0.7905 Epoch 2/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.3207 - acc: 0.9055 Epoch 3/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2455 - acc: 0.9276 Epoch 4/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2050 - acc: 0.9392 Epoch 5/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1755 - acc: 0.9484 Epoch 6/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1573 - acc: 0.9524 Epoch 7/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1385 - acc: 0.9594 Epoch 8/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1247 - acc: 0.9634 Epoch 9/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1181 - acc: 0.9644 Epoch 10/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1074 - acc: 0.9679 20000/20000 [==============================] - 0s 15us/step 40000/40000 [==============================] - 0s 4us/step Epoch 1/10 40000/40000 [==============================] - 1s 23us/step - loss: 0.7858 - acc: 0.7867 Epoch 2/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.3358 - acc: 0.9017 Epoch 3/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2603 - acc: 0.9250 Epoch 4/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2160 - acc: 0.9367 Epoch 5/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1879 - acc: 0.9435 Epoch 6/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1663 - acc: 0.9513 Epoch 7/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1493 - acc: 0.9552 Epoch 8/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1363 - acc: 0.9591 Epoch 9/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1262 - acc: 0.9609 Epoch 10/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1147 - acc: 0.9653 20000/20000 [==============================] - 0s 16us/step 40000/40000 [==============================] - 0s 4us/step Epoch 1/10 40000/40000 [==============================] - 1s 23us/step - loss: 1.1193 - acc: 0.6932 Epoch 2/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.4829 - acc: 0.8573 Epoch 3/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.3785 - acc: 0.8885 Epoch 4/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.3208 - acc: 0.9062 Epoch 5/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2811 - acc: 0.9167 Epoch 6/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2472 - acc: 0.9263 Epoch 7/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2200 - acc: 0.9362 Epoch 8/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1963 - acc: 0.9422 Epoch 9/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1772 - acc: 0.9475 Epoch 10/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1611 - acc: 0.9519 20000/20000 [==============================] - 0s 16us/step 40000/40000 [==============================] - 0s 4us/step Epoch 1/10 40000/40000 [==============================] - 1s 24us/step - loss: 1.1189 - acc: 0.6828 Epoch 2/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.4867 - acc: 0.8556 Epoch 3/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.3713 - acc: 0.8919 Epoch 4/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.3125 - acc: 0.9090 Epoch 5/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2748 - acc: 0.9214 Epoch 6/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2443 - acc: 0.9285 Epoch 7/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2141 - acc: 0.9383 Epoch 8/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1919 - acc: 0.9445 Epoch 9/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1767 - acc: 0.9485 Epoch 10/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1617 - acc: 0.9528 20000/20000 [==============================] - 0s 17us/step 40000/40000 [==============================] - 0s 4us/step Epoch 1/10 40000/40000 [==============================] - 1s 25us/step - loss: 1.0997 - acc: 0.7128 Epoch 2/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.4652 - acc: 0.8638 Epoch 3/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.3716 - acc: 0.8919 Epoch 4/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.3165 - acc: 0.9072 Epoch 5/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2735 - acc: 0.9199 Epoch 6/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2381 - acc: 0.9299 Epoch 7/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.2094 - acc: 0.9380 Epoch 8/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1863 - acc: 0.9442 Epoch 9/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1693 - acc: 0.9491 Epoch 10/10 40000/40000 [==============================] - 0s 8us/step - loss: 0.1549 - acc: 0.9538 20000/20000 [==============================] - 0s 17us/step 40000/40000 [==============================] - 0s 4us/step Epoch 1/20 40000/40000 [==============================] - 1s 26us/step - loss: 0.7406 - acc: 0.7936 Epoch 2/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.3331 - acc: 0.9019 Epoch 3/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2623 - acc: 0.9229 Epoch 4/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2174 - acc: 0.9372 Epoch 5/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1876 - acc: 0.9436 Epoch 6/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1659 - acc: 0.9498 Epoch 7/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1493 - acc: 0.9559 Epoch 8/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1358 - acc: 0.9602 Epoch 9/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1244 - acc: 0.9624 Epoch 10/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1135 - acc: 0.9655 Epoch 11/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1063 - acc: 0.9683 Epoch 12/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0963 - acc: 0.9711 Epoch 13/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0924 - acc: 0.9720 Epoch 14/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0897 - acc: 0.9732 Epoch 15/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0811 - acc: 0.9752 Epoch 16/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0798 - acc: 0.9750 Epoch 17/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0733 - acc: 0.9770 Epoch 18/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0695 - acc: 0.9789 Epoch 19/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0652 - acc: 0.9798 Epoch 20/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0622 - acc: 0.9815 20000/20000 [==============================] - 0s 18us/step 40000/40000 [==============================] - 0s 4us/step Epoch 1/20 40000/40000 [==============================] - 1s 27us/step - loss: 0.7349 - acc: 0.8012 Epoch 2/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.3201 - acc: 0.9060 Epoch 3/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2541 - acc: 0.9257 Epoch 4/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2149 - acc: 0.9375 Epoch 5/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1894 - acc: 0.9443 Epoch 6/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1694 - acc: 0.9496 Epoch 7/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1552 - acc: 0.9541 Epoch 8/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1421 - acc: 0.9580 Epoch 9/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1307 - acc: 0.9605 Epoch 10/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1200 - acc: 0.9642 Epoch 11/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1110 - acc: 0.9663 Epoch 12/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1041 - acc: 0.9681 Epoch 13/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0971 - acc: 0.9698 Epoch 14/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0943 - acc: 0.9713 Epoch 15/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0864 - acc: 0.9733 Epoch 16/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0809 - acc: 0.9748 Epoch 17/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0776 - acc: 0.9762 Epoch 18/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0745 - acc: 0.9762 Epoch 19/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0712 - acc: 0.9777 Epoch 20/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0654 - acc: 0.9793 20000/20000 [==============================] - 0s 18us/step 40000/40000 [==============================] - 0s 4us/step Epoch 1/20 40000/40000 [==============================] - 1s 27us/step - loss: 0.7435 - acc: 0.8001 Epoch 2/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.3335 - acc: 0.9029 Epoch 3/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2555 - acc: 0.9254 Epoch 4/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2168 - acc: 0.9359 Epoch 5/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1872 - acc: 0.9458 Epoch 6/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1640 - acc: 0.9514 Epoch 7/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1452 - acc: 0.9578 Epoch 8/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1313 - acc: 0.9607 Epoch 9/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1193 - acc: 0.9644 Epoch 10/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1104 - acc: 0.9670 Epoch 11/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1024 - acc: 0.9691 Epoch 12/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0955 - acc: 0.9715 Epoch 13/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0881 - acc: 0.9734 Epoch 14/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0815 - acc: 0.9756 Epoch 15/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0772 - acc: 0.9764 Epoch 16/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0745 - acc: 0.9780 Epoch 17/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0681 - acc: 0.9788 Epoch 18/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0649 - acc: 0.9806 Epoch 19/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0640 - acc: 0.9800 Epoch 20/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0604 - acc: 0.9813 20000/20000 [==============================] - 0s 19us/step 40000/40000 [==============================] - 0s 4us/step Epoch 1/20 40000/40000 [==============================] - 1s 28us/step - loss: 1.1126 - acc: 0.6943 Epoch 2/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.4792 - acc: 0.8568 Epoch 3/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.3712 - acc: 0.8910 Epoch 4/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.3155 - acc: 0.9068 Epoch 5/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2746 - acc: 0.9196 Epoch 6/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2392 - acc: 0.9293 Epoch 7/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2126 - acc: 0.9363 Epoch 8/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1923 - acc: 0.9437 Epoch 9/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1706 - acc: 0.9489 Epoch 10/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1591 - acc: 0.9530 Epoch 11/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1462 - acc: 0.9560 Epoch 12/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1342 - acc: 0.9593 Epoch 13/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1272 - acc: 0.9609 Epoch 14/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1188 - acc: 0.9639 Epoch 15/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1126 - acc: 0.9660 Epoch 16/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1072 - acc: 0.9673 Epoch 17/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1016 - acc: 0.9692 Epoch 18/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0971 - acc: 0.9697 Epoch 19/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0901 - acc: 0.9719 Epoch 20/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0869 - acc: 0.9729 20000/20000 [==============================] - 0s 19us/step 40000/40000 [==============================] - 0s 4us/step Epoch 1/20 40000/40000 [==============================] - 1s 28us/step - loss: 1.0906 - acc: 0.7216 Epoch 2/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.4527 - acc: 0.8658 Epoch 3/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.3559 - acc: 0.8956 Epoch 4/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.3055 - acc: 0.9109 Epoch 5/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2642 - acc: 0.9219 Epoch 6/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2333 - acc: 0.9317 Epoch 7/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2057 - acc: 0.9386 Epoch 8/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1858 - acc: 0.9462 Epoch 9/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1674 - acc: 0.9501 Epoch 10/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1514 - acc: 0.9548 Epoch 11/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1410 - acc: 0.9571 Epoch 12/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1303 - acc: 0.9607 Epoch 13/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1193 - acc: 0.9643 Epoch 14/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1131 - acc: 0.9649 Epoch 15/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1054 - acc: 0.9680 Epoch 16/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0999 - acc: 0.9696 Epoch 17/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0937 - acc: 0.9712 Epoch 18/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0895 - acc: 0.9731 Epoch 19/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0830 - acc: 0.9758 Epoch 20/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0790 - acc: 0.9760 20000/20000 [==============================] - 0s 20us/step 40000/40000 [==============================] - 0s 4us/step Epoch 1/20 40000/40000 [==============================] - 1s 29us/step - loss: 1.1233 - acc: 0.6955 Epoch 2/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.4793 - acc: 0.8588 Epoch 3/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.3751 - acc: 0.8898 Epoch 4/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.3203 - acc: 0.9069 Epoch 5/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2760 - acc: 0.9196 Epoch 6/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2422 - acc: 0.9286 Epoch 7/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.2155 - acc: 0.9363 Epoch 8/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1910 - acc: 0.9437 Epoch 9/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1718 - acc: 0.9489 Epoch 10/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1584 - acc: 0.9530 Epoch 11/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1435 - acc: 0.9570 Epoch 12/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1326 - acc: 0.9603 Epoch 13/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1244 - acc: 0.9615 Epoch 14/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1171 - acc: 0.9644 Epoch 15/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1093 - acc: 0.9670 Epoch 16/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.1047 - acc: 0.9692 Epoch 17/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0984 - acc: 0.9698 Epoch 18/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0910 - acc: 0.9720 Epoch 19/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0860 - acc: 0.9734 Epoch 20/20 40000/40000 [==============================] - 0s 8us/step - loss: 0.0827 - acc: 0.9748 20000/20000 [==============================] - 0s 21us/step 40000/40000 [==============================] - 0s 4us/step Epoch 1/20 60000/60000 [==============================] - 2s 31us/step - loss: 0.4007 - acc: 0.8851 Epoch 2/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.1892 - acc: 0.9439 Epoch 3/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.1432 - acc: 0.9567 Epoch 4/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.1185 - acc: 0.9643 Epoch 5/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.1050 - acc: 0.9678 Epoch 6/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0929 - acc: 0.9715 Epoch 7/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0848 - acc: 0.9737 Epoch 8/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0775 - acc: 0.9764 Epoch 9/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0723 - acc: 0.9780 Epoch 10/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0693 - acc: 0.9785 Epoch 11/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0637 - acc: 0.9802 Epoch 12/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0602 - acc: 0.9814 Epoch 13/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0585 - acc: 0.9816 Epoch 14/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0546 - acc: 0.9827 Epoch 15/20 60000/60000 [==============================] - 1s 18us/step - loss: 0.0512 - acc: 0.9834 Epoch 16/20 60000/60000 [==============================] - 1s 18us/step - loss: 0.0501 - acc: 0.9841 Epoch 17/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0481 - acc: 0.9844 Epoch 18/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0456 - acc: 0.9860 Epoch 19/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0444 - acc: 0.9857 Epoch 20/20 60000/60000 [==============================] - 1s 17us/step - loss: 0.0439 - acc: 0.9861 CPU times: user 7min 56s, sys: 1min 6s, total: 9min 3s Wall time: 3min 43s In [55]: # print results print ( f 'Best Accuracy for {grid_result.best_score_:.4} using {grid_result.best_params_} ' ) means = grid_result . cv_results_ [ 'mean_test_score' ] stds = grid_result . cv_results_ [ 'std_test_score' ] params = grid_result . cv_results_ [ 'params' ] for mean , stdev , param in zip ( means , stds , params ): print ( f 'mean= {mean:.4} , std= {stdev:.4} using {param} ' ) Best Accuracy for 0.9712 using {'batch_size': 128, 'epochs': 20, 'init': 'glorot_uniform'} mean=0.9687, std=0.002174 using {'batch_size': 128, 'epochs': 10, 'init': 'glorot_uniform'} mean=0.966, std=0.000827 using {'batch_size': 128, 'epochs': 10, 'init': 'uniform'} mean=0.9712, std=0.0006276 using {'batch_size': 128, 'epochs': 20, 'init': 'glorot_uniform'} mean=0.97, std=0.001214 using {'batch_size': 128, 'epochs': 20, 'init': 'uniform'} mean=0.9594, std=0.001476 using {'batch_size': 512, 'epochs': 10, 'init': 'glorot_uniform'} mean=0.9516, std=0.003239 using {'batch_size': 512, 'epochs': 10, 'init': 'uniform'} mean=0.9684, std=0.003607 using {'batch_size': 512, 'epochs': 20, 'init': 'glorot_uniform'} mean=0.9633, std=0.0007962 using {'batch_size': 512, 'epochs': 20, 'init': 'uniform'} if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"pages","url":"pages/lab3/solutions/"},{"title":"Lab 3: Optimization of Neural Networks","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS-109B Introduction to Data Science Lab 3: Optimization in Artificial Neural Networks Harvard University Spring 2019 Lab instructor Eleni Kaxiras Instructors: Pavlos Protopapas and Mark Glickman In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: blockquote { background: #AEDE94; } h1 { padding-top: 25px; padding-bottom: 25px; text-align: left; padding-left: 10px; background-color: #DDDDDD; color: black; } h2 { padding-top: 10px; padding-bottom: 10px; text-align: left; padding-left: 5px; background-color: #EEEEEE; color: black; } div.exercise { background-color: #ffcccc; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; } div.discussion { background-color: #ccffcc; border-color: #88E97A; border-left: 5px solid #0A8000; padding: 0.5em; } div.theme { background-color: #DDDDDD; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; font-size: 18pt; } div.gc { background-color: #AEDE94; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; font-size: 12pt; } p.q1 { padding-top: 5px; padding-bottom: 5px; text-align: left; padding-left: 5px; background-color: #EEEEEE; color: black; } header { padding-top: 35px; padding-bottom: 35px; text-align: left; padding-left: 10px; background-color: #DDDDDD; color: black; } Learning Goals In this lab, we'll explore ways to optimize the loss function of a Multilayer Learning Perceptor (MLP) by tuning the model hyperparameters. We'll also explore the use of cross-validation as a technique for checking potential values for these hyperparameters. By the end of this lab, you should: Be familiar with the use of sklearn 's optimize function. Be able to identify the hyperparameters that go into the training of a MLP. Be familiar with the implementation in keras of various optimization techniques. Apply cross-validation to check for multiple values of hyperparameters. In [2]: import matplotlib.pyplot as plt import numpy as np from scipy.optimize import minimize % matplotlib inline Part 1: Beale's function First let's look at function optimization in scipy.optimize , using Beale's function as an example Optimizing a function $f: A\\rightarrow R$, from some set A to the real numbers is finding an element $x_0\\,\\epsilon\\, A$ such that $f(x_0)\\leq f(x)$ for all $x\\,\\epsilon\\, A$ (finding the minimum) or such that $f(x_0)\\geq f(x)$ for all $x\\,\\epsilon\\, A$ (finding the maximum). To illustrate our point we will use a function of two parameters. Our goal is to optimize over these 2 parameters. We can extend to higher dimensions by plotting pairs of parameters against each other. The Wikipedia article on Test functions for optimization has a few functions that are useful for evaluating optimization algorithms. Here is Beale's function: $f(x,y)$ = $(1.5−x+xy)&#94;2+(2.25−x+xy&#94;2)&#94;2+(2.625−x+xy&#94;3)&#94;2$ [source: Wikipedia ] We already know that this function has a minimum at [3.0, 0.5]. Let's see if scipy will find it. In [3]: # define Beale's function which we want to minimize def objective ( X ): x = X [ 0 ]; y = X [ 1 ] return ( 1.5 - x + x * y ) ** 2 + ( 2.25 - x + x * y ** 2 ) ** 2 + ( 2.625 - x + x * y ** 3 ) ** 2 In [4]: # function boundaries xmin , xmax , xstep = - 4.5 , 4.5 , . 9 ymin , ymax , ystep = - 4.5 , 4.5 , . 9 In [5]: # Let's create some points x1 , y1 = np . meshgrid ( np . arange ( xmin , xmax + xstep , xstep ), np . arange ( ymin , ymax + ystep , ystep )) Let's make an initial guess In [6]: # initial guess x0 = [ 4. , 4. ] f0 = objective ( x0 ) print ( f0 ) 68891.203125 In [7]: bnds = (( xmin , xmax ), ( ymin , ymax )) minimum = minimize ( objective , x0 , bounds = bnds ) In [8]: print ( minimum ) fun: 2.068025638865627e-12 hess_inv: <2x2 LbfgsInvHessProduct with dtype=float64> jac: array([-1.55969780e-06, 9.89837957e-06]) message: b'CONVERGENCE: NORM_OF_PROJECTED_GRADIENT_<=_PGTOL' nfev: 60 nit: 14 status: 0 success: True x: array([3.00000257, 0.50000085]) In [9]: real_min = [ 3.0 , 0.5 ] print ( f 'The answer, {minimum.x} , is very close to the optimum as we know it, which is {real_min} ' ) print ( f 'The value of the objective for {real_min} is {objective(real_min)}' ) The answer, [3.00000257 0.50000085], is very close to the optimum as we know it, which is [3.0, 0.5] The value of the objective for [3.0, 0.5] is 0.0 Part 2: Optimization in neural networks In general: Learning Representation --> Objective function --> Optimization algorithm A neural network can be defined as a framework that combines inputs and tries to guess the output. If we are lucky enough to have some results, called \"the ground truth\", to compare the outputs produced by the network, we can calculate the error . So the network guesses, calculates some error function, guesses again, trying to minimize this error, guesses again, until the error does not go down any more. This is optimization. In neural networks the most common used optimization algorithms, are flavors of GD (gradient descent) . The objective function used in gradient descent is the loss function which we want to minimize . A keras Refresher Keras is a Python library for deep learning that can run on top of both Theano or TensorFlow, two powerful Python libraries for fast numerical computing created and released by Facebook and Google, respectevely. Keras was developed to make developing deep learning models as fast and easy as possible for research and practical applications. It runs on Python 2.7 or 3.5 and can seamlessly execute on GPUs and CPUs. Keras is built on the idea of a model. At its core we have a sequence of layers called the Sequential model which is a linear stack of layers. Keras also provides the functional API , a way to define complex models, such as multi-output models, directed acyclic graphs, or models with shared layers. We can summarize the construction of deep learning models in Keras using the Sequential model as follows: Define your model : create a Sequential model and add layers. Compile your model : specify loss function and optimizers and call the .compile() function. Fit your model : train the model on data by calling the .fit() function. Make predictions : use the model to generate predictions on new data by calling functions such as .evaluate() or .predict() . Callbacks: taking a peek into our model while it's training You can look at what is happening in various stages of your model by using callbacks . A callback is a set of functions to be applied at given stages of the training procedure. You can use callbacks to get a view on internal states and statistics of the model during training. You can pass a list of callbacks (as the keyword argument callbacks) to the .fit() method of the Sequential or Model classes. The relevant methods of the callbacks will then be called at each stage of the training. A callback function you are already familiar with is keras.callbacks.History() . This is automatically included in .fit() . Another very useful one is keras.callbacks.ModelCheckpoint which saves the model with its weights at a certain point in the training. This can prove useful if your model is running for a long time and a system failure happens. Not all is lost then. It's a good practice to save the model weights only when an improvement is observed as measured by the acc , for example. keras.callbacks.EarlyStopping stops the training when a monitored quantity has stopped improving. keras.callbacks.LearningRateScheduler will change the learning rate during training. We will apply some callbacks later. For full documentation on callbacks see https://keras.io/callbacks/ What are the steps to optimizing our network? In [10]: import tensorflow as tf import keras from keras import layers from keras import models from keras import utils from keras.layers import Dense from keras.models import Sequential from keras.layers import Flatten from keras.layers import Dropout from keras.layers import Activation from keras.regularizers import l2 from keras.optimizers import SGD from keras.optimizers import RMSprop from keras import datasets from keras import losses from sklearn.utils import shuffle print ( tf . VERSION ) print ( tf . keras . __version__ ) # fix random seed for reproducibility np . random . seed ( 5 ) 1.12.0 2.1.6-tf Using TensorFlow backend. Step 1 - Deciding on the network topology (not really considered optimization but is obviously very important) We will use the MNIST dataset which consists of grayscale images of handwritten digits (0-9) whose dimension is 28x28 pixels. Each pixel is 8 bits so its value ranges from 0 to 255. In [11]: #mnist = tf.keras.datasets.mnist mnist = keras . datasets . mnist ( x_train , y_train ),( x_test , y_test ) = mnist . load_data () x_train . shape , y_train . shape Out[11]: ((60000, 28, 28), (60000,)) Each label is a number between 0 and 9 In [12]: print ( y_train ) [5 0 4 ... 5 6 8] Let's look at some 10 of the images In [13]: plt . figure ( figsize = ( 10 , 10 )) for i in range ( 10 ): plt . subplot ( 5 , 5 , i + 1 ) plt . xticks ([]) plt . yticks ([]) plt . grid ( False ) plt . imshow ( x_train [ i ], cmap = plt . cm . binary ) plt . xlabel ( y_train [ i ]) In [14]: x_train [ 45 ] . shape x_train [ 45 , 15 : 20 , 15 : 20 ] Out[14]: array([[ 11, 198, 231, 41, 0], [ 82, 252, 204, 0, 0], [253, 253, 141, 0, 0], [252, 220, 36, 0, 0], [252, 96, 0, 0, 0]], dtype=uint8) In [15]: print ( f 'We have {x_train.shape[0]} train samples' ) print ( f 'We have {x_test.shape[0]} test samples' ) We have 60000 train samples We have 10000 test samples Preprocessing the data To run our NN we need to pre-process the data First we need to make the 2D image arrays into 1D (flatten them). We can either perform this by using array reshaping with numpy.reshape() or the keras ' method for this: a layer called tf.keras.layers.Flatten which transforms the format of the images from a 2d-array (of 28 by 28 pixels), to a 1D-array of 28 * 28 = 784 pixels. Then we need to normalize the pixel values (give them values between 0 and 1) using the following transformation: \\begin{align} x := \\dfrac{x - x_{min}}{x_{max} - x_{min}} \\textrm{} \\end{align} In our case $x_{min} = 0$ and $x_{max} = 255$ so the formula becomes simply $x := {x}/255$ In [16]: # normalize the data x_train , x_test = x_train / 255.0 , x_test / 255.0 In [17]: # reshape the data into 1D vectors x_train = x_train . reshape ( 60000 , 784 ) x_test = x_test . reshape ( 10000 , 784 ) num_classes = 10 In [18]: x_train . shape [ 1 ] Out[18]: 784 Now let's prepare our class vector (y) to a binary class matrix, e.g. for use with categorical_crossentropy. In [19]: # Convert class vectors to binary class matrices y_train = keras . utils . to_categorical ( y_train , num_classes ) y_test = keras . utils . to_categorical ( y_test , num_classes ) In [20]: y_train [ 0 ] Out[20]: array([0., 0., 0., 0., 0., 1., 0., 0., 0., 0.], dtype=float32) Now we are ready to build the model! Step 2 - Adjusting the learning rate One of the most common optimization algorithm is Stochastic Gradient Descent (SGD). The hyperparameters that can be optimized in SGD are learning rate , momentum , decay and nesterov . Learning rate controls the weight at the end of each batch, and momentum controls how much to let the previous update influence the current weight update. Decay indicates the learning rate decay over each update, and nesterov takes the value True or False depending on if we want to apply Nesterov momentum. Typical values for those hyperparameters are lr=0.01, decay=1e-6, momentum=0.9, and nesterov=True. The learning rate hyperparameter goes into the optimizer function which we will see below. Let's implement a learning rate adaptation schedule in Keras . We'll start with SGD and a learning rate value of 0.1. We will then train the model for 40 epochs and set the decay argument to 0.002 (0.1/50). We also include a momentum value of 0.8 since that seems to work well when using an adaptive learning rate. In [21]: epochs = 60 learning_rate = 0.1 decay_rate = learning_rate / epochs momentum = 0.8 sgd = SGD ( lr = learning_rate , momentum = momentum , decay = decay_rate , nesterov = False ) In [22]: # build the model input_dim = x_train . shape [ 1 ] lr_model = Sequential () lr_model . add ( Dense ( 64 , activation = tf . nn . relu , kernel_initializer = 'uniform' , input_dim = input_dim )) # fully-connected layer with 64 hidden units lr_model . add ( Dense ( 64 , kernel_initializer = 'uniform' , activation = tf . nn . relu )) lr_model . add ( Dense ( num_classes , kernel_initializer = 'uniform' , activation = tf . nn . softmax )) # compile the model lr_model . compile ( loss = 'categorical_crossentropy' , optimizer = sgd , metrics = [ 'acc' ]) In [23]: %%time # Fit the model batch_size = 28 lr_model_history = lr_model . fit ( x_train , y_train , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_data = ( x_test , y_test )) Train on 60000 samples, validate on 10000 samples Epoch 1/60 60000/60000 [==============================] - 3s 43us/step - loss: 0.2856 - acc: 0.9072 - val_loss: 0.1419 - val_acc: 0.9559 Epoch 2/60 60000/60000 [==============================] - 2s 41us/step - loss: 0.1043 - acc: 0.9684 - val_loss: 0.1052 - val_acc: 0.9663 Epoch 3/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0778 - acc: 0.9759 - val_loss: 0.0948 - val_acc: 0.9715 Epoch 4/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0652 - acc: 0.9798 - val_loss: 0.0866 - val_acc: 0.9728 Epoch 5/60 60000/60000 [==============================] - 2s 40us/step - loss: 0.0571 - acc: 0.9828 - val_loss: 0.0861 - val_acc: 0.9729 Epoch 6/60 60000/60000 [==============================] - 2s 40us/step - loss: 0.0516 - acc: 0.9843 - val_loss: 0.0837 - val_acc: 0.9741 Epoch 7/60 60000/60000 [==============================] - 2s 40us/step - loss: 0.0473 - acc: 0.9857 - val_loss: 0.0842 - val_acc: 0.9745 Epoch 8/60 60000/60000 [==============================] - 2s 40us/step - loss: 0.0439 - acc: 0.9870 - val_loss: 0.0858 - val_acc: 0.9725 Epoch 9/60 60000/60000 [==============================] - 2s 41us/step - loss: 0.0414 - acc: 0.9879 - val_loss: 0.0824 - val_acc: 0.9748 Epoch 10/60 60000/60000 [==============================] - 3s 42us/step - loss: 0.0392 - acc: 0.9889 - val_loss: 0.0820 - val_acc: 0.9750 Epoch 11/60 60000/60000 [==============================] - 2s 41us/step - loss: 0.0373 - acc: 0.9895 - val_loss: 0.0824 - val_acc: 0.9745 Epoch 12/60 60000/60000 [==============================] - 2s 41us/step - loss: 0.0355 - acc: 0.9903 - val_loss: 0.0823 - val_acc: 0.9741 Epoch 13/60 60000/60000 [==============================] - 2s 41us/step - loss: 0.0343 - acc: 0.9907 - val_loss: 0.0817 - val_acc: 0.9750 Epoch 14/60 60000/60000 [==============================] - 2s 40us/step - loss: 0.0330 - acc: 0.9912 - val_loss: 0.0812 - val_acc: 0.9754 Epoch 15/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0320 - acc: 0.9912 - val_loss: 0.0809 - val_acc: 0.9757 Epoch 16/60 60000/60000 [==============================] - 3s 44us/step - loss: 0.0310 - acc: 0.9917 - val_loss: 0.0815 - val_acc: 0.9761 Epoch 17/60 60000/60000 [==============================] - 3s 45us/step - loss: 0.0302 - acc: 0.9919 - val_loss: 0.0814 - val_acc: 0.9759 Epoch 18/60 60000/60000 [==============================] - 3s 46us/step - loss: 0.0292 - acc: 0.9924 - val_loss: 0.0815 - val_acc: 0.9755 Epoch 19/60 60000/60000 [==============================] - 2s 41us/step - loss: 0.0286 - acc: 0.9928 - val_loss: 0.0811 - val_acc: 0.9759 Epoch 20/60 60000/60000 [==============================] - 2s 41us/step - loss: 0.0279 - acc: 0.9929 - val_loss: 0.0808 - val_acc: 0.9759 Epoch 21/60 60000/60000 [==============================] - 2s 41us/step - loss: 0.0272 - acc: 0.9931 - val_loss: 0.0819 - val_acc: 0.9767 Epoch 22/60 60000/60000 [==============================] - 2s 41us/step - loss: 0.0267 - acc: 0.9933 - val_loss: 0.0820 - val_acc: 0.9753 Epoch 23/60 60000/60000 [==============================] - 3s 42us/step - loss: 0.0261 - acc: 0.9936 - val_loss: 0.0812 - val_acc: 0.9765 Epoch 24/60 60000/60000 [==============================] - 3s 47us/step - loss: 0.0256 - acc: 0.9935 - val_loss: 0.0821 - val_acc: 0.9759 Epoch 25/60 60000/60000 [==============================] - 3s 50us/step - loss: 0.0250 - acc: 0.9939 - val_loss: 0.0821 - val_acc: 0.9760 Epoch 26/60 60000/60000 [==============================] - 3s 47us/step - loss: 0.0246 - acc: 0.9939 - val_loss: 0.0822 - val_acc: 0.9755 Epoch 27/60 60000/60000 [==============================] - 3s 45us/step - loss: 0.0242 - acc: 0.9941 - val_loss: 0.0820 - val_acc: 0.9768 Epoch 28/60 60000/60000 [==============================] - 3s 49us/step - loss: 0.0238 - acc: 0.9941 - val_loss: 0.0825 - val_acc: 0.9761 Epoch 29/60 60000/60000 [==============================] - 3s 48us/step - loss: 0.0233 - acc: 0.9944 - val_loss: 0.0824 - val_acc: 0.9761 Epoch 30/60 60000/60000 [==============================] - 3s 46us/step - loss: 0.0231 - acc: 0.9946 - val_loss: 0.0830 - val_acc: 0.9760 Epoch 31/60 60000/60000 [==============================] - 3s 48us/step - loss: 0.0228 - acc: 0.9947 - val_loss: 0.0829 - val_acc: 0.9755 Epoch 32/60 60000/60000 [==============================] - 3s 48us/step - loss: 0.0224 - acc: 0.9947 - val_loss: 0.0829 - val_acc: 0.9760 Epoch 33/60 60000/60000 [==============================] - 3s 48us/step - loss: 0.0220 - acc: 0.9948 - val_loss: 0.0831 - val_acc: 0.9758 Epoch 34/60 60000/60000 [==============================] - 3s 51us/step - loss: 0.0218 - acc: 0.9949 - val_loss: 0.0827 - val_acc: 0.9754 Epoch 35/60 60000/60000 [==============================] - 3s 44us/step - loss: 0.0215 - acc: 0.9951 - val_loss: 0.0832 - val_acc: 0.9762 Epoch 36/60 60000/60000 [==============================] - 3s 45us/step - loss: 0.0213 - acc: 0.9952 - val_loss: 0.0833 - val_acc: 0.9759 Epoch 37/60 60000/60000 [==============================] - 2s 41us/step - loss: 0.0210 - acc: 0.9953 - val_loss: 0.0836 - val_acc: 0.9764 Epoch 38/60 60000/60000 [==============================] - 2s 40us/step - loss: 0.0208 - acc: 0.9952 - val_loss: 0.0836 - val_acc: 0.9758 Epoch 39/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0205 - acc: 0.9954 - val_loss: 0.0835 - val_acc: 0.9760 Epoch 40/60 60000/60000 [==============================] - 2s 38us/step - loss: 0.0203 - acc: 0.9955 - val_loss: 0.0838 - val_acc: 0.9761 Epoch 41/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0201 - acc: 0.9956 - val_loss: 0.0840 - val_acc: 0.9764 Epoch 42/60 60000/60000 [==============================] - 3s 47us/step - loss: 0.0198 - acc: 0.9957 - val_loss: 0.0841 - val_acc: 0.9761 Epoch 43/60 60000/60000 [==============================] - 3s 47us/step - loss: 0.0196 - acc: 0.9957 - val_loss: 0.0842 - val_acc: 0.9761 Epoch 44/60 60000/60000 [==============================] - 3s 43us/step - loss: 0.0194 - acc: 0.9958 - val_loss: 0.0840 - val_acc: 0.9760 Epoch 45/60 60000/60000 [==============================] - 2s 42us/step - loss: 0.0193 - acc: 0.9959 - val_loss: 0.0841 - val_acc: 0.9757 Epoch 46/60 60000/60000 [==============================] - 3s 42us/step - loss: 0.0190 - acc: 0.9958 - val_loss: 0.0848 - val_acc: 0.9760 Epoch 47/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0189 - acc: 0.9959 - val_loss: 0.0846 - val_acc: 0.9760 Epoch 48/60 60000/60000 [==============================] - 2s 40us/step - loss: 0.0187 - acc: 0.9961 - val_loss: 0.0846 - val_acc: 0.9764 Epoch 49/60 60000/60000 [==============================] - 3s 45us/step - loss: 0.0185 - acc: 0.9961 - val_loss: 0.0845 - val_acc: 0.9763 Epoch 50/60 60000/60000 [==============================] - 2s 41us/step - loss: 0.0184 - acc: 0.9963 - val_loss: 0.0846 - val_acc: 0.9761 Epoch 51/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0182 - acc: 0.9962 - val_loss: 0.0846 - val_acc: 0.9762 Epoch 52/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0181 - acc: 0.9963 - val_loss: 0.0849 - val_acc: 0.9761 Epoch 53/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0179 - acc: 0.9962 - val_loss: 0.0851 - val_acc: 0.9764 Epoch 54/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0177 - acc: 0.9964 - val_loss: 0.0852 - val_acc: 0.9768 Epoch 55/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0176 - acc: 0.9963 - val_loss: 0.0852 - val_acc: 0.9763 Epoch 56/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0175 - acc: 0.9965 - val_loss: 0.0854 - val_acc: 0.9763 Epoch 57/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0174 - acc: 0.9965 - val_loss: 0.0854 - val_acc: 0.9763 Epoch 58/60 60000/60000 [==============================] - 2s 39us/step - loss: 0.0172 - acc: 0.9965 - val_loss: 0.0855 - val_acc: 0.9763 Epoch 59/60 60000/60000 [==============================] - 2s 38us/step - loss: 0.0171 - acc: 0.9966 - val_loss: 0.0855 - val_acc: 0.9763 Epoch 60/60 60000/60000 [==============================] - 2s 38us/step - loss: 0.0170 - acc: 0.9967 - val_loss: 0.0853 - val_acc: 0.9757 CPU times: user 4min 27s, sys: 1min 1s, total: 5min 28s Wall time: 2min 32s In [24]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( np . sqrt ( lr_model_history . history [ 'loss' ]), 'r' , label = 'train' ) ax . plot ( np . sqrt ( lr_model_history . history [ 'val_loss' ]), 'b' , label = 'val' ) ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Loss' , fontsize = 20 ) ax . legend () ax . tick_params ( labelsize = 20 ) In [26]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( np . sqrt ( lr_model_history . history [ 'acc' ]), 'r' , label = 'train' ) ax . plot ( np . sqrt ( lr_model_history . history [ 'val_acc' ]), 'b' , label = 'val' ) ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Accuracy' , fontsize = 20 ) ax . legend () ax . tick_params ( labelsize = 20 ) Exercise: Write a function that performs exponential learning rate decay using LearningRateScheduler $lr = lr0*e&#94;{(-kt)}$ t is the iteration number Step 3 - Choosing an optimizer and a loss function When constructing a model and using it to make our predictions, for example to assign label scores to images (\"cat\", \"plane\", etc), we want to measure our success or failure by defining a \"loss\" function (or objective function). The goal of optimization is to efficiently calculate the parameters/weights that minimize this loss function. keras provides various types of loss functions . Sometimes the \"loss\" function measures the \"distance\". We can define this \"distance\" between two data points in various ways suitable to the problem or dataset. Distance Euclidean Manhattan others such as Hamming which measures distances between strings, for example. The Hamming distance of \"carolin\" and \"cathrin\" is 3. Loss functions MSE (for regression) categorical cross-entropy (for classification) binary cross entropy (for classification) In [33]: # build the model input_dim = x_train . shape [ 1 ] model = Sequential () model . add ( Dense ( 64 , activation = tf . nn . relu , kernel_initializer = 'uniform' , input_dim = input_dim )) # fully-connected layer with 64 hidden units #model.add(Dropout(0.5)) model . add ( Dense ( 64 , kernel_initializer = 'uniform' , activation = tf . nn . relu )) #model.add(Dropout(0.5)) model . add ( Dense ( num_classes , kernel_initializer = 'uniform' , activation = tf . nn . softmax )) In [34]: # defining the parameters for RMSprop (I used the keras defaults here) rms = RMSprop ( lr = 0.001 , rho = 0.9 , epsilon = None , decay = 0.0 ) model . compile ( loss = 'categorical_crossentropy' , optimizer = rms , metrics = [ 'acc' ]) Step 4 - Deciding on the batch size and number of epochs In [35]: %%time batch_size = 32 epochs = 20 model_history = model . fit ( x_train , y_train , batch_size = batch_size , epochs = epochs , verbose = 1 , validation_data = ( x_test , y_test )) Train on 60000 samples, validate on 10000 samples Epoch 1/20 60000/60000 [==============================] - 3s 46us/step - loss: 0.3931 - acc: 0.8850 - val_loss: 0.2082 - val_acc: 0.9400 Epoch 2/20 60000/60000 [==============================] - 3s 42us/step - loss: 0.1696 - acc: 0.9502 - val_loss: 0.1334 - val_acc: 0.9595 Epoch 3/20 60000/60000 [==============================] - 2s 40us/step - loss: 0.1208 - acc: 0.9641 - val_loss: 0.1162 - val_acc: 0.9677 Epoch 4/20 60000/60000 [==============================] - 2s 40us/step - loss: 0.0995 - acc: 0.9710 - val_loss: 0.1068 - val_acc: 0.9712 Epoch 5/20 60000/60000 [==============================] - 2s 40us/step - loss: 0.0846 - acc: 0.9751 - val_loss: 0.1035 - val_acc: 0.9728 Epoch 6/20 60000/60000 [==============================] - 3s 42us/step - loss: 0.0733 - acc: 0.9784 - val_loss: 0.1203 - val_acc: 0.9713 Epoch 7/20 60000/60000 [==============================] - 2s 42us/step - loss: 0.0660 - acc: 0.9807 - val_loss: 0.0993 - val_acc: 0.9735 Epoch 8/20 60000/60000 [==============================] - 3s 43us/step - loss: 0.0599 - acc: 0.9834 - val_loss: 0.1143 - val_acc: 0.9713 Epoch 9/20 60000/60000 [==============================] - 3s 46us/step - loss: 0.0533 - acc: 0.9851 - val_loss: 0.1217 - val_acc: 0.9695 Epoch 10/20 60000/60000 [==============================] - 3s 47us/step - loss: 0.0493 - acc: 0.9866 - val_loss: 0.1109 - val_acc: 0.9735 Epoch 11/20 60000/60000 [==============================] - 3s 42us/step - loss: 0.0441 - acc: 0.9875 - val_loss: 0.1186 - val_acc: 0.9742 Epoch 12/20 60000/60000 [==============================] - 2s 41us/step - loss: 0.0422 - acc: 0.9883 - val_loss: 0.1222 - val_acc: 0.9725 Epoch 13/20 60000/60000 [==============================] - 2s 41us/step - loss: 0.0397 - acc: 0.9897 - val_loss: 0.1327 - val_acc: 0.9725 Epoch 14/20 60000/60000 [==============================] - 3s 43us/step - loss: 0.0359 - acc: 0.9905 - val_loss: 0.1365 - val_acc: 0.9724 Epoch 15/20 60000/60000 [==============================] - 3s 42us/step - loss: 0.0342 - acc: 0.9910 - val_loss: 0.1445 - val_acc: 0.9748 Epoch 16/20 60000/60000 [==============================] - 2s 40us/step - loss: 0.0317 - acc: 0.9915 - val_loss: 0.1420 - val_acc: 0.9758 Epoch 17/20 60000/60000 [==============================] - 2s 40us/step - loss: 0.0303 - acc: 0.9920 - val_loss: 0.1534 - val_acc: 0.9740 Epoch 18/20 60000/60000 [==============================] - 2s 40us/step - loss: 0.0280 - acc: 0.9928 - val_loss: 0.1549 - val_acc: 0.9727 Epoch 19/20 60000/60000 [==============================] - 2s 40us/step - loss: 0.0263 - acc: 0.9938 - val_loss: 0.1691 - val_acc: 0.9731 Epoch 20/20 60000/60000 [==============================] - 3s 43us/step - loss: 0.0266 - acc: 0.9934 - val_loss: 0.1504 - val_acc: 0.9759 CPU times: user 1min 32s, sys: 20.9 s, total: 1min 53s Wall time: 50.7 s In [36]: from sklearn.metrics import r2_score as r2 score = model . evaluate ( x_test , y_test , verbose = 0 ) print ( 'Test loss:' , score [ 0 ]) print ( 'Test accuracy:' , score [ 1 ]) print ( 'Test R2:' , r2 ( y_test , model . predict ( x_test ))) Test loss: 0.1503745241531124 Test accuracy: 0.9759 Test R2: 0.9542278387249276 In [37]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( np . sqrt ( model_history . history [ 'acc' ]), 'r' , label = 'train_acc' ) ax . plot ( np . sqrt ( model_history . history [ 'val_acc' ]), 'b' , label = 'val_acc' ) ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Accuracy' , fontsize = 20 ) ax . legend () ax . tick_params ( labelsize = 20 ) In [38]: fig , ax = plt . subplots ( 1 , 1 , figsize = ( 10 , 6 )) ax . plot ( np . sqrt ( model_history . history [ 'loss' ]), 'r' , label = 'train' ) ax . plot ( np . sqrt ( model_history . history [ 'val_loss' ]), 'b' , label = 'val' ) ax . set_xlabel ( r 'Epoch' , fontsize = 20 ) ax . set_ylabel ( r 'Loss' , fontsize = 20 ) ax . legend () ax . tick_params ( labelsize = 20 ) In [39]: print ( model_history . history . keys ()) dict_keys(['val_loss', 'val_acc', 'loss', 'acc']) Step 5 - Random restarts This method does not seem to have an implementation in keras . We will leave it as a home exercise! Hint: you can use keras.callbacks.LearningRateScheduler . Tuning the Hyperparameters using Cross Validation Now instead of trying different values by hand, we will use GridSearchCV from Scikit-Learn to try out several values for our hyperparameters and compare the results. To do cross-validation with keras we will use the wrappers for the Scikit-Learn API. They provide a way to use Sequential Keras models (single-input only) as part of your Scikit-Learn workflow. There are two wrappers available: keras.wrappers.scikit_learn.KerasClassifier(build_fn=None, **sk_params) , which implements the Scikit-Learn classifier interface, keras.wrappers.scikit_learn.KerasRegressor(build_fn=None, **sk_params) , which implements the Scikit-Learn regressor interface. In [41]: import numpy from sklearn.model_selection import GridSearchCV from keras.wrappers.scikit_learn import KerasClassifier Trying different weight initializations In [42]: # Function to create model, required for KerasClassifier def create_model ( init_mode = 'uniform' ): # define model model = Sequential () model . add ( Dense ( 64 , kernel_initializer = init_mode , activation = tf . nn . relu , input_dim = 784 )) model . add ( Dropout ( 0.5 )) model . add ( Dense ( 64 , kernel_initializer = init_mode , activation = tf . nn . relu )) model . add ( Dropout ( 0.5 )) model . add ( Dense ( 10 , kernel_initializer = init_mode , activation = tf . nn . softmax )) # compile model model . compile ( loss = 'categorical_crossentropy' , optimizer = RMSprop (), metrics = [ 'accuracy' ]) return model In [43]: %%time seed = 7 numpy . random . seed ( seed ) batch_size = 128 epochs = 10 model_CV = KerasClassifier ( build_fn = create_model , epochs = epochs , batch_size = batch_size , verbose = 0 ) # define the grid search parameters init_mode = [ 'uniform' , 'lecun_uniform' , 'normal' , 'zero' , 'glorot_normal' , 'glorot_uniform' , 'he_normal' , 'he_uniform' ] param_grid = dict ( init_mode = init_mode ) grid = GridSearchCV ( estimator = model_CV , param_grid = param_grid , n_jobs =- 1 , cv = 3 ) grid_result = grid . fit ( x_train , y_train ) # summarize results print ( \"Best: %f using %s \" % ( grid_result . best_score_ , grid_result . best_params_ )) means = grid_result . cv_results_ [ 'mean_test_score' ] stds = grid_result . cv_results_ [ 'std_test_score' ] params = grid_result . cv_results_ [ 'params' ] for mean , stdev , param in zip ( means , stds , params ): print ( \" %f ( %f ) with: %r \" % ( mean , stdev , param )) Best: 0.948967 using {'init_mode': 'glorot_uniform'} 0.946117 (0.002852) with: {'init_mode': 'uniform'} 0.948400 (0.001192) with: {'init_mode': 'lecun_uniform'} 0.948117 (0.001310) with: {'init_mode': 'normal'} 0.112367 (0.002416) with: {'init_mode': 'zero'} 0.948367 (0.002593) with: {'init_mode': 'glorot_normal'} 0.948967 (0.002210) with: {'init_mode': 'glorot_uniform'} 0.947400 (0.002060) with: {'init_mode': 'he_normal'} 0.946983 (0.002908) with: {'init_mode': 'he_uniform'} CPU times: user 21 s, sys: 3.6 s, total: 24.6 s Wall time: 1min 36s Save Your Neural Network Model to JSON The Hierarchical Data Format (HDF5) is a data storage format for storing large arrays of data including values for the weights in a neural network. You can install HDF5 Python module: pip install h5py Keras gives you the ability to describe and save any model using the JSON format. In [ ]: from keras.models import model_from_json # serialize model to JSON model_json = model . to_json () with open ( \"model.json\" , \"w\" ) as json_file : json_file . write ( model_json ) # save weights to HDF5 model . save_weights ( \"model.h5\" ) print ( \"Model saved\" ) # when you want to retrieve the model: load json and create model json_file = open ( 'model.json' , 'r' ) saved_model = json_file . read () # close the file as good practice json_file . close () model_from_json = model_from_json ( saved_model ) # load weights into new model model_from_json . load_weights ( \"model.h5\" ) print ( \"Model loaded\" ) Exercise 1: Perform a GridSearch for Learning rate and number of epochs combined We will try various small standard learning rates, and momentum values from 0.1 to 0.9 in steps of 0.2. We want to include the number of epochs in an optimization as there is a dependency between the learning rate, the batch size, and the number of epochs. In [ ]: #your code here if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"pages","url":"pages/lab3/students/"},{"title":"Advanced Section 1: Optimization/Dropout","text":"Slides PDF Lecture notes PDF","tags":"A-sections","url":"a-sections/a-section1/"},{"title":"Lecture 6: NN Optimization","text":"Slides Lecture 6 PDF Lecture 6 PPTX Associated Material","tags":"lectures","url":"lectures/lecture6/"},{"title":"Lecture 5: Review of NN from 109A","text":"Slides Lecture 5 PDF Lecture 5 PPTX Associated Material","tags":"lectures","url":"lectures/lecture5/"},{"title":"Lab 2: Smooths and GAMs","text":"Lab 2 Notebooks Lab2 Smooths and GAMs Lab2 Smooths and GAMs with solutions","tags":"labs","url":"labs/lab2/"},{"title":"Lab 2: Smooths and GAMs","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS109B Data Science 2: Advanced Topics in Data Science Lab 2 - Smoothers and Generalized Additive Models Harvard University Spring 2019 Instructors: Mark Glickman and Pavlos Protopapas Lab Instructors: Will Claybaugh Contributors: Paul Tyklin and Will Claybaugh In [ ]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Learning Goals The main goal of this lab is to get familiar with calling R functions within Python. Along the way, we'll learn about the \"formula\" interface to statsmodels, which gives an intuitive way of specifying regression models, and we'll review the different approaches to fitting curves. Key Skills: Importing (base) R functions Importing R library functions Populating vectors R understands Populating dataframes R understands Populating formulas R understands Running models in R Getting results back to Python Getting model predictions in R Plotting in R Reading R's documentation In [ ]: import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Linear/Polynomial Regression (Python, Review) Hopefully, you remember working with Statsmodels during 109a Reading data and (some) exploring in Pandas: In [ ]: diab = pd . read_csv ( \"data/diabetes.csv\" ) print ( \"\"\" # Variables are: # subject: subject ID number # age: age diagnosed with diabetes # acidity: a measure of acidity called base deficit # y: natural log of serum C-peptide concentration # # Original source is Sockett et al. (1987) # mentioned in Hastie and Tibshirani's book # \"Generalized Additive Models\". \"\"\" ) display ( diab . head ()) display ( diab . dtypes ) display ( diab . describe ()) Plotting with matplotlib: In [ ]: ax0 = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data\" ) #plotting direclty from pandas! ax0 . set_xlabel ( \"Age at Diagnosis\" ) ax0 . set_ylabel ( \"Log C-Peptide Concentration\" ); Linear regression with statsmodels. Previously, we worked from a vector of target values and a design matrix we built ourself (e.g. from PolynomialFeatures). Now, Statsmodels' formula interface can help build the target value and design matrix for you. In [ ]: #Using statsmodels import statsmodels.formula.api as sm model1 = sm . ols ( 'y ~ age' , data = diab ) fit1_lm = model1 . fit () Build a data frame to predict values on (sometimes this is just the test or validation set) Very useful for making pretty plots of the model predcitions -- predict for TONS of values, not just whatever's in the training set In [ ]: x_pred = np . linspace ( 0 , 16 , 100 ) predict_df = pd . DataFrame ( data = { \"age\" : x_pred }) predict_df . head () Use get_prediction( ).summary_frame() to get the model's prediction (and error bars!) In [ ]: prediction_output = fit1_lm . get_prediction ( predict_df ) . summary_frame () prediction_output . head () Plot the model and error bars In [ ]: ax1 = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data with least-squares linear fit\" ) ax1 . set_xlabel ( \"Age at Diagnosis\" ) ax1 . set_ylabel ( \"Log C-Peptide Concentration\" ) ax1 . plot ( predict_df . age , prediction_output [ 'mean' ], color = \"green\" ) ax1 . plot ( predict_df . age , prediction_output [ 'mean_ci_lower' ], color = \"blue\" , linestyle = \"dashed\" ) ax1 . plot ( predict_df . age , prediction_output [ 'mean_ci_upper' ], color = \"blue\" , linestyle = \"dashed\" ); ax1 . plot ( predict_df . age , prediction_output [ 'obs_ci_lower' ], color = \"skyblue\" , linestyle = \"dashed\" ) ax1 . plot ( predict_df . age , prediction_output [ 'obs_ci_upper' ], color = \"skyblue\" , linestyle = \"dashed\" ); Discussion What are the dark error bars? What are the light error bars? Exercise 1 Fit a 3rd degree polynomial model and plot the model+error bars Route1: Build a design df with a column for each of age , age**2 , age**3 Route2: Just edit the formula Answers : 1. In [ ]: # your code here In [ ]: fit2_lm = sm . ols ( formula = \"y ~ age + np.power(age, 2) + np.power(age, 3)\" , data = diab ) . fit () poly_predictions = fit2_lm . get_prediction ( predict_df ) . summary_frame () poly_predictions . head () 2. In [ ]: # your code here In [ ]: ax2 = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data with least-squares cubic fit\" ) ax2 . set_xlabel ( \"Age at Diagnosis\" ) ax2 . set_ylabel ( \"Log C-Peptide Concentration\" ) ax2 . plot ( predict_df . age , poly_predictions [ 'mean' ], color = \"green\" ) ax2 . plot ( predict_df . age , poly_predictions [ 'mean_ci_lower' ], color = \"blue\" , linestyle = \"dashed\" ) ax2 . plot ( predict_df . age , poly_predictions [ 'mean_ci_upper' ], color = \"blue\" , linestyle = \"dashed\" ); ax2 . plot ( predict_df . age , poly_predictions [ 'obs_ci_lower' ], color = \"skyblue\" , linestyle = \"dashed\" ) ax2 . plot ( predict_df . age , poly_predictions [ 'obs_ci_upper' ], color = \"skyblue\" , linestyle = \"dashed\" ); Linear/Polynomial Regression, but make it R This is the meat of the lab. After this section we'll know everything we need to in order to work with R models. The rest of the lab is just applying these concepts to run particular models. This section therefore is your 'cheat sheet' for working in R. What we need to know: Importing (base) R functions Importing R Library functions Populating vectors R understands Populating DataFrames R understands Populating Formulas R understands Running models in R Getting results back to Python Getting model predictions in R Plotting in R Reading R's documentation Importing R functions In [ ]: # if you're on JupyterHub you may need to specify the path to R #import os #os.environ['R_HOME'] = \"/usr/share/anaconda3/lib/R\" import rpy2.robjects as robjects In [ ]: r_lm = robjects . r [ \"lm\" ] r_predict = robjects . r [ \"predict\" ] #r_plot = robjects.r[\"plot\"] # more on plotting later #lm() and predict() are two of the most common functions we'll use Importing R libraries In [ ]: from rpy2.robjects.packages import importr #r_cluster = importr('cluster') #r_cluster.pam; Populating vectors R understands In [ ]: r_y = robjects . FloatVector ( diab [ 'y' ]) r_age = robjects . FloatVector ( diab [ 'age' ]) # What happens if we pass the wrong type? # How does r_age display? # How does r_age print? Populating Data Frames R understands In [ ]: diab_r = robjects . DataFrame ({ \"y\" : r_y , \"age\" : r_age }) # How does diab_r display? # How does diab_r print? Populating formulas R understands In [ ]: simple_formula = robjects . Formula ( \"y~age\" ) simple_formula . environment [ \"y\" ] = r_y #populate the formula's .environment, so it knows what 'y' and 'age' refer to simple_formula . environment [ \"age\" ] = r_age Running Models in R In [ ]: diab_lm = r_lm ( formula = simple_formula ) # the formula object is storing all the needed variables In [ ]: simple_formula = robjects . Formula ( \"y~age\" ) # reset the formula diab_lm = r_lm ( formula = simple_formula , data = diab_r ) #can also use a 'dumb' formula and pass a dataframe Getting results back to Python In [ ]: diab_lm #the result is already 'in' python, but it's a special object In [ ]: print ( diab_lm . names ) # view all names In [ ]: diab_lm [ 0 ] #grab the first element In [ ]: diab_lm . rx2 ( \"coefficients\" ) #use rx2 to get elements by name! In [ ]: np . array ( diab_lm . rx2 ( \"coefficients\" )) #r vectors can be converted to numpy (but rarely needed) Getting Predictions In [ ]: # make a df to predict on (might just be the validation or test dataframe) predict_df = robjects . DataFrame ({ \"age\" : robjects . FloatVector ( np . linspace ( 0 , 16 , 100 ))}) # call R's predict() function, passing the model and the data predictions = r_predict ( diab_lm , predict_df ) In [ ]: x_vals = predict_df . rx2 ( \"age\" ) In [ ]: ax = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data\" ) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ); ax . plot ( x_vals , predictions ); #plt still works with r vectors as input! Plotting in R In [ ]: % load_ext rpy2.ipython The above turns on the %R \"magic\" R's plot() command responds differently based on what you hand to it; Different models get different plots! For any specific model search for plot.modelname. E.g. for a GAM model, search plot.gam for any details of plotting a GAM model The %R \"magic\" runs R code in 'notebook' mode, so figures display nicely Ahead of the plot( ) code we pass in the variables R needs to know about ( -i is for \"input\") In [ ]: % R -i diab_lm plot(diab_lm); Reading R's documentation The documentation for the lm() funciton is here , and a prettier version (same content) is here . When googling, perfer rdocumentation.org when possible. Sections: Usage : gives the function signature, including all optional arguments Arguments : What each function input controls Details : additional info on what the funciton does and how arguments interact. Often the right place to start reading Value : the structure of the object returned by the function Refferences : The relevant academic papers See Also : other functions of interest Exercise 2 Add confidence intervals calculated in R to the linear regression plot above. Use the interval= argument to r_predict() (documentation here ). You will have to work with a matrix returned by R. Fit a 5th degree polynomial to the diabetes data in R. Search the web for an easier method than writing out a formula with all 5 polynomial terms. Answers 1. In [ ]: # your code here In [ ]: CI_matrix = np . array ( r_predict ( diab_lm , predict_df , interval = \"confidence\" )) ax = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data\" ) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ); ax . plot ( x_vals , CI_matrix [:, 0 ], label = \"prediction\" ) ax . plot ( x_vals , CI_matrix [:, 1 ], label = \"95% CI\" , c = 'g' ) ax . plot ( x_vals , CI_matrix [:, 2 ], label = \"95% CI\" , c = 'g' ) plt . legend (); 2. In [ ]: # your code here In [ ]: ploy5_formula = robjects . Formula ( \"y~poly(age,5)\" ) # reset the formula diab5_lm = r_lm ( formula = ploy5_formula , data = diab_r ) #can also use a 'dumb' formula and pass a dataframe predictions = r_predict ( diab5_lm , predict_df , interval = \"confidence\" ) ax = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data\" ) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ); ax . plot ( x_vals , predictions ); Lowess Smoothing Lowess Smoothing is implemented in both Python and R. We'll use it as another example as we transition languages. Discussion What is lowess smoothing? Which 109a models is it related to? How explainable is lowess? What are the tunable parameters? In Python In [ ]: from statsmodels.nonparametric.smoothers_lowess import lowess as lowess ss1 = lowess ( diab [ 'y' ], diab [ 'age' ], frac = 0.15 ) ss2 = lowess ( diab [ 'y' ], diab [ 'age' ], frac = 0.25 ) ss3 = lowess ( diab [ 'y' ], diab [ 'age' ], frac = 0.7 ) ss4 = lowess ( diab [ 'y' ], diab [ 'age' ], frac = 1 ) In [ ]: ss1 [: 10 ,:] # we get back simple a smoothed y value for each x value in the data Notice the clean code to plot different models. We'll see even cleaner code in a minute In [ ]: for cur_model , cur_frac in zip ([ ss1 , ss2 , ss3 , ss4 ],[ 0.15 , 0.25 , 0.7 , 1 ]): ax = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Lowess Fit, Fraction = {} \" . format ( cur_frac )) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ) ax . plot ( cur_model [:, 0 ], cur_model [:, 1 ], color = \"blue\" ) plt . show () Discussion Which model has high variance, which has high bias? What makes a model high variance or high bias? In R We need to: Import the loess function Send data over to R Call the function and get results In [ ]: r_loess = robjects . r [ 'loess.smooth' ] #extract R function r_y = robjects . FloatVector ( diab [ 'y' ]) r_age = robjects . FloatVector ( diab [ 'age' ]) ss1_r = r_loess ( r_age , r_y , span = 0.15 , degree = 1 ) In [ ]: ss1_r #again, a smoothed y value for each x value in the data Exercise 3 Predict the output of ss1_r[0] ss1_r.rx2(\"y\") 1. your answer here 2. your answer here Varying span Next, some extremely clean code to fit and plot models with various parameter settings. (Though the zip() method seen earlier is great when e.g. the label and the parameter differ) In [ ]: for cur_frac in [ 0.15 , 0.25 , 0.7 , 1 ]: cur_smooth = r_loess ( r_age , r_y , span = cur_frac ) ax = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Lowess Fit, Fraction = {} \" . format ( cur_frac )) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ) ax . plot ( cur_smooth [ 0 ], cur_smooth [ 1 ], color = \"blue\" ) plt . show () Discussion Mark wasn't kidding; the Python and R results differ for frac=.15. Thoughts? Why isn't the bottom plot a straight line? We're using 100% of the data in each window... Smoothing Splines From this point forward, we're working with R functions; these models aren't (well) supported in Python. For clarity: this is the fancy spline model that minimizes $MSE - \\lambda\\cdot\\text{wiggle penalty}$ $=$ $\\sum_{i=1}&#94;N \\left(y_i - f(x_i)\\right)&#94;2 - \\lambda \\int \\left(f''(x)\\right)&#94;2$, across all possible functions $f$. The winner will always be a continuous, cubic polynomial with a knot at each data point Discussion Any idea why the winner is cubic? How interpretable is this model? What are the tunable parameters? In [ ]: r_smooth_spline = robjects . r [ 'smooth.spline' ] #extract R function # run smoothing function spline1 = r_smooth_spline ( r_age , r_y , spar = 0 ) Exercise 4 We actually set the spar parameter, a scale-free value that translates to a $\\lambda$ through a complex expression. Inspect the 'spline1' result and extract the implied value of $\\lambda$ Working from the fitting/plotting loop examples above, produce a plot like the one below for spar = [0,.5,.9,2], including axes labels and title. 1. In [ ]: # your answer here In [ ]: lambda1 = spline1 . rx2 ( \"lambda\" ) 2. In [ ]: # your answer here In [ ]: for cur_spar in [ 0 , 0.5 , 0.9 , 2 ]: cur_model = r_smooth_spline ( r_age , r_y , spar = cur_spar ) cur_lambda = cur_model . rx2 ( \"lambda\" )[ 0 ] ax = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"$\\lambda=$\" + str ( cur_lambda )) #can use TeX style in labels ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ) ax . plot ( cur_model . rx2 ( \"x\" ), cur_model . rx2 ( \"y\" ), color = \"darkgreen\" ) plt . show () CV R's smooth_spline funciton has built-in CV to find a good lambda. See package docs . In [ ]: spline_cv = r_smooth_spline ( r_age , r_y , cv = True ) lambda_cv = spline_cv . rx2 ( \"lambda\" )[ 0 ] ax19 = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"smoothing spline with $\\lambda=$\" + str ( np . round ( lambda_cv , 4 )) + \", chosen by cross-validation\" ) ax19 . set_xlabel ( \"Age at Diagnosis\" ) ax19 . set_ylabel ( \"Log C-Peptide Concentration\" ) ax19 . plot ( spline_cv . rx2 ( \"x\" ), spline_cv . rx2 ( \"y\" ), color = \"darkgreen\" ); Discussion Does the selected model look reasonable? How would you describe the effect of age at diagnosis on C_peptide concentration? What are the costs/benefits of the (fancy) spline model, relative to the linear regression we fit above? Natural & Basis Splines Here, we take a step backward on model complexity, but a step forward in coding complexity. We'll be working with R's formula interface again, so we will need to populate Formulas and DataFrames. Discussion In what way are Natural and Basis splines less complex than the splines we were just working with? What makes a spline 'natural'? What makes a spline 'basis'? What are the tuning parameters? In [ ]: #We will now work with a new dataset, called GAGurine. #The dataset description (from the R package MASS) is below: #Data were collected on the concentration of a chemical GAG # in the urine of 314 children aged from zero to seventeen years. # The aim of the study was to produce a chart to help a paediatrican # to assess if a child's GAG concentration is ‘normal'. #The variables are: # Age: age of child in years. # GAG: concentration of GAG (the units have been lost). In [ ]: GAGurine = pd . read_csv ( \"data/GAGurine.csv\" ) display ( GAGurine . head ()) ax31 = GAGurine . plot . scatter ( x = 'Age' , y = 'GAG' , c = 'black' , title = \"GAG in urine of children\" ) ax31 . set_xlabel ( \"Age\" ); ax31 . set_ylabel ( \"GAG\" ); Standard stuff: import function, convert variables to R format, call function In [ ]: from rpy2.robjects.packages import importr r_splines = importr ( 'splines' ) # populate R variables r_gag = robjects . FloatVector ( GAGurine [ 'GAG' ] . values ) r_age = robjects . FloatVector ( GAGurine [ 'Age' ] . values ) r_quarts = robjects . FloatVector ( np . quantile ( r_age ,[ . 25 , . 5 , . 75 ])) #woah, numpy functions run on R objects! What happens when we call the ns or bs functions from r_splines? In [ ]: ns_design = r_splines . ns ( r_age , knots = r_quarts ) bs_design = r_splines . bs ( r_age , knots = r_quarts ) In [ ]: print ( ns_design ) ns and bs return design matrices, not model objects! That's because they're meant to work with lm 's formula interface. To get a model object we populate a formula including ns( , ) and fit to data In [ ]: r_lm = robjects . r [ 'lm' ] r_predict = robjects . r [ 'predict' ] # populate the formula ns_formula = robjects . Formula ( \"Gag ~ ns(Age, knots=r_quarts)\" ) ns_formula . environment [ 'Gag' ] = r_gag ns_formula . environment [ 'Age' ] = r_age ns_formula . environment [ 'r_quarts' ] = r_quarts # fit the model ns_model = r_lm ( ns_formula ) Predict like usual: build a dataframe to predict on and call predict() In [ ]: # predict predict_frame = robjects . DataFrame ({ \"Age\" : robjects . FloatVector ( np . linspace ( 0 , 20 , 100 ))}) ns_out = r_predict ( ns_model , predict_frame ) In [ ]: ax32 = GAGurine . plot . scatter ( x = 'Age' , y = 'GAG' , c = 'grey' , title = \"GAG in urine of children\" ) ax32 . set_xlabel ( \"Age\" ) ax32 . set_ylabel ( \"GAG\" ) ax32 . plot ( predict_frame . rx2 ( \"Age\" ), ns_out , color = 'red' ) ax32 . legend ([ \"Natural spline, knots at quartiles\" ]); Exercise 5 Fit a basis spline model with the same knots, and add it to the plot above Fit a basis spline with 8 knots placed at [2,4,6...14,16] and add it to the plot above Answers: 1. In [ ]: # your answer here In [ ]: bs_formula = robjects . Formula ( \"Gag ~ bs(Age, knots=r_quarts)\" ) bs_formula . environment [ 'Gag' ] = r_gag bs_formula . environment [ 'Age' ] = r_age bs_formula . environment [ 'r_quarts' ] = r_quarts bs_model = r_lm ( bs_formula ) bs_out = r_predict ( bs_model , predict_frame ) In [ ]: ax32 = GAGurine . plot . scatter ( x = 'Age' , y = 'GAG' , c = 'grey' , title = \"GAG in urine of children\" ) ax32 . set_xlabel ( \"Age\" ) ax32 . set_ylabel ( \"GAG\" ) ax32 . plot ( predict_frame . rx2 ( \"Age\" ), ns_out , color = 'red' ) ax32 . plot ( predict_frame . rx2 ( \"Age\" ), bs_out , color = 'blue' ) ax32 . legend ([ \"Natural spline, knots at quartiles\" , \"B-spline, knots at quartiles\" ]); 2. In [ ]: # your answer here In [ ]: overfit_formula = robjects . Formula ( \"Gag ~ bs(Age, knots=r_quarts)\" ) overfit_formula . environment [ 'Gag' ] = r_gag overfit_formula . environment [ 'Age' ] = r_age overfit_formula . environment [ 'r_quarts' ] = robjects . FloatVector ( np . array ([ 2 , 4 , 6 , 8 , 10 , 12 , 14 , 16 ])) overfit_model = r_lm ( overfit_formula ) overfit_out = r_predict ( overfit_model , predict_frame ) In [ ]: ax32 = GAGurine . plot . scatter ( x = 'Age' , y = 'GAG' , c = 'grey' , title = \"GAG in urine of children\" ) ax32 . set_xlabel ( \"Age\" ) ax32 . set_ylabel ( \"GAG\" ) ax32 . plot ( predict_frame . rx2 ( \"Age\" ), ns_out , color = 'red' ) ax32 . plot ( predict_frame . rx2 ( \"Age\" ), bs_out , color = 'blue' ) ax32 . plot ( predict_frame . rx2 ( \"Age\" ), overfit_out , color = 'green' ) ax32 . legend ([ \"Natural spline, knots at quartiles\" , \"B-spline, knots at quartiles\" , \"B-spline, lots of knots\" ]); In [ ]: #%R -i overfit_model plot(overfit_model) # we'd get the same diagnostic plot we get from an lm model GAMs We come, at last, to our most advanced model. The coding here isn't any more complex than we've done before, though the behind-the-scenes is awesome. First, let's get our (multivariate!) data In [ ]: kyphosis = pd . read_csv ( \"data/kyphosis.csv\" ) print ( \"\"\" # kyphosis - wherther a particular deformation was present post-operation # age - patient's age in months # number - the number of vertebrae involved in the operation # start - the number of the topmost vertebrae operated on \"\"\" ) display ( kyphosis . head ()) display ( kyphosis . describe ( include = 'all' )) display ( kyphosis . dtypes ) In [ ]: #If there are errors about missing R packages, run the code below: #r_utils = importr('utils') #r_utils.install_packages('codetools') #r_utils.install_packages('gam') To fit a GAM, we Import the gam library Populate a formula including s( ) on variables we want to fit smooths for Call gam(formula, family= ) where family is a string naming a probability distribution, chosen based on how the response variable is thought to occur. Rough family guidelines: Response is binary or \"N occurances out of M tries\", e.g. number of lab rats (out of 10) developing disease: chooose \"binomial\" Response is a count with no logical upper bound, e.g. number of ice creams sold: choose \"poisson\" Response is real, with normally-distributed noise, e.g. person's height: choose \"gaussian\" (the default) In [ ]: #There is a Python library in development for using GAMs (https://github.com/dswah/pyGAM) # but it is not yet as comprehensive as the R GAM library, which we will use here instead. # R also has the mgcv library, which implements some more advanced/flexible fitting methods r_gam_lib = importr ( 'gam' ) r_gam = r_gam_lib . gam r_kyph = robjects . FactorVector ( kyphosis [[ \"Kyphosis\" ]] . values ) r_Age = robjects . FloatVector ( kyphosis [[ \"Age\" ]] . values ) r_Number = robjects . FloatVector ( kyphosis [[ \"Number\" ]] . values ) r_Start = robjects . FloatVector ( kyphosis [[ \"Start\" ]] . values ) kyph1_fmla = robjects . Formula ( \"Kyphosis ~ s(Age) + s(Number) + s(Start)\" ) kyph1_fmla . environment [ 'Kyphosis' ] = r_kyph kyph1_fmla . environment [ 'Age' ] = r_Age kyph1_fmla . environment [ 'Number' ] = r_Number kyph1_fmla . environment [ 'Start' ] = r_Start kyph1_gam = r_gam ( kyph1_fmla , family = \"binomial\" ) The fitted gam model has a lot of interesting data within it In [ ]: print ( kyph1_gam . names ) Remember plotting? Calling R's plot() on a gam model is the easiest way to view the fitted splines In [ ]: % R -i kyph1_gam plot(kyph1_gam, residuals=TRUE,se=TRUE, scale=20); Prediction works like normal (build a data frame to predict on, if you don't already have one, and call predict() ). However, predict always reports the sum of the individual variable effects. If family is non-default this can be different from the actual prediction for that point. For instance, we're doing a 'logistic regression' so the raw prediction is log odds, but we can get probability by using in predict(..., type=\"response\") In [ ]: kyph_new = robjects . DataFrame ({ 'Age' : robjects . IntVector (( 84 , 85 , 86 )), 'Start' : robjects . IntVector (( 5 , 3 , 1 )), 'Number' : robjects . IntVector (( 1 , 6 , 10 ))}) print ( \"Raw response (so, Log odds):\" ) display ( r_predict ( kyph1_gam , kyph_new )) print ( \"Scaled response (so, probabilty of kyphosis):\" ) display ( r_predict ( kyph1_gam , kyph_new , type = \"response\" )) Discussion Exercise 6 What lambda did we use? What is the model telling us about the effects of age, starting vertebrae, and number of vertebae operated on If we fit a logistic regression instead, which variables might want quadratic terms. What is the cost and benefit of a logistic regression model versus a GAM? Critique the model: What is it assuming? Are the assumptions reasonable Are we using the right data? Does the model's story about the world make sense? Appendix GAMs and smoothing splines support hypothesis tets to compare models. (We can always compare models via out-of-sample prediction quality (i.e. performance on a validation set), but statistical ideas like hypothesis tests yet information criteria allow us to use all data for training and still compare the quality of model A to model B) In [ ]: r_anova = robjects . r [ \"anova\" ] kyph0_fmla = robjects . Formula ( \"Kyphosis~1\" ) kyph0_fmla . environment [ 'Kyphosis' ] = r_kyph kyph0_gam = r_gam ( kyph0_fmla , family = \"binomial\" ) print ( r_anova ( kyph0_gam , kyph1_gam , test = \"Chi\" )) Explicitly joining spline functions In [ ]: def h ( x , xi , pow_arg ): #pow is a reserved keyword in Python if ( x > xi ): return pow (( x - xi ), pow_arg ) else : return 0 h = np . vectorize ( h , otypes = [ np . float ]) #default behavior is to return ints, which gives incorrect answer #also, vectorize does not play nicely with default arguments, so better to set directly (e.g., pow_arg=1) In [ ]: xvals = np . arange ( 0 , 10.1 , 0.1 ) ax20 = plt . plot ( xvals , h ( xvals , 4 , 1 ), color = \"red\" ) _ = plt . title ( \"Truncated linear basis function with knot at x=4\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$(x-4)_+$\" ) #note the use of TeX in the label In [ ]: ax21 = plt . plot ( xvals , h ( xvals , 4 , 3 ), color = \"red\" ) _ = plt . title ( \"Truncated cubic basis function with knot at x=4\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$(x-4)_+&#94;3$\" ) In [ ]: ax22 = plt . plot ( xvals , 2 + xvals + 3 * h ( xvals , 2 , 1 ) - 4 * h ( xvals , 5 , 1 ) + 0.5 * h ( xvals , 8 , 1 ), color = \"red\" ) _ = plt . title ( \"Piecewise linear spline with knots at x=2, 5, and 8\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) Comparing splines to the (noisy) model that generated them. In [ ]: x = np . arange ( 0.1 , 10 , 9.9 / 100 ) from scipy.stats import norm #ppf (percent point function) is the rather unusual name for #the quantile or inverse CDF function in SciPy y = norm . ppf ( x / 10 ) + np . random . normal ( 0 , 0.4 , 100 ) ax23 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"3 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( 'y~x+h(x,2,1)+h(x,5,1)+h(x,8,1)' , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) In [ ]: ax24 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"6 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( 'y~x+h(x,1,1)+h(x,2,1)+h(x,3.5,1)+h(x,5,1)+h(x,6.5,1)+h(x,8,1)' , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) In [ ]: ax25 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"9 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( 'y~x+h(x,1,1)+h(x,2,1)+h(x,3,1)+h(x,4,1)+h(x,5,1)+h(x,6,1)+h(x,7,1)+h(x,8,1)+h(x,9,1)' , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) In [ ]: regstr = 'y~x+' for i in range ( 1 , 26 ): regstr += 'h(x,' + str ( i / 26 * 10 ) + ',1)+' regstr = regstr [: - 1 ] #drop last + ax26 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"25 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( regstr , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) Exercise: Try generating random data from different distributions and fitting polynomials of different degrees to it. What do you observe? In [ ]: # try it here In [ ]: #So, we see that increasing the number of knots results in a more polynomial-like fit In [ ]: #Next, we look at cubic splines with increasing numbers of knots ax27 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"3 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( 'y~x+np.power(x,2)+np.power(x,3)+h(x,2,3)+h(x,5,3)+h(x,8,3)' , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) In [ ]: ax28 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"6 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( 'y~x+np.power(x,2)+np.power(x,3)+h(x,1,3)+h(x,2,3)+h(x,3.5,3)+h(x,5,3)+h(x,6.5,3)+h(x,8,3)' , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) In [ ]: ax29 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"9 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( 'y~x+np.power(x,2)+np.power(x,3)+h(x,1,3)+h(x,2,3)+h(x,3,3)+h(x,4,3)+h(x,5,3)+h(x,6,3)+h(x,7,3)+h(x,8,3)+h(x,9,3)' , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) In [ ]: regstr2 = 'y~x+np.power(x,2)+np.power(x,3)+' for i in range ( 1 , 26 ): regstr2 += 'h(x,' + str ( i / 26 * 10 ) + ',3)+' regstr2 = regstr2 [: - 1 ] #drop last + ax30 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"25 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( regstr2 , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lab2/solutions/"},{"title":"Lab 2: Smooths and GAMs","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } CS109B Data Science 2: Advanced Topics in Data Science Lab 2 - Smoothers and Generalized Additive Models Harvard University Spring 2019 Instructors: Mark Glickman and Pavlos Protopapas Lab Instructors: Will Claybaugh Contributors: Paul Tyklin and Will Claybaugh In [1]: ## RUN THIS CELL TO PROPERLY HIGHLIGHT THE EXERCISES import requests from IPython.core.display import HTML styles = requests . get ( \"https://raw.githubusercontent.com/Harvard-IACS/2019-CS109B/master/content/styles/cs109.css\" ) . text HTML ( styles ) Out[1]: blockquote { background: #AEDE94; } h1 { padding-top: 25px; padding-bottom: 25px; text-align: left; padding-left: 10px; background-color: #DDDDDD; color: black; } h2 { padding-top: 10px; padding-bottom: 10px; text-align: left; padding-left: 5px; background-color: #EEEEEE; color: black; } div.exercise { background-color: #ffcccc; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; } div.discussion { background-color: #ccffcc; border-color: #88E97A; border-left: 5px solid #0A8000; padding: 0.5em; } div.theme { background-color: #DDDDDD; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; font-size: 18pt; } div.gc { background-color: #AEDE94; border-color: #E9967A; border-left: 5px solid #800080; padding: 0.5em; font-size: 12pt; } p.q1 { padding-top: 5px; padding-bottom: 5px; text-align: left; padding-left: 5px; background-color: #EEEEEE; color: black; } header { padding-top: 35px; padding-bottom: 35px; text-align: left; padding-left: 10px; background-color: #DDDDDD; color: black; } Learning Goals The main goal of this lab is to get familiar with calling R functions within Python. Along the way, we'll learn about the \"formula\" interface to statsmodels, which gives an intuitive way of specifying regression models, and we'll review the different approaches to fitting curves. Key Skills: Importing (base) R functions Importing R library functions Populating vectors R understands Populating dataframes R understands Populating formulas R understands Running models in R Getting results back to Python Getting model predictions in R Plotting in R Reading R's documentation In [ ]: import numpy as np import pandas as pd import matplotlib.pyplot as plt % matplotlib inline Linear/Polynomial Regression (Python, Review) Hopefully, you remember working with Statsmodels during 109a Reading data and (some) exploring in Pandas: In [ ]: diab = pd . read_csv ( \"data/diabetes.csv\" ) print ( \"\"\" # Variables are: # subject: subject ID number # age: age diagnosed with diabetes # acidity: a measure of acidity called base deficit # y: natural log of serum C-peptide concentration # # Original source is Sockett et al. (1987) # mentioned in Hastie and Tibshirani's book # \"Generalized Additive Models\". \"\"\" ) display ( diab . head ()) display ( diab . dtypes ) display ( diab . describe ()) Plotting with matplotlib: In [ ]: ax0 = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data\" ) #plotting direclty from pandas! ax0 . set_xlabel ( \"Age at Diagnosis\" ) ax0 . set_ylabel ( \"Log C-Peptide Concentration\" ); Linear regression with statsmodels. Previously, we worked from a vector of target values and a design matrix we built ourself (e.g. from PolynomialFeatures). Now, Statsmodels' formula interface can help build the target value and design matrix for you. In [ ]: #Using statsmodels import statsmodels.formula.api as sm model1 = sm . ols ( 'y ~ age' , data = diab ) fit1_lm = model1 . fit () Build a data frame to predict values on (sometimes this is just the test or validation set) Very useful for making pretty plots of the model predcitions -- predict for TONS of values, not just whatever's in the training set In [ ]: x_pred = np . linspace ( 0 , 16 , 100 ) predict_df = pd . DataFrame ( data = { \"age\" : x_pred }) predict_df . head () Use get_prediction( ).summary_frame() to get the model's prediction (and error bars!) In [ ]: prediction_output = fit1_lm . get_prediction ( predict_df ) . summary_frame () prediction_output . head () Plot the model and error bars In [ ]: ax1 = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data with least-squares linear fit\" ) ax1 . set_xlabel ( \"Age at Diagnosis\" ) ax1 . set_ylabel ( \"Log C-Peptide Concentration\" ) ax1 . plot ( predict_df . age , prediction_output [ 'mean' ], color = \"green\" ) ax1 . plot ( predict_df . age , prediction_output [ 'mean_ci_lower' ], color = \"blue\" , linestyle = \"dashed\" ) ax1 . plot ( predict_df . age , prediction_output [ 'mean_ci_upper' ], color = \"blue\" , linestyle = \"dashed\" ); ax1 . plot ( predict_df . age , prediction_output [ 'obs_ci_lower' ], color = \"skyblue\" , linestyle = \"dashed\" ) ax1 . plot ( predict_df . age , prediction_output [ 'obs_ci_upper' ], color = \"skyblue\" , linestyle = \"dashed\" ); Discussion What are the dark error bars? What are the light error bars? Exercise 1 Fit a 3rd degree polynomial model and plot the model+error bars Route1: Build a design df with a column for each of age , age**2 , age**3 Route2: Just edit the formula Answers : 1. In [ ]: # your code here 2. In [ ]: # your code here Linear/Polynomial Regression, but make it R This is the meat of the lab. After this section we'll know everything we need to in order to work with R models. The rest of the lab is just applying these concepts to run particular models. This section therefore is your 'cheat sheet' for working in R. What we need to know: Importing (base) R functions Importing R Library functions Populating vectors R understands Populating DataFrames R understands Populating Formulas R understands Running models in R Getting results back to Python Getting model predictions in R Plotting in R Reading R's documentation Importing R functions In [ ]: # if you're on JupyterHub you may need to specify the path to R #import os #os.environ['R_HOME'] = \"/usr/share/anaconda3/lib/R\" import rpy2.robjects as robjects In [ ]: r_lm = robjects . r [ \"lm\" ] r_predict = robjects . r [ \"predict\" ] #r_plot = robjects.r[\"plot\"] # more on plotting later #lm() and predict() are two of the most common functions we'll use Importing R libraries In [ ]: from rpy2.robjects.packages import importr #r_cluster = importr('cluster') #r_cluster.pam; Populating vectors R understands In [ ]: r_y = robjects . FloatVector ( diab [ 'y' ]) r_age = robjects . FloatVector ( diab [ 'age' ]) # What happens if we pass the wrong type? # How does r_age display? # How does r_age print? Populating Data Frames R understands In [ ]: diab_r = robjects . DataFrame ({ \"y\" : r_y , \"age\" : r_age }) # How does diab_r display? # How does diab_r print? Populating formulas R understands In [ ]: simple_formula = robjects . Formula ( \"y~age\" ) simple_formula . environment [ \"y\" ] = r_y #populate the formula's .environment, so it knows what 'y' and 'age' refer to simple_formula . environment [ \"age\" ] = r_age Running Models in R In [ ]: diab_lm = r_lm ( formula = simple_formula ) # the formula object is storing all the needed variables In [ ]: simple_formula = robjects . Formula ( \"y~age\" ) # reset the formula diab_lm = r_lm ( formula = simple_formula , data = diab_r ) #can also use a 'dumb' formula and pass a dataframe Getting results back to Python In [ ]: diab_lm #the result is already 'in' python, but it's a special object In [ ]: print ( diab_lm . names ) # view all names In [ ]: diab_lm [ 0 ] #grab the first element In [ ]: diab_lm . rx2 ( \"coefficients\" ) #use rx2 to get elements by name! In [ ]: np . array ( diab_lm . rx2 ( \"coefficients\" )) #r vectors can be converted to numpy (but rarely needed) Getting Predictions In [ ]: # make a df to predict on (might just be the validation or test dataframe) predict_df = robjects . DataFrame ({ \"age\" : robjects . FloatVector ( np . linspace ( 0 , 16 , 100 ))}) # call R's predict() function, passing the model and the data predictions = r_predict ( diab_lm , predict_df ) In [ ]: x_vals = predict_df . rx2 ( \"age\" ) In [ ]: ax = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Diabetes data\" ) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ); ax . plot ( x_vals , predictions ); #plt still works with r vectors as input! Plotting in R In [ ]: % load_ext rpy2.ipython The above turns on the %R \"magic\" R's plot() command responds differently based on what you hand to it; Different models get different plots! For any specific model search for plot.modelname. E.g. for a GAM model, search plot.gam for any details of plotting a GAM model The %R \"magic\" runs R code in 'notebook' mode, so figures display nicely Ahead of the plot( ) code we pass in the variables R needs to know about ( -i is for \"input\") In [ ]: % R -i diab_lm plot(diab_lm); Reading R's documentation The documentation for the lm() funciton is here , and a prettier version (same content) is here . When googling, perfer rdocumentation.org when possible. Sections: Usage : gives the function signature, including all optional arguments Arguments : What each function input controls Details : additional info on what the funciton does and how arguments interact. Often the right place to start reading Value : the structure of the object returned by the function Refferences : The relevant academic papers See Also : other functions of interest Exercise 2 Add confidence intervals calculated in R to the linear regression plot above. Use the interval= argument to r_predict() (documentation here ). You will have to work with a matrix returned by R. Fit a 5th degree polynomial to the diabetes data in R. Search the web for an easier method than writing out a formula with all 5 polynomial terms. Answers 1. In [ ]: # your code here 2. In [ ]: # your code here Lowess Smoothing Lowess Smoothing is implemented in both Python and R. We'll use it as another example as we transition languages. Discussion What is lowess smoothing? Which 109a models is it related to? How explainable is lowess? What are the tunable parameters? In Python In [ ]: from statsmodels.nonparametric.smoothers_lowess import lowess as lowess ss1 = lowess ( diab [ 'y' ], diab [ 'age' ], frac = 0.15 ) ss2 = lowess ( diab [ 'y' ], diab [ 'age' ], frac = 0.25 ) ss3 = lowess ( diab [ 'y' ], diab [ 'age' ], frac = 0.7 ) ss4 = lowess ( diab [ 'y' ], diab [ 'age' ], frac = 1 ) In [ ]: ss1 [: 10 ,:] # we get back simple a smoothed y value for each x value in the data Notice the clean code to plot different models. We'll see even cleaner code in a minute In [ ]: for cur_model , cur_frac in zip ([ ss1 , ss2 , ss3 , ss4 ],[ 0.15 , 0.25 , 0.7 , 1 ]): ax = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Lowess Fit, Fraction = {} \" . format ( cur_frac )) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ) ax . plot ( cur_model [:, 0 ], cur_model [:, 1 ], color = \"blue\" ) plt . show () Discussion Which model has high variance, which has high bias? What makes a model high variance or high bias? In R We need to: Import the loess function Send data over to R Call the function and get results In [ ]: r_loess = robjects . r [ 'loess.smooth' ] #extract R function r_y = robjects . FloatVector ( diab [ 'y' ]) r_age = robjects . FloatVector ( diab [ 'age' ]) ss1_r = r_loess ( r_age , r_y , span = 0.15 , degree = 1 ) In [ ]: ss1_r #again, a smoothed y value for each x value in the data Exercise 3 Predict the output of ss1_r[0] ss1_r.rx2(\"y\") 1. your answer here 2. your answer here Varying span Next, some extremely clean code to fit and plot models with various parameter settings. (Though the zip() method seen earlier is great when e.g. the label and the parameter differ) In [ ]: for cur_frac in [ 0.15 , 0.25 , 0.7 , 1 ]: cur_smooth = r_loess ( r_age , r_y , span = cur_frac ) ax = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"Lowess Fit, Fraction = {} \" . format ( cur_frac )) ax . set_xlabel ( \"Age at Diagnosis\" ) ax . set_ylabel ( \"Log C-Peptide Concentration\" ) ax . plot ( cur_smooth [ 0 ], cur_smooth [ 1 ], color = \"blue\" ) plt . show () Discussion Mark wasn't kidding; the Python and R results differ for frac=.15. Thoughts? Why isn't the bottom plot a straight line? We're using 100% of the data in each window... Smoothing Splines From this point forward, we're working with R functions; these models aren't (well) supported in Python. For clarity: this is the fancy spline model that minimizes $MSE - \\lambda\\cdot\\text{wiggle penalty}$ $=$ $\\sum_{i=1}&#94;N \\left(y_i - f(x_i)\\right)&#94;2 - \\lambda \\int \\left(f''(x)\\right)&#94;2$, across all possible functions $f$. The winner will always be a continuous, cubic polynomial with a knot at each data point Discussion Any idea why the winner is cubic? How interpretable is this model? What are the tunable parameters? In [ ]: r_smooth_spline = robjects . r [ 'smooth.spline' ] #extract R function # run smoothing function spline1 = r_smooth_spline ( r_age , r_y , spar = 0 ) Exercise 4 We actually set the spar parameter, a scale-free value that translates to a $\\lambda$ through a complex expression. Inspect the 'spline1' result and extract the implied value of $\\lambda$ Working from the fitting/plotting loop examples above, produce a plot like the one below for spar = [0,.5,.9,2], including axes labels and title. 1. In [ ]: # your answer here 2. In [ ]: # your answer here CV R's smooth_spline funciton has built-in CV to find a good lambda. See package docs . In [ ]: spline_cv = r_smooth_spline ( r_age , r_y , cv = True ) lambda_cv = spline_cv . rx2 ( \"lambda\" )[ 0 ] ax19 = diab . plot . scatter ( x = 'age' , y = 'y' , c = 'Red' , title = \"smoothing spline with $\\lambda=$\" + str ( np . round ( lambda_cv , 4 )) + \", chosen by cross-validation\" ) ax19 . set_xlabel ( \"Age at Diagnosis\" ) ax19 . set_ylabel ( \"Log C-Peptide Concentration\" ) ax19 . plot ( spline_cv . rx2 ( \"x\" ), spline_cv . rx2 ( \"y\" ), color = \"darkgreen\" ); Discussion Does the selected model look reasonable? How would you describe the effect of age at diagnosis on C_peptide concentration? What are the costs/benefits of the (fancy) spline model, relative to the linear regression we fit above? Natural & Basis Splines Here, we take a step backward on model complexity, but a step forward in coding complexity. We'll be working with R's formula interface again, so we will need to populate Formulas and DataFrames. Discussion In what way are Natural and Basis splines less complex than the splines we were just working with? What makes a spline 'natural'? What makes a spline 'basis'? What are the tuning parameters? In [ ]: #We will now work with a new dataset, called GAGurine. #The dataset description (from the R package MASS) is below: #Data were collected on the concentration of a chemical GAG # in the urine of 314 children aged from zero to seventeen years. # The aim of the study was to produce a chart to help a paediatrican # to assess if a child's GAG concentration is ‘normal'. #The variables are: # Age: age of child in years. # GAG: concentration of GAG (the units have been lost). In [ ]: GAGurine = pd . read_csv ( \"data/GAGurine.csv\" ) display ( GAGurine . head ()) ax31 = GAGurine . plot . scatter ( x = 'Age' , y = 'GAG' , c = 'black' , title = \"GAG in urine of children\" ) ax31 . set_xlabel ( \"Age\" ); ax31 . set_ylabel ( \"GAG\" ); Standard stuff: import function, convert variables to R format, call function In [ ]: from rpy2.robjects.packages import importr r_splines = importr ( 'splines' ) # populate R variables r_gag = robjects . FloatVector ( GAGurine [ 'GAG' ] . values ) r_age = robjects . FloatVector ( GAGurine [ 'Age' ] . values ) r_quarts = robjects . FloatVector ( np . quantile ( r_age ,[ . 25 , . 5 , . 75 ])) #woah, numpy functions run on R objects! What happens when we call the ns or bs functions from r_splines? In [ ]: ns_design = r_splines . ns ( r_age , knots = r_quarts ) bs_design = r_splines . bs ( r_age , knots = r_quarts ) In [ ]: print ( ns_design ) ns and bs return design matrices, not model objects! That's because they're meant to work with lm 's formula interface. To get a model object we populate a formula including ns( , ) and fit to data In [ ]: r_lm = robjects . r [ 'lm' ] r_predict = robjects . r [ 'predict' ] # populate the formula ns_formula = robjects . Formula ( \"Gag ~ ns(Age, knots=r_quarts)\" ) ns_formula . environment [ 'Gag' ] = r_gag ns_formula . environment [ 'Age' ] = r_age ns_formula . environment [ 'r_quarts' ] = r_quarts # fit the model ns_model = r_lm ( ns_formula ) Predict like usual: build a dataframe to predict on and call predict() In [ ]: # predict predict_frame = robjects . DataFrame ({ \"Age\" : robjects . FloatVector ( np . linspace ( 0 , 20 , 100 ))}) ns_out = r_predict ( ns_model , predict_frame ) In [ ]: ax32 = GAGurine . plot . scatter ( x = 'Age' , y = 'GAG' , c = 'grey' , title = \"GAG in urine of children\" ) ax32 . set_xlabel ( \"Age\" ) ax32 . set_ylabel ( \"GAG\" ) ax32 . plot ( predict_frame . rx2 ( \"Age\" ), ns_out , color = 'red' ) ax32 . legend ([ \"Natural spline, knots at quartiles\" ]); Exercise 5 Fit a basis spline model with the same knots, and add it to the plot above Fit a basis spline with 8 knots placed at [2,4,6...14,16] and add it to the plot above Answers: 1. In [ ]: # your answer here 2. In [ ]: # your answer here In [ ]: #%R -i overfit_model plot(overfit_model) # we'd get the same diagnostic plot we get from an lm model GAMs We come, at last, to our most advanced model. The coding here isn't any more complex than we've done before, though the behind-the-scenes is awesome. First, let's get our (multivariate!) data In [ ]: kyphosis = pd . read_csv ( \"data/kyphosis.csv\" ) print ( \"\"\" # kyphosis - wherther a particular deformation was present post-operation # age - patient's age in months # number - the number of vertebrae involved in the operation # start - the number of the topmost vertebrae operated on \"\"\" ) display ( kyphosis . head ()) display ( kyphosis . describe ( include = 'all' )) display ( kyphosis . dtypes ) In [ ]: #If there are errors about missing R packages, run the code below: #r_utils = importr('utils') #r_utils.install_packages('codetools') #r_utils.install_packages('gam') To fit a GAM, we Import the gam library Populate a formula including s( ) on variables we want to fit smooths for Call gam(formula, family= ) where family is a string naming a probability distribution, chosen based on how the response variable is thought to occur. Rough family guidelines: Response is binary or \"N occurances out of M tries\", e.g. number of lab rats (out of 10) developing disease: chooose \"binomial\" Response is a count with no logical upper bound, e.g. number of ice creams sold: choose \"poisson\" Response is real, with normally-distributed noise, e.g. person's height: choose \"gaussian\" (the default) In [ ]: #There is a Python library in development for using GAMs (https://github.com/dswah/pyGAM) # but it is not yet as comprehensive as the R GAM library, which we will use here instead. # R also has the mgcv library, which implements some more advanced/flexible fitting methods r_gam_lib = importr ( 'gam' ) r_gam = r_gam_lib . gam r_kyph = robjects . FactorVector ( kyphosis [[ \"Kyphosis\" ]] . values ) r_Age = robjects . FloatVector ( kyphosis [[ \"Age\" ]] . values ) r_Number = robjects . FloatVector ( kyphosis [[ \"Number\" ]] . values ) r_Start = robjects . FloatVector ( kyphosis [[ \"Start\" ]] . values ) kyph1_fmla = robjects . Formula ( \"Kyphosis ~ s(Age) + s(Number) + s(Start)\" ) kyph1_fmla . environment [ 'Kyphosis' ] = r_kyph kyph1_fmla . environment [ 'Age' ] = r_Age kyph1_fmla . environment [ 'Number' ] = r_Number kyph1_fmla . environment [ 'Start' ] = r_Start kyph1_gam = r_gam ( kyph1_fmla , family = \"binomial\" ) The fitted gam model has a lot of interesting data within it In [ ]: print ( kyph1_gam . names ) Remember plotting? Calling R's plot() on a gam model is the easiest way to view the fitted splines In [ ]: % R -i kyph1_gam plot(kyph1_gam, residuals=TRUE,se=TRUE, scale=20); Prediction works like normal (build a data frame to predict on, if you don't already have one, and call predict() ). However, predict always reports the sum of the individual variable effects. If family is non-default this can be different from the actual prediction for that point. For instance, we're doing a 'logistic regression' so the raw prediction is log odds, but we can get probability by using in predict(..., type=\"response\") In [ ]: kyph_new = robjects . DataFrame ({ 'Age' : robjects . IntVector (( 84 , 85 , 86 )), 'Start' : robjects . IntVector (( 5 , 3 , 1 )), 'Number' : robjects . IntVector (( 1 , 6 , 10 ))}) print ( \"Raw response (so, Log odds):\" ) display ( r_predict ( kyph1_gam , kyph_new )) print ( \"Scaled response (so, probabilty of kyphosis):\" ) display ( r_predict ( kyph1_gam , kyph_new , type = \"response\" )) Discussion Exercise 6 What lambda did we use? What is the model telling us about the effects of age, starting vertebrae, and number of vertebae operated on If we fit a logistic regression instead, which variables might want quadratic terms. What is the cost and benefit of a logistic regression model versus a GAM? Critique the model: What is it assuming? Are the assumptions reasonable Are we using the right data? Does the model's story about the world make sense? Appendix GAMs and smoothing splines support hypothesis tets to compare models. (We can always compare models via out-of-sample prediction quality (i.e. performance on a validation set), but statistical ideas like hypothesis tests yet information criteria allow us to use all data for training and still compare the quality of model A to model B) In [ ]: r_anova = robjects . r [ \"anova\" ] kyph0_fmla = robjects . Formula ( \"Kyphosis~1\" ) kyph0_fmla . environment [ 'Kyphosis' ] = r_kyph kyph0_gam = r_gam ( kyph0_fmla , family = \"binomial\" ) print ( r_anova ( kyph0_gam , kyph1_gam , test = \"Chi\" )) Explicitly joining spline functions In [ ]: def h ( x , xi , pow_arg ): #pow is a reserved keyword in Python if ( x > xi ): return pow (( x - xi ), pow_arg ) else : return 0 h = np . vectorize ( h , otypes = [ np . float ]) #default behavior is to return ints, which gives incorrect answer #also, vectorize does not play nicely with default arguments, so better to set directly (e.g., pow_arg=1) In [ ]: xvals = np . arange ( 0 , 10.1 , 0.1 ) ax20 = plt . plot ( xvals , h ( xvals , 4 , 1 ), color = \"red\" ) _ = plt . title ( \"Truncated linear basis function with knot at x=4\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$(x-4)_+$\" ) #note the use of TeX in the label In [ ]: ax21 = plt . plot ( xvals , h ( xvals , 4 , 3 ), color = \"red\" ) _ = plt . title ( \"Truncated cubic basis function with knot at x=4\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$(x-4)_+&#94;3$\" ) In [ ]: ax22 = plt . plot ( xvals , 2 + xvals + 3 * h ( xvals , 2 , 1 ) - 4 * h ( xvals , 5 , 1 ) + 0.5 * h ( xvals , 8 , 1 ), color = \"red\" ) _ = plt . title ( \"Piecewise linear spline with knots at x=2, 5, and 8\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) Comparing splines to the (noisy) model that generated them. In [ ]: x = np . arange ( 0.1 , 10 , 9.9 / 100 ) from scipy.stats import norm #ppf (percent point function) is the rather unusual name for #the quantile or inverse CDF function in SciPy y = norm . ppf ( x / 10 ) + np . random . normal ( 0 , 0.4 , 100 ) ax23 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"3 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( 'y~x+h(x,2,1)+h(x,5,1)+h(x,8,1)' , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) In [ ]: ax24 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"6 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( 'y~x+h(x,1,1)+h(x,2,1)+h(x,3.5,1)+h(x,5,1)+h(x,6.5,1)+h(x,8,1)' , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) In [ ]: ax25 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"9 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( 'y~x+h(x,1,1)+h(x,2,1)+h(x,3,1)+h(x,4,1)+h(x,5,1)+h(x,6,1)+h(x,7,1)+h(x,8,1)+h(x,9,1)' , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) In [ ]: regstr = 'y~x+' for i in range ( 1 , 26 ): regstr += 'h(x,' + str ( i / 26 * 10 ) + ',1)+' regstr = regstr [: - 1 ] #drop last + ax26 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"25 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( regstr , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) Exercise: Try generating random data from different distributions and fitting polynomials of different degrees to it. What do you observe? In [ ]: # try it here In [ ]: #So, we see that increasing the number of knots results in a more polynomial-like fit In [ ]: #Next, we look at cubic splines with increasing numbers of knots ax27 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"3 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( 'y~x+np.power(x,2)+np.power(x,3)+h(x,2,3)+h(x,5,3)+h(x,8,3)' , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) In [ ]: ax28 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"6 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( 'y~x+np.power(x,2)+np.power(x,3)+h(x,1,3)+h(x,2,3)+h(x,3.5,3)+h(x,5,3)+h(x,6.5,3)+h(x,8,3)' , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) In [ ]: ax29 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"9 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( 'y~x+np.power(x,2)+np.power(x,3)+h(x,1,3)+h(x,2,3)+h(x,3,3)+h(x,4,3)+h(x,5,3)+h(x,6,3)+h(x,7,3)+h(x,8,3)+h(x,9,3)' , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) In [ ]: regstr2 = 'y~x+np.power(x,2)+np.power(x,3)+' for i in range ( 1 , 26 ): regstr2 += 'h(x,' + str ( i / 26 * 10 ) + ',3)+' regstr2 = regstr2 [: - 1 ] #drop last + ax30 = plt . scatter ( x , y , facecolors = 'none' , edgecolors = 'black' ) _ = plt . title ( \"25 knots\" ) _ = plt . xlabel ( \"$x$\" ) _ = plt . ylabel ( \"$y$\" ) _ = plt . plot ( x , sm . ols ( regstr2 , data = { 'x' : x , 'y' : y }) . fit () . predict (), color = \"darkblue\" , linewidth = 2 ) _ = plt . plot ( x , norm . ppf ( x / 10 ), color = \"red\" ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lab2/students/"},{"title":"Lecture 4: Smoothing and Additive 3/3","text":"Slides This lecture is only available to registered students Lecture 2-4 PDF Associated Material Lab 2 Lab 2 solutions","tags":"pages","url":"pages/lecture4/"},{"title":"Lecture 2: Smoothing and Additive 1/3","text":"Slides This lecture is only available to registered students Lecture 2-4 PDF Associated Material Lab 2 Lab 2 solutions","tags":"pages","url":"pages/lecture2/"},{"title":"Lecture 3: Smoothing and Additive 2/3","text":"Slides This lecture is only available to registered students Lecture 2-4 PDF Associated Material Lab 2 Lab 2 solutions","tags":"pages","url":"pages/lecture3/"},{"title":"Lab 1: Setting up environment","text":"Slides PDF PPTX Notebooks R_setup Notes Installation Instructions for JupyterHub","tags":"labs","url":"labs/lab1/"},{"title":"Lab 1: R set up","text":"/*! * * IPython notebook * */ /* CSS font colors for translated ANSI colors. */ .ansibold { font-weight: bold; } /* use dark versions for foreground, to improve visibility */ .ansiblack { color: black; } .ansired { color: darkred; } .ansigreen { color: darkgreen; } .ansiyellow { color: #c4a000; } .ansiblue { color: darkblue; } .ansipurple { color: darkviolet; } .ansicyan { color: steelblue; } .ansigray { color: gray; } /* and light for background, for the same reason */ .ansibgblack { background-color: black; } .ansibgred { background-color: red; } .ansibggreen { background-color: green; } .ansibgyellow { background-color: yellow; } .ansibgblue { background-color: blue; } .ansibgpurple { background-color: magenta; } .ansibgcyan { background-color: cyan; } .ansibggray { background-color: gray; } div.cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; border-radius: 2px; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; border-width: 1px; border-style: solid; border-color: transparent; width: 100%; padding: 5px; /* This acts as a spacer between cells, that is outside the border */ margin: 0px; outline: none; border-left-width: 1px; padding-left: 5px; background: linear-gradient(to right, transparent -40px, transparent 1px, transparent 1px, transparent 100%); } div.cell.jupyter-soft-selected { border-left-color: #90CAF9; border-left-color: #E3F2FD; border-left-width: 1px; padding-left: 5px; border-right-color: #E3F2FD; border-right-width: 1px; background: #E3F2FD; } @media print { div.cell.jupyter-soft-selected { border-color: transparent; } } div.cell.selected { border-color: #ababab; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 5px, transparent 5px, transparent 100%); } @media print { div.cell.selected { border-color: transparent; } } div.cell.selected.jupyter-soft-selected { border-left-width: 0; padding-left: 6px; background: linear-gradient(to right, #42A5F5 -40px, #42A5F5 7px, #E3F2FD 7px, #E3F2FD 100%); } .edit_mode div.cell.selected { border-color: #66BB6A; border-left-width: 0px; padding-left: 6px; background: linear-gradient(to right, #66BB6A -40px, #66BB6A 5px, transparent 5px, transparent 100%); } @media print { .edit_mode div.cell.selected { border-color: transparent; } } .prompt { /* This needs to be wide enough for 3 digit prompt numbers: In[100]: */ min-width: 14ex; /* This padding is tuned to match the padding on the CodeMirror editor. */ padding: 0.4em; margin: 0px; font-family: monospace; text-align: right; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; /* Don't highlight prompt number selection */ -webkit-touch-callout: none; -webkit-user-select: none; -khtml-user-select: none; -moz-user-select: none; -ms-user-select: none; user-select: none; /* Use default cursor */ cursor: default; } @media (max-width: 540px) { .prompt { text-align: left; } } div.inner_cell { min-width: 0; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_area { border: 1px solid #cfcfcf; border-radius: 2px; background: #f7f7f7; line-height: 1.21429em; } /* This is needed so that empty prompt areas can collapse to zero height when there is no content in the output_subarea and the prompt. The main purpose of this is to make sure that empty JavaScript output_subareas have no height. */ div.prompt:empty { padding-top: 0; padding-bottom: 0; } div.unrecognized_cell { padding: 5px 5px 5px 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.unrecognized_cell .inner_cell { border-radius: 2px; padding: 5px; font-weight: bold; color: red; border: 1px solid #cfcfcf; background: #eaeaea; } div.unrecognized_cell .inner_cell a { color: inherit; text-decoration: none; } div.unrecognized_cell .inner_cell a:hover { color: inherit; text-decoration: none; } @media (max-width: 540px) { div.unrecognized_cell > div.prompt { display: none; } } div.code_cell { /* avoid page breaking on code cells when printing */ } @media print { div.code_cell { page-break-inside: avoid; } } /* any special styling for code cells that are currently running goes here */ div.input { page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.input { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } /* input_area and input_prompt must match in top border and margin for alignment */ div.input_prompt { color: #303F9F; border-top: 1px solid transparent; } div.input_area > div.highlight { margin: 0.4em; border: none; padding: 0px; background-color: transparent; } div.input_area > div.highlight > pre { margin: 0px; border: none; padding: 0px; background-color: transparent; } /* The following gets added to the <head> if it is detected that the user has a * monospace font with inconsistent normal/bold/italic height. See * notebookmain.js. Such fonts will have keywords vertically offset with * respect to the rest of the text. The user should select a better font. * See: https://github.com/ipython/ipython/issues/1503 * * .CodeMirror span { * vertical-align: bottom; * } */ .CodeMirror { line-height: 1.21429em; /* Changed from 1em to our global default */ font-size: 14px; height: auto; /* Changed to auto to autogrow */ background: none; /* Changed from white to allow our bg to show through */ } .CodeMirror-scroll { /* The CodeMirror docs are a bit fuzzy on if overflow-y should be hidden or visible.*/ /* We have found that if it is visible, vertical scrollbars appear with font size changes.*/ overflow-y: hidden; overflow-x: auto; } .CodeMirror-lines { /* In CM2, this used to be 0.4em, but in CM3 it went to 4px. We need the em value because */ /* we have set a different line-height and want this to scale with that. */ padding: 0.4em; } .CodeMirror-linenumber { padding: 0 8px 0 4px; } .CodeMirror-gutters { border-bottom-left-radius: 2px; border-top-left-radius: 2px; } .CodeMirror pre { /* In CM3 this went to 4px from 0 in CM2. We need the 0 value because of how we size */ /* .CodeMirror-lines */ padding: 0; border: 0; border-radius: 0; } /* Original style from softwaremaniacs.org (c) Ivan Sagalaev <Maniac@SoftwareManiacs.Org> Adapted from GitHub theme */ .highlight-base { color: #000; } .highlight-variable { color: #000; } .highlight-variable-2 { color: #1a1a1a; } .highlight-variable-3 { color: #333333; } .highlight-string { color: #BA2121; } .highlight-comment { color: #408080; font-style: italic; } .highlight-number { color: #080; } .highlight-atom { color: #88F; } .highlight-keyword { color: #008000; font-weight: bold; } .highlight-builtin { color: #008000; } .highlight-error { color: #f00; } .highlight-operator { color: #AA22FF; font-weight: bold; } .highlight-meta { color: #AA22FF; } /* previously not defined, copying from default codemirror */ .highlight-def { color: #00f; } .highlight-string-2 { color: #f50; } .highlight-qualifier { color: #555; } .highlight-bracket { color: #997; } .highlight-tag { color: #170; } .highlight-attribute { color: #00c; } .highlight-header { color: blue; } .highlight-quote { color: #090; } .highlight-link { color: #00c; } /* apply the same style to codemirror */ .cm-s-ipython span.cm-keyword { color: #008000; font-weight: bold; } .cm-s-ipython span.cm-atom { color: #88F; } .cm-s-ipython span.cm-number { color: #080; } .cm-s-ipython span.cm-def { color: #00f; } .cm-s-ipython span.cm-variable { color: #000; } .cm-s-ipython span.cm-operator { color: #AA22FF; font-weight: bold; } .cm-s-ipython span.cm-variable-2 { color: #1a1a1a; } .cm-s-ipython span.cm-variable-3 { color: #333333; } .cm-s-ipython span.cm-comment { color: #408080; font-style: italic; } .cm-s-ipython span.cm-string { color: #BA2121; } .cm-s-ipython span.cm-string-2 { color: #f50; } .cm-s-ipython span.cm-meta { color: #AA22FF; } .cm-s-ipython span.cm-qualifier { color: #555; } .cm-s-ipython span.cm-builtin { color: #008000; } .cm-s-ipython span.cm-bracket { color: #997; } .cm-s-ipython span.cm-tag { color: #170; } .cm-s-ipython span.cm-attribute { color: #00c; } .cm-s-ipython span.cm-header { color: blue; } .cm-s-ipython span.cm-quote { color: #090; } .cm-s-ipython span.cm-link { color: #00c; } .cm-s-ipython span.cm-error { color: #f00; } .cm-s-ipython span.cm-tab { background: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAADAAAAAMCAYAAAAkuj5RAAAAAXNSR0IArs4c6QAAAGFJREFUSMft1LsRQFAQheHPowAKoACx3IgEKtaEHujDjORSgWTH/ZOdnZOcM/sgk/kFFWY0qV8foQwS4MKBCS3qR6ixBJvElOobYAtivseIE120FaowJPN75GMu8j/LfMwNjh4HUpwg4LUAAAAASUVORK5CYII=); background-position: right; background-repeat: no-repeat; } div.output_wrapper { /* this position must be relative to enable descendents to be absolute within it */ position: relative; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; z-index: 1; } /* class for the output area when it should be height-limited */ div.output_scroll { /* ideally, this would be max-height, but FF barfs all over that */ height: 24em; /* FF needs this *and the wrapper* to specify full width, or it will shrinkwrap */ width: 100%; overflow: auto; border-radius: 2px; -webkit-box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); box-shadow: inset 0 2px 8px rgba(0, 0, 0, 0.8); display: block; } /* output div while it is collapsed */ div.output_collapsed { margin: 0px; padding: 0px; /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } div.out_prompt_overlay { height: 100%; padding: 0px 0.4em; position: absolute; border-radius: 2px; } div.out_prompt_overlay:hover { /* use inner shadow to get border that is computed the same on WebKit/FF */ -webkit-box-shadow: inset 0 0 1px #000; box-shadow: inset 0 0 1px #000; background: rgba(240, 240, 240, 0.5); } div.output_prompt { color: #D84315; } /* This class is the outer container of all output sections. */ div.output_area { padding: 0px; page-break-inside: avoid; /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } div.output_area .MathJax_Display { text-align: left !important; } div.output_area div.output_area div.output_area img, div.output_area svg { max-width: 100%; height: auto; } div.output_area img.unconfined, div.output_area svg.unconfined { max-width: none; } /* This is needed to protect the pre formating from global settings such as that of bootstrap */ .output { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } @media (max-width: 540px) { div.output_area { /* Old browsers */ display: -webkit-box; -webkit-box-orient: vertical; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: vertical; -moz-box-align: stretch; display: box; box-orient: vertical; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: column; align-items: stretch; } } div.output_area pre { margin: 0; padding: 0; border: 0; vertical-align: baseline; color: black; background-color: transparent; border-radius: 0; } /* This class is for the output subarea inside the output_area and after the prompt div. */ div.output_subarea { overflow-x: auto; padding: 0.4em; /* Old browsers */ -webkit-box-flex: 1; -moz-box-flex: 1; box-flex: 1; /* Modern browsers */ flex: 1; max-width: calc(100% - 14ex); } div.output_scroll div.output_subarea { overflow-x: visible; } /* The rest of the output_* classes are for special styling of the different output types */ /* all text output has this class: */ div.output_text { text-align: left; color: #000; /* This has to match that of the the CodeMirror class line-height below */ line-height: 1.21429em; } /* stdout/stderr are 'text' as well as 'stream', but execute_result/error are *not* streams */ div.output_stderr { background: #fdd; /* very light red background for stderr */ } div.output_latex { text-align: left; } /* Empty output_javascript divs should have no height */ div.output_javascript:empty { padding: 0; } .js-error { color: darkred; } /* raw_input styles */ div.raw_input_container { line-height: 1.21429em; padding-top: 5px; } pre.raw_input_prompt { /* nothing needed here. */ } input.raw_input { font-family: monospace; font-size: inherit; color: inherit; width: auto; /* make sure input baseline aligns with prompt */ vertical-align: baseline; /* padding + margin = 0.5em between prompt and cursor */ padding: 0em 0.25em; margin: 0em 0.25em; } input.raw_input:focus { box-shadow: none; } p.p-space { margin-bottom: 10px; } div.output_unrecognized { padding: 5px; font-weight: bold; color: red; } div.output_unrecognized a { color: inherit; text-decoration: none; } div.output_unrecognized a:hover { color: inherit; text-decoration: none; } .rendered_html { color: #000; /* any extras will just be numbers: */ } .rendered_html :link { text-decoration: underline; } .rendered_html :visited { text-decoration: underline; } .rendered_html h1:first-child { margin-top: 0.538em; } .rendered_html h2:first-child { margin-top: 0.636em; } .rendered_html h3:first-child { margin-top: 0.777em; } .rendered_html h4:first-child { margin-top: 1em; } .rendered_html h5:first-child { margin-top: 1em; } .rendered_html h6:first-child { margin-top: 1em; } .rendered_html * + ul { margin-top: 1em; } .rendered_html * + ol { margin-top: 1em; } .rendered_html pre, .rendered_html tr, .rendered_html th, .rendered_html td, .rendered_html * + table { margin-top: 1em; } .rendered_html * + p { margin-top: 1em; } .rendered_html * + img { margin-top: 1em; } .rendered_html img, .rendered_html img.unconfined, div.text_cell { /* Old browsers */ display: -webkit-box; -webkit-box-orient: horizontal; -webkit-box-align: stretch; display: -moz-box; -moz-box-orient: horizontal; -moz-box-align: stretch; display: box; box-orient: horizontal; box-align: stretch; /* Modern browsers */ display: flex; flex-direction: row; align-items: stretch; } @media (max-width: 540px) { div.text_cell > div.prompt { display: none; } } div.text_cell_render { /*font-family: \"Helvetica Neue\", Arial, Helvetica, Geneva, sans-serif;*/ outline: none; resize: none; width: inherit; border-style: none; padding: 0.5em 0.5em 0.5em 0.4em; color: #000; box-sizing: border-box; -moz-box-sizing: border-box; -webkit-box-sizing: border-box; } a.anchor-link:link { text-decoration: none; padding: 0px 20px; visibility: hidden; } h1:hover .anchor-link, h2:hover .anchor-link, h3:hover .anchor-link, h4:hover .anchor-link, h5:hover .anchor-link, h6:hover .anchor-link { visibility: visible; } .text_cell.rendered .input_area { display: none; } .text_cell.rendered .text_cell.unrendered .text_cell_render { display: none; } .cm-header-1, .cm-header-2, .cm-header-3, .cm-header-4, .cm-header-5, .cm-header-6 { font-weight: bold; font-family: \"Helvetica Neue\", Helvetica, Arial, sans-serif; } .cm-header-1 { font-size: 185.7%; } .cm-header-2 { font-size: 157.1%; } .cm-header-3 { font-size: 128.6%; } .cm-header-4 { font-size: 110%; } .cm-header-5 { font-size: 100%; font-style: italic; } .cm-header-6 { font-size: 100%; font-style: italic; } .highlight .hll { background-color: #ffffcc } .highlight { background: #f8f8f8; } .highlight .c { color: #408080; font-style: italic } /* Comment */ .highlight .err { border: 1px solid #FF0000 } /* Error */ .highlight .k { color: #008000; font-weight: bold } /* Keyword */ .highlight .o { color: #666666 } /* Operator */ .highlight .ch { color: #408080; font-style: italic } /* Comment.Hashbang */ .highlight .cm { color: #408080; font-style: italic } /* Comment.Multiline */ .highlight .cp { color: #BC7A00 } /* Comment.Preproc */ .highlight .cpf { color: #408080; font-style: italic } /* Comment.PreprocFile */ .highlight .c1 { color: #408080; font-style: italic } /* Comment.Single */ .highlight .cs { color: #408080; font-style: italic } /* Comment.Special */ .highlight .gd { color: #A00000 } /* Generic.Deleted */ .highlight .ge { font-style: italic } /* Generic.Emph */ .highlight .gr { color: #FF0000 } /* Generic.Error */ .highlight .gh { color: #000080; font-weight: bold } /* Generic.Heading */ .highlight .gi { color: #00A000 } /* Generic.Inserted */ .highlight .go { color: #888888 } /* Generic.Output */ .highlight .gp { color: #000080; font-weight: bold } /* Generic.Prompt */ .highlight .gs { font-weight: bold } /* Generic.Strong */ .highlight .gu { color: #800080; font-weight: bold } /* Generic.Subheading */ .highlight .gt { color: #0044DD } /* Generic.Traceback */ .highlight .kc { color: #008000; font-weight: bold } /* Keyword.Constant */ .highlight .kd { color: #008000; font-weight: bold } /* Keyword.Declaration */ .highlight .kn { color: #008000; font-weight: bold } /* Keyword.Namespace */ .highlight .kp { color: #008000 } /* Keyword.Pseudo */ .highlight .kr { color: #008000; font-weight: bold } /* Keyword.Reserved */ .highlight .kt { color: #B00040 } /* Keyword.Type */ .highlight .m { color: #666666 } /* Literal.Number */ .highlight .s { color: #BA2121 } /* Literal.String */ .highlight .na { color: #7D9029 } /* Name.Attribute */ .highlight .nb { color: #008000 } /* Name.Builtin */ .highlight .nc { color: #0000FF; font-weight: bold } /* Name.Class */ .highlight .no { color: #880000 } /* Name.Constant */ .highlight .nd { color: #AA22FF } /* Name.Decorator */ .highlight .ni { color: #999999; font-weight: bold } /* Name.Entity */ .highlight .ne { color: #D2413A; font-weight: bold } /* Name.Exception */ .highlight .nf { color: #0000FF } /* Name.Function */ .highlight .nl { color: #A0A000 } /* Name.Label */ .highlight .nn { color: #0000FF; font-weight: bold } /* Name.Namespace */ .highlight .nt { color: #008000; font-weight: bold } /* Name.Tag */ .highlight .nv { color: #19177C } /* Name.Variable */ .highlight .ow { color: #AA22FF; font-weight: bold } /* Operator.Word */ .highlight .w { color: #bbbbbb } /* Text.Whitespace */ .highlight .mb { color: #666666 } /* Literal.Number.Bin */ .highlight .mf { color: #666666 } /* Literal.Number.Float */ .highlight .mh { color: #666666 } /* Literal.Number.Hex */ .highlight .mi { color: #666666 } /* Literal.Number.Integer */ .highlight .mo { color: #666666 } /* Literal.Number.Oct */ .highlight .sa { color: #BA2121 } /* Literal.String.Affix */ .highlight .sb { color: #BA2121 } /* Literal.String.Backtick */ .highlight .sc { color: #BA2121 } /* Literal.String.Char */ .highlight .dl { color: #BA2121 } /* Literal.String.Delimiter */ .highlight .sd { color: #BA2121; font-style: italic } /* Literal.String.Doc */ .highlight .s2 { color: #BA2121 } /* Literal.String.Double */ .highlight .se { color: #BB6622; font-weight: bold } /* Literal.String.Escape */ .highlight .sh { color: #BA2121 } /* Literal.String.Heredoc */ .highlight .si { color: #BB6688; font-weight: bold } /* Literal.String.Interpol */ .highlight .sx { color: #008000 } /* Literal.String.Other */ .highlight .sr { color: #BB6688 } /* Literal.String.Regex */ .highlight .s1 { color: #BA2121 } /* Literal.String.Single */ .highlight .ss { color: #19177C } /* Literal.String.Symbol */ .highlight .bp { color: #008000 } /* Name.Builtin.Pseudo */ .highlight .fm { color: #0000FF } /* Name.Function.Magic */ .highlight .vc { color: #19177C } /* Name.Variable.Class */ .highlight .vg { color: #19177C } /* Name.Variable.Global */ .highlight .vi { color: #19177C } /* Name.Variable.Instance */ .highlight .vm { color: #19177C } /* Name.Variable.Magic */ .highlight .il { color: #666666 } /* Literal.Number.Integer.Long */ /* Temporary definitions which will become obsolete with Notebook release 5.0 */ .ansi-black-fg { color: #3E424D; } .ansi-black-bg { background-color: #3E424D; } .ansi-black-intense-fg { color: #282C36; } .ansi-black-intense-bg { background-color: #282C36; } .ansi-red-fg { color: #E75C58; } .ansi-red-bg { background-color: #E75C58; } .ansi-red-intense-fg { color: #B22B31; } .ansi-red-intense-bg { background-color: #B22B31; } .ansi-green-fg { color: #00A250; } .ansi-green-bg { background-color: #00A250; } .ansi-green-intense-fg { color: #007427; } .ansi-green-intense-bg { background-color: #007427; } .ansi-yellow-fg { color: #DDB62B; } .ansi-yellow-bg { background-color: #DDB62B; } .ansi-yellow-intense-fg { color: #B27D12; } .ansi-yellow-intense-bg { background-color: #B27D12; } .ansi-blue-fg { color: #208FFB; } .ansi-blue-bg { background-color: #208FFB; } .ansi-blue-intense-fg { color: #0065CA; } .ansi-blue-intense-bg { background-color: #0065CA; } .ansi-magenta-fg { color: #D160C4; } .ansi-magenta-bg { background-color: #D160C4; } .ansi-magenta-intense-fg { color: #A03196; } .ansi-magenta-intense-bg { background-color: #A03196; } .ansi-cyan-fg { color: #60C6C8; } .ansi-cyan-bg { background-color: #60C6C8; } .ansi-cyan-intense-fg { color: #258F8F; } .ansi-cyan-intense-bg { background-color: #258F8F; } .ansi-white-fg { color: #C5C1B4; } .ansi-white-bg { background-color: #C5C1B4; } .ansi-white-intense-fg { color: #A1A6B2; } .ansi-white-intense-bg { background-color: #A1A6B2; } .ansi-bold { font-weight: bold; } In [ ]: #---------Test Imports------------- import numpy as np import keras import gensim import nltk keras . layers . Dense ( 20 ) In [ ]: #-------Download R packages--------- ## these two lines for JupyterHub only #import os #os.environ['R_HOME'] = \"/usr/share/anaconda3/lib/R\" import rpy2 from rpy2.robjects.packages import importr r_utils = importr ( 'utils' ) package_list = [ 'aplpack' , 'cluster' , 'codetools' , 'dbscan' , 'factoextra' , 'gam' , 'ggplot2' , 'splines' , 'TeachingDemos' ] for name in package_list : r_utils . install_packages ( name ) import rpy2 from rpy2.robjects.packages import importr r_utils = importr ( 'utils' ) package_list = [ 'aplpack' , 'cluster' , 'codetools' , 'dbscan' , 'factoextra' , 'gam' , 'ggplot2' , 'splines' , 'TeachingDemos' ] for name in package_list : r_utils . install_packages ( name ) if (!document.getElementById('mathjaxscript_pelican_#%@#$@#')) { var mathjaxscript = document.createElement('script'); mathjaxscript.id = 'mathjaxscript_pelican_#%@#$@#'; mathjaxscript.type = 'text/javascript'; mathjaxscript.src = '//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML'; mathjaxscript[(window.opera ? \"innerHTML\" : \"text\")] = \"MathJax.Hub.Config({\" + \" config: ['MMLorHTML.js'],\" + \" TeX: { extensions: ['AMSmath.js','AMSsymbols.js','noErrors.js','noUndefined.js'], equationNumbers: { autoNumber: 'AMS' } },\" + \" jax: ['input/TeX','input/MathML','output/HTML-CSS'],\" + \" extensions: ['tex2jax.js','mml2jax.js','MathMenu.js','MathZoom.js'],\" + \" displayAlign: 'center',\" + \" displayIndent: '0em',\" + \" showMathMenu: true,\" + \" tex2jax: { \" + \" inlineMath: [ ['$','$'] ], \" + \" displayMath: [ ['$$','$$'] ],\" + \" processEscapes: true,\" + \" preview: 'TeX',\" + \" }, \" + \" 'HTML-CSS': { \" + \" linebreaks: { automatic: true, width: '95% container' }, \" + \" styles: { '.MathJax_Display, .MathJax .mo, .MathJax .mi, .MathJax .mn': {color: 'black ! important'} }\" + \" } \" + \"}); \"; (document.body || document.getElementsByTagName('head')[0]).appendChild(mathjaxscript); }","tags":"labs","url":"labs/lab1/Rset/"},{"title":"Lecture 1:  Introduction, Review of 109A and preview of 109B","text":"Slides PDF PPTX","tags":"lectures","url":"lectures/lecture1/"},{"title":"Advance Sections 4","text":"Advance Sections 4","tags":"pages","url":"pages/a-section4/"},{"title":"Advance Sections 5","text":"Advance Sections 5","tags":"pages","url":"pages/a-section5/"},{"title":"Advance Sections 6","text":"Advance Sections 6","tags":"pages","url":"pages/a-section6/"},{"title":"Advance Sections 7","text":"Advance Sections 7","tags":"pages","url":"pages/a-section7/"},{"title":"Lecture 24","text":"Lecture 24","tags":"pages","url":"pages/lecture24/"},{"title":"Lecture 25","text":"Lecture 25","tags":"pages","url":"pages/lecture25/"},{"title":"CS 109B: Advanced Topics in Data","text":"Spring 2019 Pavlos Protopapas and Mark Glickman pavlos@seas.harvard.edu glickman@fas.harvard.edu Pavlos: Mondays 3-4pm at MD G108 Mark: By appointment Head TFs: Eleni Kaxiras eleni@seas.harvard.edu Head TF for DCE: Sol Girouard solgirouard@g.harvard.edu Lectures: Mon and Wed 1:30‐2:45pm in Maxwell-Dworkin G-115 Labs: Thur 4:30-6:00pm in Pierce 301 Advanced Sections: Wed. 3:00pm-4:15pm, location TBD (starting 2/13) Prerequisites: CS 109a, AC 209a, Stat 121a, CSCI E-109a or equivalent. Data Science 2 is the second half of a one-year introduction to data science. Building upon the material in Data Science 1, the course introduces advanced methods for data wrangling, data visualization, and deep neural networks, statistical modeling, and prediction. Topics include big data and database management, multiple deep learning subjects such as CNNs, RNNs, autoencoders, and generative models as well as basic Bayesian methods, nonlinear statistical models and unsupervised learning. Announcements: Video-recorded Lectures from CS109A Fall '18 Advanced Sections start tomorrow Wed. Feb 13th at NW B-103 Pierce Hall is at 29 Oxford St in Cambridge. Maxwell Dworkin (MD) is at 33 Oxford St, Cambridge. Northwest Building (NW) is at 52 Oxford St, Cambridge. HELPLINE: cs109b2019@gmail.com Office Hours : Weekly Schedule For enrollment issues including cross-registration: contact the FAS Registrar's Office either in person at the Smith Campus Center (1350 Massachusetts Avenue, Suite 450) or by sending an email to registrar@fas.harvard.ed Previous Material: 2018 pre { background-color: #F5F5F5; display: block; font-family: monospace; font-size: 14px; white-space: pre; border-color: #999999; border-width: 1px; border-style: solid; border-radius: 6px; margin: 1em 0; padding: 5px; white-space: pre-wrap; } .containerMain { display: flex; width: 100%; height: 300px; } .contentA { flex: 1; flex-direction:column; } .contentB { flex: 3; }","tags":"pages","url":"pages/cs-109b-advanced-topics-in-data/"}]}